<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Bartlomiej Lukaszuk" />
  <title>Solutions - Association - Romeo and Julia, where Romeo is Basic Statistics</title>
  <link rel="stylesheet" href="./style.css"/>
    <script src="./mousetrap.min.js"></script>
    <style>
  @font-face {
    font-family: JuliaMono-Regular;
    src: url("./JuliaMono-Regular.woff2");
  }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <link rel="stylesheet" href="./github.min.css">
<script src="./highlight.min.js"></script>
<script src="./julia.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', (event) => {
    document.querySelectorAll('pre').forEach((el) => {
        if (!el.classList.contains('output')) {
            hljs.highlightElement(el);
        }
    });
});
</script>
 
</head>
<body>
<script>
function click_next() {
  var next = document.getElementById('nav-next');
  next.firstElementChild.nextElementSibling.click();
}
function click_prev() {
  var prev = document.getElementById('nav-prev');
  prev.firstElementChild.click();
}
Mousetrap.bind('right', click_next);
Mousetrap.bind('h', click_prev);
Mousetrap.bind('left', click_prev);
Mousetrap.bind('l', click_next);
</script>

<div class="books-container">
<aside class="books-menu">
<input type="checkbox" id="menu">
<label for="menu">☰</label>
<div class="books-title">
<a href="./">Romeo and Julia, where Romeo is Basic Statistics</a>
</div><br />
<span class="books-subtitle">

</span>
<div class="books-menu-content">
<li><a class="menu-level-1" href="./about.html"><b>1</b> About</a></li>
<li><a class="menu-level-1" href="./why_julia.html"><b>2</b> Why Julia</a></li>
<li><a class="menu-level-2" href="./julia_is_fast.html"><b>2.1</b> Julia is fast</a></li>
<li><a class="menu-level-2" href="./julia_is_simple.html"><b>2.2</b> Julia is simple</a></li>
<li><a class="menu-level-2" href="./jl_pleasure_to_write.html"><b>2.3</b> Pleasure to write</a></li>
<li><a class="menu-level-2" href="./jl_not_mainstream.html"><b>2.4</b> Not mainstream</a></li>
<li><a class="menu-level-2" href="./jl_open_source.html"><b>2.5</b> Julia is free</a></li>
<li><a class="menu-level-1" href="./julia_first_encounter.html"><b>3</b> Julia - first encounter</a></li>
<li><a class="menu-level-2" href="./julia_installation.html"><b>3.1</b> Installation</a></li>
<li><a class="menu-level-2" href="./julia_language_constructs.html"><b>3.2</b> Language Constructs</a></li>
<li><a class="menu-level-2" href="./julia_language_variables.html"><b>3.3</b> Variables</a></li>
<li><a class="menu-level-2" href="./julia_language_functions.html"><b>3.4</b> Functions</a></li>
<li><a class="menu-level-2" href="./julia_language_decision_making.html"><b>3.5</b> Decision Making</a></li>
<li><a class="menu-level-2" href="./julia_language_repetition.html"><b>3.6</b> Repetition</a></li>
<li><a class="menu-level-2" href="./julia_language_libraries.html"><b>3.7</b> Additional libraries</a></li>
<li><a class="menu-level-2" href="./julia_language_exercises.html"><b>3.8</b> Julia - Exercises</a></li>
<li><a class="menu-level-2" href="./julia_language_exercises_solutions.html"><b>3.9</b> Julia - Solutions</a></li>
<li><a class="menu-level-1" href="./statistics_intro.html"><b>4</b> Statistics - introduction</a></li>
<li><a class="menu-level-2" href="./statistics_intro_imports.html"><b>4.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./statistics_intro_probability_definition.html"><b>4.2</b> Probability - definition</a></li>
<li><a class="menu-level-2" href="./statistics_intro_probability_properties.html"><b>4.3</b> Probability - properties</a></li>
<li><a class="menu-level-2" href="./statistics_prob_theor_practice.html"><b>4.4</b> Probability - theory and..</a></li>
<li><a class="menu-level-2" href="./statistics_prob_distribution.html"><b>4.5</b> Probability distribution</a></li>
<li><a class="menu-level-2" href="./statistics_normal_distribution.html"><b>4.6</b> Normal distribution</a></li>
<li><a class="menu-level-2" href="./statistics_intro_hypothesis_testing.html"><b>4.7</b> Hypothesis testing</a></li>
<li><a class="menu-level-2" href="./statistics_intro_exercises.html"><b>4.8</b> Statistics intro - Exerc..</a></li>
<li><a class="menu-level-2" href="./statistics_intro_exercises_solutions.html"><b>4.9</b> Statistics intro - Solut..</a></li>
<li><a class="menu-level-1" href="./compare_contin_data.html"><b>5</b> Comparisons - continuous d..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_imports.html"><b>5.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_one_samp_ttest.html"><b>5.2</b> One sample Student’s t..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_two_samp_ttest.html"><b>5.3</b> Two samples Student’s ..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_one_way_anova.html"><b>5.4</b> One-way ANOVA</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_post_hoc_tests.html"><b>5.5</b> Post-hoc tests</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_multip_correction.html"><b>5.6</b> Multiplicity correction</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_exercises.html"><b>5.7</b> Exercises - Comparisons ..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_exercises_solutions.html"><b>5.8</b> Solutions - Comparisons ..</a></li>
<li><a class="menu-level-1" href="./compare_categ_data.html"><b>6</b> Comparisons - categorical ..</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_imports.html"><b>6.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_flashback.html"><b>6.2</b> Flashback</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_chisq_test.html"><b>6.3</b> Chi squared test</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_fisher_exact_text.html"><b>6.4</b> Fisher’s exact test</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_bigger_table.html"><b>6.5</b> Bigger table</a></li>
<li><a class="menu-level-2" href="./compare_categ_test_for_independence.html"><b>6.6</b> Test for independence</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_exercises.html"><b>6.7</b> Exercises - Comparisons ..</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_exercises_solutions.html"><b>6.8</b> Solutions - Comparisons ..</a></li>
<li><a class="menu-level-1" href="./association.html"><b>7</b> Association</a></li>
<li><a class="menu-level-2" href="./association_imports.html"><b>7.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./association_lin_relation.html"><b>7.2</b> Linear relation</a></li>
<li><a class="menu-level-2" href="./association_covariance.html"><b>7.3</b> Covariance</a></li>
<li><a class="menu-level-2" href="./association_correlation.html"><b>7.4</b> Correlation</a></li>
<li><a class="menu-level-2" href="./association_corr_pitfalls.html"><b>7.5</b> Correlation Pitfalls</a></li>
<li><a class="menu-level-2" href="./association_exercises.html"><b>7.6</b> Exercises - Association</a></li>
<li><a class="menu-level-2" href="./association_exercises_solutions.html"><b>7.7</b> Solutions - Association</a></li>
<li><a class="menu-level-1" href="./prediction.html"><b>8</b> Prediction</a></li>
<li><a class="menu-level-2" href="./prediction_imports.html"><b>8.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./pred_simple_lin_reg.html"><b>8.2</b> Simple Linear Regression</a></li>
<li><a class="menu-level-1" href="./appendix.html"><b></b> Appendix</a></li>
<li><a class="menu-level-1" href="./references.html"><b>9</b> References</a></li>
</div>
</aside>

<div class="books-content">
<h2 data-number="7.7" id="sec:association_exercises_solutions"><span class="header-section-number">7.7</span> Solutions - Association</h2>
<p>In this sub-chapter you will find exemplary solutions to the exercises from the previous section.</p>
<h3 data-number="7.7.1" id="sec:association_ex1_solution"><span class="header-section-number">7.7.1</span> Solution to Exercise 1</h3>
<p>Let’s write <code>getRanks</code>, but let’s start simple and use it on a sorted vector <code>[100, 500, 1000]</code> without ties. In this case the body of <code>getRanks</code> function would be something like.</p>
<pre class="language-julia"><code># for now the function is without types
function getRanksVer1(v)
    # or: ranks = collect(1:length(v))
    ranks = collect(eachindex(v))
    return ranks
end

getRanksVer1([100, 500, 1000])</code></pre>
<pre class="output"><code>[1, 2, 3]</code></pre>
<p>Time to complicate stuff a bit by adding some ties in numbers.</p>
<pre class="language-julia"><code># for now the function is without types
function getRanksVer2(v)
    initialRanks = collect(eachindex(v))
    finalRanks = zeros(length(v))
    for i in eachindex(v)
        indicesInV = findall(x -&gt; x == v[i], v)
        finalRanks[i] = Stats.mean(initialRanks[indicesInV])
    end
    return finalRanks
end

(
    getRanksVer2([100, 500, 500, 1000]),
    getRanksVer2([100, 500, 500, 500, 1000])
)</code></pre>
<pre class="output"><code>([1.0, 2.5, 2.5, 4.0],
 [1.0, 3.0, 3.0, 3.0, 5.0])</code></pre>
<p>The <code>findall</code> function accepts a <code>Funcion</code> and a <code>Vector</code> (actually, an <code>Array</code>, still, a <code>Vector</code> is a special type of an <code>Array</code>). Next, it runs the function on every element of the <code>Array</code> and returns the indices for which the result was <code>true</code>. Then, we use <code>indicesInV</code> to get the <code>initialRanks</code>. The <code>initialRanks[indicesInV]</code> returns a <code>Vector</code> that contains one or more (if ties occur) <code>initialRanks</code> for a given element of <code>v</code>. Finally, we calculate the average rank for a given number in <code>v</code> by using <code>Stats.mean</code>. The function may be sub-optimall as for <code>[100, 500, 500, 1000]</code> the average rank for <code>500</code> is calculated twice (once for <code>500</code> at index 2 and once for <code>500</code> at index 3) and for <code>[100, 500, 500, 500, 1000]</code> the average rank for <code>500</code> is calculated three times. Still, we are more concerned with the correct result and not the efficiency (assuming that the function is fast enough) so we will leave it as it is.</p>
<p>Now, the final tweak. The input vector is shuffled.</p>
<pre class="language-julia"><code># for now the function is without types
function getRanksVer3(v)
    sortedV = collect(sort(v))
    initialRanks = collect(eachindex(sortedV))
    finalRanks = zeros(length(v))
    for i in eachindex(v)
        indicesInSortedV = findall(x -&gt; x == v[i], sortedV)
        finalRanks[i] = Stats.mean(initialRanks[indicesInSortedV])
    end
    return finalRanks
end

(
    getRanksVer3([500, 100, 1000]),
    getRanksVer3([500, 100, 500, 1000]),
    getRanksVer3([500, 100, 500, 1000, 500])
)</code></pre>
<pre class="output"><code>([2.0, 1.0, 3.0],
 [2.5, 1.0, 2.5, 4.0],
 [3.0, 1.0, 3.0, 5.0, 3.0])</code></pre>
<p>Here, we let the built in function <code>sort</code> to arrange the numbers from <code>v</code> in the ascending order. Then for each number from <code>v</code> we get its indices in <code>sortedV</code> and its ranks based on that (<code>initialRanks[indicesInSortedV]</code>). As in <code>getRanksVer2</code> the latter is used to calculate their average.</p>
<p>OK, time for cleanup + adding some types for future references (before we forget them).</p>
<pre class="language-julia"><code>function getRanks(v::Vector{&lt;:Real})::Vector{&lt;:Float64}
    sortedV::Vector{&lt;:Real} = collect(sort(v))
    initialRanks::Vector{&lt;:Int} = collect(eachindex(sortedV))
    finalRanks::Vector{&lt;:Float64} = zeros(length(v))
    for i in eachindex(v)
        indicesInSortedV = findall(x -&gt; x == v[i], sortedV)
        finalRanks[i] = Stats.mean(initialRanks[indicesInSortedV])
    end
    return finalRanks
end

(
    getRanks([100, 500, 1000]),
    getRanks([100, 500, 500, 1000]),
    getRanks([500, 100, 1000]),
    getRanks([500, 100, 500, 1000]),
    getRanks([500, 100, 500, 1000, 500])
)</code></pre>
<pre class="output"><code>([1.0, 2.0, 3.0],
 [1.0, 2.5, 2.5, 4.0],
 [2.0, 1.0, 3.0],
 [2.5, 1.0, 2.5, 4.0],
 [3.0, 1.0, 3.0, 5.0, 3.0])</code></pre>
<p>After long last we can define <code>getSpearmCorAndPval</code> and apply it to <code>animals</code> data frame.</p>
<pre class="language-julia"><code>function getSpearmCorAndPval(
    v1::Vector{&lt;:Real}, v2::Vector{&lt;:Real})::Tuple{Float64, Float64}
    return getCorAndPval(getRanks(v1), getRanks(v2))
end

getSpearmCorAndPval(animals.Body, animals.Brain)</code></pre>
<pre class="output"><code>(0.7162994456021083, 1.8128636948722132e-5)</code></pre>
<p>The result appears to reflect the general relationship well (compare with Figure 32).</p>
<h3 data-number="7.7.2" id="sec:association_ex2_solution"><span class="header-section-number">7.7.2</span> Solution to Exercise 2</h3>
<p>The solution should be quite simple assuming you did solve exercise 4 from ch05 (see Section <a href="./compare_contin_data_exercises.html#sec:compare_contin_data_ex4">5.7.4</a> and Section <a href="./compare_contin_data_exercises_solutions.html#sec:compare_contin_data_ex4_solution">5.8.4</a>) and exercise 5 from ch06 (see Section <a href="./compare_categ_data_exercises.html#sec:compare_categ_data_ex5">6.7.5</a> and Section <a href="./compare_categ_data_exercises_solutions.html#sec:compare_categ_data_ex5_solution">6.8.5</a>).</p>
<p>For it we are going to use two helper functions, <code>getUniquePairs</code> (Section <a href="./compare_contin_data_exercises_solutions.html#sec:compare_contin_data_ex4_solution">5.8.4</a>) and <code>getSortedKeysVals</code> (Section <a href="./statistics_prob_distribution.html#sec:statistics_prob_distribution">4.5</a>) developed previously. For your convenience I paste them below.</p>
<pre>
function getUniquePairs(names::Vector{T})::Vector{Tuple{T,T}} where {T}
    @assert (length(names) >= 2) "the input must be of length >= 2"
    uniquePairs::Vector{Tuple{T,T}} =
        Vector{Tuple{T,T}}(undef, binomial(length(names), 2))
    currInd::Int = 1
    for i in eachindex(names)[1:(end-1)]
        for j in eachindex(names)[(i+1):end]
            uniquePairs[currInd] = (names[i], names[j])
            currInd += 1
        end
    end
    return uniquePairs
end

function getSortedKeysVals(d::Dict{T1,T2})::Tuple{
    Vector{T1},Vector{T2}} where {T1,T2}
    sortedKeys::Vector{T1} = keys(d) |> collect |> sort
    sortedVals::Vector{T2} = [d[k] for k in sortedKeys]
    return (sortedKeys, sortedVals)
end
</pre>
<p>Now, time to get all possible ‘raw’ correlations.</p>
<pre class="language-julia"><code>function getAllCorsAndPvals(
    df::Dfs.DataFrame, colsNames::Vector{String}
)::Dict{Tuple{String,String},Tuple{Float64,Float64}}

    uniquePairs::Vector{Tuple{String,String}} = getUniquePairs(colsNames)
    allCors::Dict{Tuple{String,String},Tuple{Float64,Float64}} = Dict(
        (n1, n2) =&gt; getCorAndPval(df[:, n1], df[:, n2]) for (n1, n2)
        in
        uniquePairs)

    return allCors
end</code></pre>
<pre class="output"><code>getAllCorsAndPvals (generic function with 1 method)</code></pre>
<p>We start by getting the <code>uniquePairs</code> for the columns of interest <code>colNames</code>. Then we use dictionary comprehension to get our result. We iterate through each pair <code>for (n1, n2) in uniquePairs</code>. Each <code>uniquePair</code> is composed of a tuple <code>(n1, n2)</code>, where <code>n1</code> - name1, <code>n2</code> - name2. While traversing the <code>uniquePairs</code> we calculate the correlations and p-values (<code>getCorAndPval</code>) by selecting columns of interest (<code>df[:, n1]</code> and <code>df[:, n2]</code>). And that’s it. Let’s see how it works and how many false positives we got (remember, we expect 2 or 3).</p>
<pre class="language-julia"><code>allCorsPvals = getAllCorsAndPvals(bogusCors, letters)
falsePositves = (map(t -&gt; t[2], values(allCorsPvals)) .&lt;= 0.05) |&gt; sum
falsePositves</code></pre>
<p>3</p>
<p>First, we extract the values from our dictionary with <code>values(allCorsPvals)</code>. The values are a vector of tuples [<code>(cor, pval)</code>]. To get p-values alone, we use map function that takes every tuple (<code>t</code>) and returns its second element (<code>t[2]</code>). Finally, we compare the p-values with our cutoff level for type 1 error (<span class="math inline">\(\alpha = 0.05\)</span>). And sum the <code>Bool</code>s (each <code>true</code> is counted as 1, and each <code>false</code> as 0).</p>
<p>Anyway, as expected we got 3 false positives. All that’s left to do is to apply the multiplicity correction.</p>
<pre class="language-julia"><code>function adjustPvals(
    corsAndPvals::Dict{Tuple{String,String},Tuple{Float64,Float64}},
    adjMeth::Type{M}
)::Dict{Tuple{String,String},Tuple{Float64,Float64}} where
    {M&lt;:Mt.PValueAdjustment}

    ks, vs = getSortedKeysVals(corsAndPvals)
    cors::Vector{&lt;:Float64} = map(t -&gt; t[1], vs)
    pvals::Vector{&lt;:Float64} = map(t -&gt; t[2], vs)
    adjustedPVals::Vector{&lt;:Float64} = Mt.adjust(pvals, adjMeth())
    newVs::Vector{Tuple{Float64,Float64}} = collect(
        zip(cors, adjustedPVals))

    return Dict(ks[i] =&gt; newVs[i] for i in eachindex(ks))
end</code></pre>
<pre class="output"><code>adjustPvals (generic function with 1 method)</code></pre>
<p>The code is rather self explanatory and relies on step by step getting our p-values (<code>pvals</code>) applying an adjustment method (<code>adjMeth</code>) on them (<code>Mt.adjust</code>) and combining the adjusted p-values (<code>adjustedPVals</code>) with <code>cors</code> again. For that we use <code>zip</code> function we met in Section <a href="./compare_categ_data_exercises_solutions.html#sec:compare_categ_data_ex1_solution">6.8.1</a>. Finally we recreate a dictionary using comprehension. Time for some tests.</p>
<pre class="language-julia"><code>allCorsPvalsAdj = adjustPvals(allCorsPvals, Mt.BenjaminiHochberg)
falsePositves = (map(t -&gt; t[2], values(allCorsPvalsAdj)) .&lt;= 0.05) |&gt; sum
falsePositves</code></pre>
<p>0</p>
<p>The correction appears to be working correctly, we got rid of false positives.</p>
<h3 data-number="7.7.3" id="sec:association_ex3_solution"><span class="header-section-number">7.7.3</span> Solution to Exercise 3</h3>
<p>Let’s start by writing a function to get a correlation matrix. We could use for that <a href="https://docs.julialang.org/en/v1/stdlib/Statistics/#Statistics.cor">Stats.cor</a> like so <code>Stats.cor(bogusCors)</code>. But since we need to add significance markers then the p-values for the correlations are indispensable. As far as I’m aware the package does not have it, then we will write a function of our own.</p>
<pre class="language-julia"><code>function getCorsAndPvalsMatrix(
    df::Dfs.DataFrame,
    colNames::Vector{String})::Array{&lt;:Tuple{Float64, Float64}}

    len::Int = length(colNames)
    corsPvals::Dict{Tuple{String,String},Tuple{Float64,Float64}} =
        getAllCorsAndPvals(df, colNames)
    mCorsPvals::Array{Tuple{Float64,Float64}} = fill((0.0, 0.0), len, len)

    for cn in eachindex(colNames) # cn - column number
        for rn in eachindex(colNames) # rn - row number
            corPval = (
                haskey(corsPvals, (colNames[rn], colNames[cn])) ?
                corsPvals[(colNames[rn], colNames[cn])] :
                get(corsPvals, (colNames[cn], colNames[rn]), (1, 1))
            )
            mCorsPvals[rn, cn] = corPval
        end
    end

    return mCorsPvals
end</code></pre>
<pre class="output"><code>getCorsAndPvalsMatrix (generic function with 1 method)</code></pre>
<p>The function <code>getCorsAndPvalsMatrix</code> uses <code>getAllCorsAndPvals</code> we developed previously (Section <a href="./association_exercises_solutions.html#sec:association_ex2_solution">7.7.2</a>). Then we define the matrix (our result), we initialize it with the <a href="https://docs.julialang.org/en/v1/base/arrays/#Base.fill">fill function</a> that takes an initial value and returns an array of a given size filled with that value (<code>(0.0, 0.0)</code>). Next, we replace the initial values in <code>mCorsPvals</code> with the correct ones by using two <code>for</code> loops. Inside them we extract a tuple (<code>corPval</code>) from the unique <code>corsPvals</code>. First, we test if a <code>corPval</code> for a given two variables (e.g. “a” and “b”) is in the dictionary <code>corsPvals</code> (<code>haskey</code> etc.). If so then we insert it into the <code>mCorsPvals</code>. If not, then we search in <code>corsPvals</code> by its reverse (so, e.g. “b” and “a”) with <code>get(corsPvals, (colNames[cn], colNames[rn]), etc.)</code>. If that combination is not present then we are looking for the correlation of a variable with itself (e.g. “a” and “a”) which is equal to <code>(1, 1)</code> (for correlation coefficient and p-value, respectively). Once we are done we return our <code>mCorsPvals</code> matrix (aka <code>Array</code>). Time to give it a test run.</p>
<pre class="language-julia"><code>getCorsAndPvalsMatrix(bogusCors, [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;])</code></pre>
<pre class="output"><code>3×3 Matrix{Tuple{Float64, Float64}}:
 (1.0, 1.0)             (0.194, 0.591239)      (-0.432251, 0.212195)
 (0.194, 0.591239)      (1.0, 1.0)             (-0.205942, 0.568128)
 (-0.432251, 0.212195)  (-0.205942, 0.568128)  (1.0, 1.0)</code></pre>
<p>The numbers seem to be OK. In the future, you may consider changing the function so that the p-values are adjusted, e.g. by using <code>Mt.BenjaminiHochberg</code> correction, but here we need some statistical significance for our heatmap so we will leave it as it is.</p>
<p>Now, let’s move to drawing a plot.</p>
<pre>
mCorsPvals = getCorsAndPvalsMatrix(bogusCors, letters)
cors = map(t -> t[1], mCorsPvals)
pvals = map(t -> t[2], mCorsPvals)
nRows, _ = size(cors) # same num of rows and cols in our matrix
xs = repeat(1:nRows, inner=nRows)
ys = repeat(1:nRows, outer=nRows)[end:-1:1]

fig = Cmk.Figure()
ax, hm = Cmk.heatmap(fig[1, 1], xs, ys, [cors...],
    colormap=:RdBu, colorrange=(-1, 1),
    axis=(;
        xticks=(1:1:nRows, letters[1:nRows]),
        yticks=(1:1:nRows, letters[1:nRows][end:-1:1])
    ))
Cmk.hlines!(fig[1, 1], 1.5:1:nRows, color="black", linewidth=0.25)
Cmk.vlines!(fig[1, 1], 1.5:1:nRows, color="black", linewidth=0.25)
Cmk.Colorbar(fig[:, end+1], hm)
fig
</pre>
<p>We begin by preparing the necessary helper variables (<code>mCorsPvals</code>, <code>cors</code>, <code>pvals</code>, <code>nRows</code>, <code>xs</code>, <code>ys</code>). The last two are the coordinates of the centers of squares on the X- and Y-axis. The <code>cors</code> will be flattened row by row using <code>[cors...]</code> syntax. For your information <code>repeat([1, 2], inner = 2)</code> returns <code>[1, 1, 2, 2]</code> and <code>repeat([1, 2], outer = 2)</code> returns <code>[1, 2, 1, 2]</code>. The <code>ys</code> vector is then reversed with <code>[end:-1:1]</code> to make it reflect better the order of correlations in <code>cors</code> (left to right, row by row). The same goes for <code>yticks</code> below. The above was determined to be the right option by trial and error. The next important parameter is <code>colorrange=(-1, 1)</code> it ensures that <code>-1</code> is always the leftmost color (red) from the <code>:RdBu</code> colormap and <code>1</code> is always the rightmost color (blue) from the colormap. Without it the colors would be set to <code>minimum(cors)</code> and <code>maximum(cors)</code> which we do not want since the <code>minimum</code> will change from matrix to matrix. Over our heatmap we overlay the grid (<code>hlines!</code> and <code>vlines!</code>) to make the squares separate better from one another. The centers of the squares are at integers, and the edges are at halves, that’s why we start the ticks at <code>1.5</code>. Finlay, we add <code>Colorbar</code> as they did in the docs for <code>Cmk.heatmap</code>. The result of this code is visible in Figure 33 from the previous section.</p>
<p>OK, let’s add the correlation coefficients and statistical significance markers. But firs, two little helper functions.</p>
<pre class="language-julia"><code>function getColorForCor(corCoeff::Float64)::String
    @assert (0 &lt;= abs(corCoeff) &lt;= 1) &quot;abc(corCoeff) must be in range [0-1]&quot;
    return (abs(corCoeff) &gt;= 0.65) ? &quot;white&quot; : &quot;black&quot;
end

function getMarkerForPval(pval::Float64)::String
    @assert (0 &lt;= pval &lt;= 1) &quot;probability must be in range [0-1]&quot;
    return (pval &lt;= 0.05) ? &quot;#&quot; : &quot;&quot;
end</code></pre>
<pre class="output"><code>getMarkerForPval (generic function with 1 method)</code></pre>
<p>As you can see <code>getColorForCor</code> returns a color (“white” or “black”) for a given value of correlation coefficient (white color will make it easier to read the correlation coefficient on a dark red/blue background of a square). On the other hand <code>getMarkerForPval</code> returns a marker (" #") when a pvalue is below a customary cutoff level for type I error.</p>
<pre>
fig = Cmk.Figure()
ax, hm = Cmk.heatmap(fig[1, 1], xs, ys, [cors...],
    colormap=:RdBu, colorrange=(-1, 1),
    axis=(;
        xticks=(1:1:nRows, letters[1:nRows]),
        yticks=(1:1:nRows, letters[1:nRows][end:-1:1])
    ))
Cmk.text!(fig[1, 1], xs, ys,
    text=string.(round.([cors...], digits=2)) .*
        getMarkerForPval.([pvals...]),
    align=(:center, :center),
    color=getColorForCor.([cors...]))
Cmk.hlines!(fig[1, 1], 1.5:1:nRows, color="black", linewidth=0.25)
Cmk.vlines!(fig[1, 1], 1.5:1:nRows, color="black", linewidth=0.25)
Cmk.Colorbar(fig[:, end+1], hm)
fig
</pre>
<p>The only new element here is <code>Cmk.text!</code> function but since we used it a couple of times throughout this book, then I will leave the explanation of how the code piece works for you. Anyway, the result is to be found below.</p>
<figure>
<img src="./images/ch07ex3v2.png" id="fig:ch07ex3v2" alt="Figure 34: Correlation heatmap for data in bogusCors with the coefficients and significance markers." /><figcaption aria-hidden="true">Figure 34: Correlation heatmap for data in <code>bogusCors</code> with the coefficients and significance markers.</figcaption>
</figure>
<p>It looks good. Also the number of significance markers is right. Previously (Section <a href="./association_exercises_solutions.html#sec:association_ex2_solution">7.7.2</a>) we said we got 3 significant correlations (based on ‘raw’ p-values). Since, the upper right triangle of the heatmap is a mirror reflection of the lower left triangle, then we should see 6 significance markers altogether.</p>
<p>To be continued…</p>


<div class="bottom-nav">
    <p id="nav-prev" style="text-align: left;">
        <a class="menu-level-2" href="./association_exercises.html"><b>7.6</b> Exercises - Association</a> <kbd>←</kbd>
        <span id="nav-next" style="float: right;">
            <kbd>→</kbd> <a class="menu-level-1" href="./prediction.html"><b>8</b> Prediction</a>
        </span>
    </p>
</div>


<div class="license">
    <br/>
  <br/>
  <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>
    Bartlomiej Lukaszuk
</div>
</div>
</div>
</body>
</html>