<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Bartlomiej Lukaszuk" />
  <title>Multiple Linear Regression - Romeo and Julia, where Romeo is Basic Statistics</title>
  <link rel="stylesheet" href="./style.css"/>
    <script src="./mousetrap.min.js"></script>
    <style>
  @font-face {
    font-family: JuliaMono-Regular;
    src: url("./JuliaMono-Regular.woff2");
  }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <link rel="stylesheet" href="./github.min.css">
<script src="./highlight.min.js"></script>
<script src="./julia.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', (event) => {
    document.querySelectorAll('pre').forEach((el) => {
        if (!el.classList.contains('output')) {
            hljs.highlightElement(el);
        }
    });
});
</script>
 
</head>
<body>
<script>
function click_next() {
  var next = document.getElementById('nav-next');
  next.firstElementChild.nextElementSibling.click();
}
function click_prev() {
  var prev = document.getElementById('nav-prev');
  prev.firstElementChild.click();
}
Mousetrap.bind('right', click_next);
Mousetrap.bind('h', click_prev);
Mousetrap.bind('left', click_prev);
Mousetrap.bind('l', click_next);
</script>

<div class="books-container">
<aside class="books-menu">
<input type="checkbox" id="menu">
<label for="menu">☰</label>
<div class="books-title">
<a href="./">Romeo and Julia, where Romeo is Basic Statistics</a>
</div><br />
<span class="books-subtitle">

</span>
<div class="books-menu-content">
<li><a class="menu-level-1" href="./about.html"><b>1</b> About</a></li>
<li><a class="menu-level-1" href="./why_julia.html"><b>2</b> Why Julia</a></li>
<li><a class="menu-level-2" href="./julia_is_fast.html"><b>2.1</b> Julia is fast</a></li>
<li><a class="menu-level-2" href="./julia_is_simple.html"><b>2.2</b> Julia is simple</a></li>
<li><a class="menu-level-2" href="./jl_pleasure_to_write.html"><b>2.3</b> Pleasure to write</a></li>
<li><a class="menu-level-2" href="./jl_not_mainstream.html"><b>2.4</b> Not mainstream</a></li>
<li><a class="menu-level-2" href="./jl_open_source.html"><b>2.5</b> Julia is free</a></li>
<li><a class="menu-level-1" href="./julia_first_encounter.html"><b>3</b> Julia - first encounter</a></li>
<li><a class="menu-level-2" href="./julia_installation.html"><b>3.1</b> Installation</a></li>
<li><a class="menu-level-2" href="./julia_language_constructs.html"><b>3.2</b> Language Constructs</a></li>
<li><a class="menu-level-2" href="./julia_language_variables.html"><b>3.3</b> Variables</a></li>
<li><a class="menu-level-2" href="./julia_language_functions.html"><b>3.4</b> Functions</a></li>
<li><a class="menu-level-2" href="./julia_language_decision_making.html"><b>3.5</b> Decision Making</a></li>
<li><a class="menu-level-2" href="./julia_language_repetition.html"><b>3.6</b> Repetition</a></li>
<li><a class="menu-level-2" href="./julia_language_libraries.html"><b>3.7</b> Additional libraries</a></li>
<li><a class="menu-level-2" href="./julia_language_exercises.html"><b>3.8</b> Julia - Exercises</a></li>
<li><a class="menu-level-2" href="./julia_language_exercises_solutions.html"><b>3.9</b> Julia - Solutions</a></li>
<li><a class="menu-level-1" href="./statistics_intro.html"><b>4</b> Statistics - introduction</a></li>
<li><a class="menu-level-2" href="./statistics_intro_imports.html"><b>4.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./statistics_intro_probability_definition.html"><b>4.2</b> Probability - definition</a></li>
<li><a class="menu-level-2" href="./statistics_intro_probability_properties.html"><b>4.3</b> Probability - properties</a></li>
<li><a class="menu-level-2" href="./statistics_prob_theor_practice.html"><b>4.4</b> Probability - theory and..</a></li>
<li><a class="menu-level-2" href="./statistics_prob_distribution.html"><b>4.5</b> Probability distribution</a></li>
<li><a class="menu-level-2" href="./statistics_normal_distribution.html"><b>4.6</b> Normal distribution</a></li>
<li><a class="menu-level-2" href="./statistics_intro_hypothesis_testing.html"><b>4.7</b> Hypothesis testing</a></li>
<li><a class="menu-level-2" href="./statistics_intro_exercises.html"><b>4.8</b> Statistics intro - Exerc..</a></li>
<li><a class="menu-level-2" href="./statistics_intro_exercises_solutions.html"><b>4.9</b> Statistics intro - Solut..</a></li>
<li><a class="menu-level-1" href="./compare_contin_data.html"><b>5</b> Comparisons - continuous d..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_imports.html"><b>5.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_one_samp_ttest.html"><b>5.2</b> One sample Student’s t..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_two_samp_ttest.html"><b>5.3</b> Two samples Student’s ..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_one_way_anova.html"><b>5.4</b> One-way ANOVA</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_post_hoc_tests.html"><b>5.5</b> Post-hoc tests</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_multip_correction.html"><b>5.6</b> Multiplicity correction</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_exercises.html"><b>5.7</b> Exercises - Comparisons ..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_exercises_solutions.html"><b>5.8</b> Solutions - Comparisons ..</a></li>
<li><a class="menu-level-1" href="./compare_categ_data.html"><b>6</b> Comparisons - categorical ..</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_imports.html"><b>6.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_flashback.html"><b>6.2</b> Flashback</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_chisq_test.html"><b>6.3</b> Chi squared test</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_fisher_exact_text.html"><b>6.4</b> Fisher’s exact test</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_bigger_table.html"><b>6.5</b> Bigger table</a></li>
<li><a class="menu-level-2" href="./compare_categ_test_for_independence.html"><b>6.6</b> Test for independence</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_exercises.html"><b>6.7</b> Exercises - Comparisons ..</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_exercises_solutions.html"><b>6.8</b> Solutions - Comparisons ..</a></li>
<li><a class="menu-level-1" href="./association.html"><b>7</b> Association</a></li>
<li><a class="menu-level-2" href="./association_imports.html"><b>7.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./association_lin_relation.html"><b>7.2</b> Linear relation</a></li>
<li><a class="menu-level-2" href="./association_covariance.html"><b>7.3</b> Covariance</a></li>
<li><a class="menu-level-2" href="./association_correlation.html"><b>7.4</b> Correlation</a></li>
<li><a class="menu-level-2" href="./association_corr_pitfalls.html"><b>7.5</b> Correlation Pitfalls</a></li>
<li><a class="menu-level-2" href="./association_exercises.html"><b>7.6</b> Exercises - Association</a></li>
<li><a class="menu-level-2" href="./association_exercises_solutions.html"><b>7.7</b> Solutions - Association</a></li>
<li><a class="menu-level-1" href="./prediction.html"><b>8</b> Prediction</a></li>
<li><a class="menu-level-2" href="./prediction_imports.html"><b>8.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./pred_simple_lin_reg.html"><b>8.2</b> Simple Linear Regression</a></li>
<li><a class="menu-level-2" href="./pred_multiple_lin_reg.html"><b>8.3</b> Multiple Linear Regressi..</a></li>
<li><a class="menu-level-1" href="./appendix.html"><b></b> Appendix</a></li>
<li><a class="menu-level-1" href="./references.html"><b>9</b> References</a></li>
</div>
</aside>

<div class="books-content">
<h2 data-number="8.3" id="sec:pred_multiple_lin_reg"><span class="header-section-number">8.3</span> Multiple Linear Regression</h2>
<p>Multiple linear regression is a linear regression with more than one predictor variable. Take a look at the <a href="https://vincentarelbundock.github.io/Rdatasets/doc/Ecdat/Icecream.html">Icecream</a> data frame.</p>
<pre class="language-julia"><code>import RDatasets as RD

ice = RD.dataset(&quot;Ecdat&quot;, &quot;Icecream&quot;)
first(ice, 5)
</code></pre>
<div id="tbl:icecreamDf">
<table>
<caption>Table 15: Icecream consumption data.</caption>
<thead>
<tr class="header">
<th style="text-align: right;">Cons</th>
<th style="text-align: right;">Income</th>
<th style="text-align: right;">Price</th>
<th style="text-align: right;">Temp</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.386</td>
<td style="text-align: right;">78.0</td>
<td style="text-align: right;">0.27</td>
<td style="text-align: right;">41.0</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.374</td>
<td style="text-align: right;">79.0</td>
<td style="text-align: right;">0.282</td>
<td style="text-align: right;">56.0</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.393</td>
<td style="text-align: right;">81.0</td>
<td style="text-align: right;">0.277</td>
<td style="text-align: right;">63.0</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.425</td>
<td style="text-align: right;">80.0</td>
<td style="text-align: right;">0.28</td>
<td style="text-align: right;">68.0</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.406</td>
<td style="text-align: right;">76.0</td>
<td style="text-align: right;">0.272</td>
<td style="text-align: right;">69.0</td>
</tr>
</tbody>
</table>
</div>
<p>We got 4 columns altogether (more detail in the link above):</p>
<ul>
<li><code>Cons</code> - consumption of ice cream (pints),</li>
<li><code>Income</code> - average family income (USD),</li>
<li><code>Price</code> - price of ice cream (USD),</li>
<li><code>Temp</code> - temperature (Fahrenheit)</li>
</ul>
<p>Imagine you are an ice cream truck owner and are interested to know which factors influence (and in what way) the consumption (<code>Cons</code>) of ice-cream by your customers. Let’s start by building a model with all the possible explanatory variables.</p>
<pre class="language-julia"><code>iceMod1 = Glm.lm(Glm.@formula(Cons ~ Income + Price + Temp), ice)
iceMod1</code></pre>
<pre class="output"><code>Cons ~ 1 + Income + Price + Temp

Coefficients:
────────────────────────────────────────────────────────────────────────────────
                   Coef.   Std. Error      t  Pr(&gt;|t|)     Lower 95%   Upper 95%
────────────────────────────────────────────────────────────────────────────────
(Intercept)   0.197315    0.270216      0.73    0.4718  -0.358122     0.752752
Income        0.00330776  0.00117142    2.82    0.0090   0.000899875  0.00571565
Price        -1.04441     0.834357     -1.25    0.2218  -2.75946      0.670632
Temp          0.00345843  0.000445547   7.76    &lt;1e-07   0.00254259   0.00437426
────────────────────────────────────────────────────────────────────────────────</code></pre>
<p>Right away we can see that the price of ice-cream negatively affects (<code>Coef.</code> = -1.044) the volume of ice cream consumed (the more expensive the ice cream is the less people eat it, 1.044 pint less for every additional USD of price). The relationship is in line with our intuition. However, there is not enough evidence (p &gt; 0.05) that the real influence of <code>Price</code> on consumption isn’t 0 (so no influence). Therefore, you wonder should you perhaps remove the variable <code>Price</code> from the model like so</p>
<pre class="language-julia"><code>iceMod2 = Glm.lm(Glm.@formula(Cons ~ Income + Temp), ice)
iceMod2</code></pre>
<pre class="output"><code>Cons ~ 1 + Income + Temp

Coefficients:
───────────────────────────────────────────────────────────────────────────────
                   Coef.   Std. Error      t  Pr(&gt;|t|)    Lower 95%   Upper 95%
───────────────────────────────────────────────────────────────────────────────
(Intercept)  -0.113195    0.10828      -1.05    0.3051  -0.335367    0.108977
Income        0.00353017  0.00116996    3.02    0.0055   0.00112961  0.00593072
Temp          0.00354331  0.000444956   7.96    &lt;1e-07   0.00263033  0.00445628
───────────────────────────────────────────────────────────────────────────────</code></pre>
<p>Now, we got <code>Income</code> and <code>Temp</code> in our model, both of which are statistically significant. The values of <code>Coef.</code>s for <code>Income</code> and <code>Temp</code> somewhat changed between the models, but such changes (and even greater) are to be expected. Still, we would like to know if our new <code>iceMod2</code> is really better than <code>iceMod1</code> that we came up with before.</p>
<p>In our first try to solve the problem we could resort to the coefficient of determination (<span class="math inline">\(r^2\)</span>) that we met in Section <a href="./pred_simple_lin_reg.html#sec:pred_simple_lin_reg">8.2</a>. Intuition tells us that a better model should have a bigger <span class="math inline">\(r^2\)</span>.</p>
<pre class="language-julia"><code>round.([Glm.r2(iceMod1), Glm.r2(iceMod2)],
    digits = 3)</code></pre>
<pre class="output"><code>[0.719, 0.702]</code></pre>
<p>Hmm, <span class="math inline">\(r^2\)</span> is bigger for <code>iceMod1</code> than <code>iceMod2</code>. However, there are two problems with it: 1) the difference between the coefficients is quite small, and 2) <span class="math inline">\(r^2\)</span> gets easily inflated by any additional variable in the model. And I mean any, if you add, let’s say 10 random variables to the <code>ice</code> data frame and put them into model the coefficient of determination will go up even though this makes no sense (we know their real influence is 0). That is why we got an improved metrics called the adjusted coefficient of determination. This parameter (adj. <span class="math inline">\(r^2\)</span>) penalizes for every additional variable added to the model. Therefore the ‘noise’ variables will lower the adjusted <span class="math inline">\(r^2\)</span> whereas only truly impactful ones will be able to raise it.</p>
<pre class="language-julia"><code>round.([Glm.adjr2(iceMod1), Glm.adjr2(iceMod2)],
    digits = 3)</code></pre>
<pre class="output"><code>[0.687, 0.68]</code></pre>
<p><code>iceMod1</code> still explains more variability in <code>Cons</code> (ice cream consumption) but the magnitude of the difference dropped. This makes our decision even harder. Luckily, <code>Glm</code> has <code>ftest</code> function to help us determine if one model is significantly better than the other.</p>
<pre class="language-julia"><code>Glm.ftest(iceMod1.model, iceMod2.model)</code></pre>
<pre class="output"><code>F-test: 2 models fitted on 30 observations
───────────────────────────────────────────────────────────────
     DOF  ΔDOF     SSR    ΔSSR      R²      ΔR²      F*   p(&gt;F)
───────────────────────────────────────────────────────────────
[1]    5        0.0353          0.7190                         
[2]    4    -1  0.0374  0.0021  0.7021  -0.0169  1.5669  0.2218
───────────────────────────────────────────────────────────────</code></pre>
<p>The table contains two rows:</p>
<ul>
<li><code>[1]</code> - first model from the left (in <code>Glm.ftest</code> argument list)</li>
<li><code>[2]</code> - second model from the left (in <code>Glm.ftest</code> argument list)</li>
</ul>
<p>and a few columns:</p>
<ul>
<li><code>DOF</code> - degrees of freedom (more elements in formula, bigger <code>DOF</code>)</li>
<li><code>ΔDOF</code> - <code>DOF[2]</code> - <code>DOF[1]</code></li>
<li><code>SSR</code> - residual sum of squares (the smaller the better)</li>
<li><code>ΔSSR</code> - <code>SSR[2]</code> - <code>SSR[1]</code></li>
<li><code>R2</code> - coefficient of determination</li>
<li><code>ΔR2</code> - <code>R2[2]</code> - <code>R2[1]</code></li>
<li><code>F*</code> - F-Statistic (similar to the one we met in Section <a href="./compare_contin_data_one_way_anova.html#sec:compare_contin_data_one_way_anova">5.4</a>)</li>
<li><code>p(&gt;F)</code> - p-value for the comparison between the two models</li>
</ul>
<p>Based on the test we see that none of the models is clearly better from the other (p &gt; 0.05). Therefore, in line with <a href="https://en.wikipedia.org/wiki/Occam%27s_razor">Occam’s razor</a> principle (when two equally good explanations exist, choose the simpler one) we can safely pick <code>iceMod2</code> as our final model.</p>
<p>What we did here was the construction of a so called minimal adequate model (the smallest model that explains the greatest amount of variance in the dependent/outcome variable). We did this using top to bottom approach. We started with a ‘full’ model. Then we follow by removing explanatory variables (one by one) that do not contribute to the model (we start from highest p-value above 0.05) until only meaningful explanatory variables remain. The removal of the variables reflects our common sense, because usually we (or others that will use our model) do not want to spend time/money/energy on collecting data that are of no use to us.</p>
<p>OK, let’s inspect our minimal adequate model again.</p>
<pre class="language-julia"><code>[(cn, round(c, digits = 4)) for (cn, c) in
     zip(Glm.coefnames(iceMod2), Glm.coef(iceMod2))]</code></pre>
<pre class="output"><code>(&quot;(Intercept)&quot;, -0.1132)</code></pre>
<pre class="output"><code>(&quot;Income&quot;, 0.0035)</code></pre>
<pre class="output"><code>(&quot;Temp&quot;, 0.0035)</code></pre>
<p>We can see that for every extra dollar of <code>Income</code> our customer consumes 0.003 pint (~1.47 mL) of ice cream more. Roughly the same change is produced by each additional grade (in Fahrenheit) of temperature. So, a simultaneous increase in <code>Income</code> by 1 USD and <code>Temp</code> by 1 unit translates into roughly 0.003 + 0.003 = 0.006 (~2.94 mL) greater consumption of ice cream per person. Now, (remember you were to imagine you are an ice cream truck owner) you could use the model to make predictions (with <code>Glm.predict</code> as we did in Section <a href="./pred_simple_lin_reg.html#sec:pred_simple_lin_reg">8.2</a>) to your benefit (e.g. by preparing enough product for your customers on a hot day).</p>
<p>So the time passes by and one sunny day when you open a bottle of beer a drunk genie pops out of it. To compensate you for the lost beer he offers to fulfill one wish. He won’t give you cash right away since you will not be able to explain it to the tax office. Instead, he will give you the ability to control either <code>Income</code> or <code>Temp</code> variable at will. That way you will get your money and none is the wiser. Which one do you choose, answer quickly, before the genie changes his mind.</p>
<p>Hmm, now that’s a dilemma, but judging by the coefficients above it seems it doesn’t make much of a difference (both <code>Coef.</code>s are roughly equal to 0.0035). Or does it? Well, the <code>Coef.</code>s are similar, but we are comparing incomparable, i.e. dollars (<code>Income</code>) with degrees Fahrenheit (<code>Temp</code>) and their influence on <code>Cons</code>. We may however, <a href="https://en.wikipedia.org/wiki/Standardized_coefficient">standardize the coefficients</a> to overcome the problem.</p>
<pre class="language-julia"><code># fn from ch04
# how many std. devs is a value above or below the mean
function getZScore(value::Real, mean::Real, sd::Real)::Float64
    return (value - mean)/sd
end

# adding new columns to the data frame
ice.ConsStand = getZScore.(
    ice.Cons, Stats.mean(ice.Cons), Stats.std(ice.Cons))
ice.IncomeStand = getZScore.(
    ice.Income, Stats.mean(ice.Income), Stats.std(ice.Income))
ice.TempStand = getZScore.(
    ice.Temp, Stats.mean(ice.Temp), Stats.std(ice.Temp))

iceMod2Stand = Glm.lm(
    Glm.@formula(ConsStand ~ IncomeStand + TempStand), ice)
iceMod2Stand</code></pre>
<pre class="output"><code>ConsStand ~ 1 + IncomeStand + TempStand

Coefficients:
────────────────────────────────────────────────────────────────────────────
                    Coef.  Std. Error      t  Pr(&gt;|t|)  Lower 95%  Upper 95%
────────────────────────────────────────────────────────────────────────────
(Intercept)  -7.92872e-17    0.103281  -0.00    1.0000  -0.211916   0.211916
IncomeStand   0.335122       0.111065   3.02    0.0055   0.107235   0.563009
TempStand     0.884442       0.111065   7.96    &lt;1e-07   0.656555   1.11233
────────────────────────────────────────────────────────────────────────────</code></pre>
<p>When expressed on the same scale (using <code>getZScore</code> function we met in Section <a href="./statistics_normal_distribution.html#sec:statistics_intro_distributions_package">4.6.2</a>) it becomes clear that the <code>Temp</code> (<code>Coef.</code> ~0.884) is a much more influential factor with regards to ice cream consumption (<code>Cons</code>) than <code>Income</code> (<code>Coef.</code> ~0.335). Therefore, we can be pretty sure that modifying the temperature by 1 standard deviation (which should not attract much attention) will bring you more money than modifying customers income by 1 standard deviation. Thanks genie.</p>
<p>Let’s look at another example of regression to get a better feel of it and discuss categorical variables and an interaction term in the model. We will operate on <a href="https://vincentarelbundock.github.io/Rdatasets/doc/HSAUR/agefat.html">agefat</a> data frame.</p>
<pre class="language-julia"><code>agefat = RD.dataset(&quot;HSAUR&quot;, &quot;agefat&quot;)
</code></pre>
<div id="tbl:agefatDf">
<table>
<caption>Table 16: Total body composition.</caption>
<thead>
<tr class="header">
<th style="text-align: right;">Age</th>
<th style="text-align: right;">Fat</th>
<th style="text-align: right;">Sex</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">24</td>
<td style="text-align: right;">15.5</td>
<td style="text-align: right;">male</td>
</tr>
<tr class="even">
<td style="text-align: right;">37</td>
<td style="text-align: right;">20.9</td>
<td style="text-align: right;">male</td>
</tr>
<tr class="odd">
<td style="text-align: right;">41</td>
<td style="text-align: right;">18.6</td>
<td style="text-align: right;">male</td>
</tr>
<tr class="even">
<td style="text-align: right;">60</td>
<td style="text-align: right;">28.0</td>
<td style="text-align: right;">male</td>
</tr>
<tr class="odd">
<td style="text-align: right;">31</td>
<td style="text-align: right;">34.7</td>
<td style="text-align: right;">female</td>
</tr>
</tbody>
</table>
</div>
<p>Here we are interested to predict body fat percentage (<code>Fat</code>) from the other two variables. Let’s get down to business.</p>
<pre class="language-julia"><code>agefatM1 = Glm.lm(Glm.@formula(Fat ~ Age + Sex), agefat)
agefatM1</code></pre>
<pre class="output"><code>Fat ~ 1 + Age + Sex

Coefficients:
───────────────────────────────────────────────────────────────────────────
                  Coef.  Std. Error      t  Pr(&gt;|t|)   Lower 95%  Upper 95%
───────────────────────────────────────────────────────────────────────────
(Intercept)   19.6479     4.10781     4.78    &lt;1e-04   11.1288    28.1669
Age            0.265563   0.0795324   3.34    0.0030    0.100623   0.430503
Sex: male    -10.5489     2.0914     -5.04    &lt;1e-04  -14.8862    -6.21161
───────────────────────────────────────────────────────────────────────────</code></pre>
<p>It appears that the older a person is the more fat it has (+0.27% of body fat per 1 extra year of age). Moreover, male subjects got smaller percentage of body fat (on average by 10.5%) than female individuals (this is to be expected: <a href="https://en.wikipedia.org/wiki/Body_fat_percentage">see here</a>). In the case of categorical variables the reference group is the one that comes first in the alphabet (here <code>female</code> is before <code>male</code>). The internals of the model assign 0 to the reference group and 1 to the other group. This yields us the formula: <span class="math inline">\(y = a + b*x + c*z\)</span> or <span class="math inline">\(Fat = a + b*Age + c*Sex\)</span>, where <code>Sex</code> is 0 for <code>female</code> and 1 for <code>male</code>. As before we can use this formula for prediction (either write one of our own or use <code>Glm.predict</code> we met before).</p>
<p>We may also want to fit a model with an interaction term to see if we gain some additional precision in our predictions.</p>
<pre class="language-julia"><code># or shortcut: Glm.@formula(Fat ~ Age * Sex)
agefatM2 = Glm.lm(Glm.@formula(Fat ~ Age + Sex + Age&amp;Sex), agefat)
agefatM2</code></pre>
<pre class="output"><code>Fat ~ 1 + Age + Sex + Age &amp; Sex

Coefficients:
────────────────────────────────────────────────────────────────────────────────
                      Coef.  Std. Error      t  Pr(&gt;|t|)    Lower 95%  Upper 95%
────────────────────────────────────────────────────────────────────────────────
(Intercept)       25.6686      5.32813    4.82    &lt;1e-04   14.5882     36.7491
Age                0.143685    0.105305   1.36    0.1869   -0.0753093   0.362678
Sex: male        -21.7625      6.96246   -3.13    0.0051  -36.2418     -7.28332
Age &amp; Sex: male    0.257462    0.153053   1.68    0.1073   -0.0608298   0.575753
────────────────────────────────────────────────────────────────────────────────</code></pre>
<p>Here, we do not have enough evidence that the interaction term (<code>Age &amp; Sex: male</code>) matters (p &gt; 0.05). Still, let’s explain what is this interaction in case you ever find one that is important. For that, take a look at the graph below.</p>
<figure>
<img src="./images/ch08agefat.png" id="fig:ch08agefat" alt="Figure 36: Body fat percentage vs. Age and Sex" /><figcaption aria-hidden="true">Figure 36: Body fat percentage vs. Age and Sex</figcaption>
</figure>
<p>As you can see the model without interaction fits two regression lines (one for each <code>Sex</code>) with different intercepts, but the same slopes. On the other hand, the model with interaction fits two regression lines (one for each <code>Sex</code>) with different intercepts and different slopes. Since the coefficient (<code>Coef.</code>) for the interaction term (<code>Age &amp; Sex: male</code>) is positive, this means that the slope for <code>Sex: male</code> is more steep (more positive).</p>
<p>So, when to use the interaction term in your model? The advice I heard was that in general, you should construct simple models and only use interaction when there are some good reasons for it. For instance, in the discussed case (<code>agefat</code> data frame), we might wanted to know if the accretion of body fat occurs faster in one of the genders as the people age.</p>
<p>To be continued …</p>


<div class="bottom-nav">
    <p id="nav-prev" style="text-align: left;">
        <a class="menu-level-2" href="./pred_simple_lin_reg.html"><b>8.2</b> Simple Linear Regression</a> <kbd>←</kbd>
        <span id="nav-next" style="float: right;">
            <kbd>→</kbd> <a class="menu-level-1" href="./appendix.html"><b></b> Appendix</a>
        </span>
    </p>
</div>


<div class="license">
    <br/>
  <br/>
  <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>
    Bartlomiej Lukaszuk
</div>
</div>
</div>
</body>
</html>