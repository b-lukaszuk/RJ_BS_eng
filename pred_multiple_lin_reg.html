<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Bartlomiej Lukaszuk" />
  <title>Multiple Linear Regression - Romeo and Julia, where Romeo is Basic Statistics</title>
  <link rel="stylesheet" href="./style.css"/>
    <script src="./mousetrap.min.js"></script>
    <style>
  @font-face {
    font-family: JuliaMono-Regular;
    src: url("./JuliaMono-Regular.woff2");
  }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <link rel="stylesheet" href="./github.min.css">
<script src="./highlight.min.js"></script>
<script src="./julia.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', (event) => {
    document.querySelectorAll('pre').forEach((el) => {
        if (!el.classList.contains('output')) {
            hljs.highlightElement(el);
        }
    });
});
</script>
 
</head>
<body>
<script>
function click_next() {
  var next = document.getElementById('nav-next');
  next.firstElementChild.nextElementSibling.click();
}
function click_prev() {
  var prev = document.getElementById('nav-prev');
  prev.firstElementChild.click();
}
Mousetrap.bind('right', click_next);
Mousetrap.bind('h', click_prev);
Mousetrap.bind('left', click_prev);
Mousetrap.bind('l', click_next);
</script>

<div class="books-container">
<aside class="books-menu">
<input type="checkbox" id="menu">
<label for="menu">☰</label>
<div class="books-title">
<a href="./">Romeo and Julia, where Romeo is Basic Statistics</a>
</div><br />
<span class="books-subtitle">

</span>
<div class="books-menu-content">
<li><a class="menu-level-1" href="./about.html"><b>1</b> About</a></li>
<li><a class="menu-level-1" href="./why_julia.html"><b>2</b> Why Julia</a></li>
<li><a class="menu-level-2" href="./julia_is_fast.html"><b>2.1</b> Julia is fast</a></li>
<li><a class="menu-level-2" href="./julia_is_simple.html"><b>2.2</b> Julia is simple</a></li>
<li><a class="menu-level-2" href="./jl_pleasure_to_write.html"><b>2.3</b> Pleasure to write</a></li>
<li><a class="menu-level-2" href="./jl_not_mainstream.html"><b>2.4</b> Not mainstream</a></li>
<li><a class="menu-level-2" href="./jl_open_source.html"><b>2.5</b> Julia is free</a></li>
<li><a class="menu-level-1" href="./julia_first_encounter.html"><b>3</b> Julia - first encounter</a></li>
<li><a class="menu-level-2" href="./julia_installation.html"><b>3.1</b> Installation</a></li>
<li><a class="menu-level-2" href="./julia_language_constructs.html"><b>3.2</b> Language Constructs</a></li>
<li><a class="menu-level-2" href="./julia_language_variables.html"><b>3.3</b> Variables</a></li>
<li><a class="menu-level-2" href="./julia_language_functions.html"><b>3.4</b> Functions</a></li>
<li><a class="menu-level-2" href="./julia_language_decision_making.html"><b>3.5</b> Decision Making</a></li>
<li><a class="menu-level-2" href="./julia_language_repetition.html"><b>3.6</b> Repetition</a></li>
<li><a class="menu-level-2" href="./julia_language_libraries.html"><b>3.7</b> Additional libraries</a></li>
<li><a class="menu-level-2" href="./julia_language_exercises.html"><b>3.8</b> Julia - Exercises</a></li>
<li><a class="menu-level-2" href="./julia_language_exercises_solutions.html"><b>3.9</b> Julia - Solutions</a></li>
<li><a class="menu-level-1" href="./statistics_intro.html"><b>4</b> Statistics - introduction</a></li>
<li><a class="menu-level-2" href="./statistics_intro_imports.html"><b>4.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./statistics_intro_probability_definition.html"><b>4.2</b> Probability - definition</a></li>
<li><a class="menu-level-2" href="./statistics_intro_probability_properties.html"><b>4.3</b> Probability - properties</a></li>
<li><a class="menu-level-2" href="./statistics_prob_theor_practice.html"><b>4.4</b> Probability - theory and..</a></li>
<li><a class="menu-level-2" href="./statistics_prob_distribution.html"><b>4.5</b> Probability distribution</a></li>
<li><a class="menu-level-2" href="./statistics_normal_distribution.html"><b>4.6</b> Normal distribution</a></li>
<li><a class="menu-level-2" href="./statistics_intro_hypothesis_testing.html"><b>4.7</b> Hypothesis testing</a></li>
<li><a class="menu-level-2" href="./statistics_intro_exercises.html"><b>4.8</b> Statistics intro - Exerc..</a></li>
<li><a class="menu-level-2" href="./statistics_intro_exercises_solutions.html"><b>4.9</b> Statistics intro - Solut..</a></li>
<li><a class="menu-level-1" href="./compare_contin_data.html"><b>5</b> Comparisons - continuous d..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_imports.html"><b>5.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_one_samp_ttest.html"><b>5.2</b> One sample Student’s t..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_two_samp_ttest.html"><b>5.3</b> Two samples Student’s ..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_one_way_anova.html"><b>5.4</b> One-way ANOVA</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_post_hoc_tests.html"><b>5.5</b> Post-hoc tests</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_multip_correction.html"><b>5.6</b> Multiplicity correction</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_exercises.html"><b>5.7</b> Exercises - Comparisons ..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_exercises_solutions.html"><b>5.8</b> Solutions - Comparisons ..</a></li>
<li><a class="menu-level-1" href="./compare_categ_data.html"><b>6</b> Comparisons - categorical ..</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_imports.html"><b>6.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_flashback.html"><b>6.2</b> Flashback</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_chisq_test.html"><b>6.3</b> Chi squared test</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_fisher_exact_text.html"><b>6.4</b> Fisher’s exact test</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_bigger_table.html"><b>6.5</b> Bigger table</a></li>
<li><a class="menu-level-2" href="./compare_categ_test_for_independence.html"><b>6.6</b> Test for independence</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_exercises.html"><b>6.7</b> Exercises - Comparisons ..</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_exercises_solutions.html"><b>6.8</b> Solutions - Comparisons ..</a></li>
<li><a class="menu-level-1" href="./association.html"><b>7</b> Association</a></li>
<li><a class="menu-level-2" href="./association_imports.html"><b>7.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./association_lin_relation.html"><b>7.2</b> Linear relation</a></li>
<li><a class="menu-level-2" href="./association_covariance.html"><b>7.3</b> Covariance</a></li>
<li><a class="menu-level-2" href="./association_correlation.html"><b>7.4</b> Correlation</a></li>
<li><a class="menu-level-2" href="./association_corr_pitfalls.html"><b>7.5</b> Correlation Pitfalls</a></li>
<li><a class="menu-level-2" href="./association_exercises.html"><b>7.6</b> Exercises - Association</a></li>
<li><a class="menu-level-2" href="./association_exercises_solutions.html"><b>7.7</b> Solutions - Association</a></li>
<li><a class="menu-level-1" href="./prediction.html"><b>8</b> Prediction</a></li>
<li><a class="menu-level-2" href="./prediction_imports.html"><b>8.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./pred_simple_lin_reg.html"><b>8.2</b> Simple Linear Regression</a></li>
<li><a class="menu-level-2" href="./pred_multiple_lin_reg.html"><b>8.3</b> Multiple Linear Regressi..</a></li>
<li><a class="menu-level-1" href="./appendix.html"><b></b> Appendix</a></li>
<li><a class="menu-level-1" href="./references.html"><b>9</b> References</a></li>
</div>
</aside>

<div class="books-content">
<h2 data-number="8.3" id="sec:pred_multiple_lin_reg"><span class="header-section-number">8.3</span> Multiple Linear Regression</h2>
<p>Multiple linear regression is a linear regression with more than one predictor variables. Take look at the <a href="https://vincentarelbundock.github.io/Rdatasets/doc/Ecdat/Icecream.html">Icecream</a> data frame.</p>
<pre class="language-julia"><code>ice = RD.dataset(&quot;Ecdat&quot;, &quot;Icecream&quot;)
first(ice, 5)
</code></pre>
<div id="tbl:icecreamDf">
<table>
<caption>Table 15: Icecream consumption data.</caption>
<thead>
<tr class="header">
<th style="text-align: right;">Cons</th>
<th style="text-align: right;">Income</th>
<th style="text-align: right;">Price</th>
<th style="text-align: right;">Temp</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.386</td>
<td style="text-align: right;">78.0</td>
<td style="text-align: right;">0.27</td>
<td style="text-align: right;">41.0</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.374</td>
<td style="text-align: right;">79.0</td>
<td style="text-align: right;">0.282</td>
<td style="text-align: right;">56.0</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.393</td>
<td style="text-align: right;">81.0</td>
<td style="text-align: right;">0.277</td>
<td style="text-align: right;">63.0</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.425</td>
<td style="text-align: right;">80.0</td>
<td style="text-align: right;">0.28</td>
<td style="text-align: right;">68.0</td>
</tr>
<tr class="odd">
<td style="text-align: right;">0.406</td>
<td style="text-align: right;">76.0</td>
<td style="text-align: right;">0.272</td>
<td style="text-align: right;">69.0</td>
</tr>
</tbody>
</table>
</div>
<p>We got 4 columns altogether (more detail in the link above):</p>
<ul>
<li><code>Cons</code> - consumption of ice cream (pints),</li>
<li><code>Income</code> - average family income (USD),</li>
<li><code>Price</code> - price of ice cream (USD),</li>
<li><code>Temp</code> - average temperature (Fahrenheit)</li>
</ul>
<p>Imagine you are an ice cream truck owner and are interested to know which factors influence (and in what way) the consumption (<code>Cons</code>) of ice-cream by your customers. Let’s start by building a model with all the possible explanatory variables.</p>
<pre class="language-julia"><code>iceMod1 = Glm.lm(Glm.@formula(Cons ~ Income + Price + Temp), ice)
iceMod1</code></pre>
<pre class="output"><code>Cons ~ 1 + Income + Price + Temp

Coefficients:
────────────────────────────────────────────────────────────────────────────────
                   Coef.   Std. Error      t  Pr(&gt;|t|)     Lower 95%   Upper 95%
────────────────────────────────────────────────────────────────────────────────
(Intercept)   0.197315    0.270216      0.73    0.4718  -0.358122     0.752752
Income        0.00330776  0.00117142    2.82    0.0090   0.000899875  0.00571565
Price        -1.04441     0.834357     -1.25    0.2218  -2.75946      0.670632
Temp          0.00345843  0.000445547   7.76    &lt;1e-07   0.00254259   0.00437426
────────────────────────────────────────────────────────────────────────────────</code></pre>
<p>Right away we can see that the price of ice-cream negatively affects (<code>Coef.</code> = -1.044) the volume of ice cream consumed (the more expensive the ice cream is the less people eat it, 1.044 pint less for every additional USD of price). The relationship is in line with out intuition. However, there is not enough evidence (p &gt; 0.05) that the real influence of <code>Price</code> on consumption isn’t 0 (so no influence). Therefore, you wonder should you perhaps remove the variable <code>Price</code> from the model like so</p>
<pre class="language-julia"><code>iceMod2 = Glm.lm(Glm.@formula(Cons ~ Income + Temp), ice)
iceMod2</code></pre>
<pre class="output"><code>Cons ~ 1 + Income + Temp

Coefficients:
───────────────────────────────────────────────────────────────────────────────
                   Coef.   Std. Error      t  Pr(&gt;|t|)    Lower 95%   Upper 95%
───────────────────────────────────────────────────────────────────────────────
(Intercept)  -0.113195    0.10828      -1.05    0.3051  -0.335367    0.108977
Income        0.00353017  0.00116996    3.02    0.0055   0.00112961  0.00593072
Temp          0.00354331  0.000444956   7.96    &lt;1e-07   0.00263033  0.00445628
───────────────────────────────────────────────────────────────────────────────</code></pre>
<p>Now, we got <code>Income</code> and <code>Temp</code> in our model, both of which are statistically significant. The values of <code>Coef.</code>s for <code>Income</code> and <code>Temp</code> somewhat changed between the models, but such changes (and even greater) are to be expected. Still, we would like to know if our new <code>iceMod2</code> is really better than <code>iceMod1</code> that we came up with before.</p>
<p>In our first try to solve the problem we could resort to the coefficient of determination (<span class="math inline">\(r^2\)</span>) that we met in Section <a href="./pred_simple_lin_reg.html#sec:pred_simple_lin_reg">8.2</a>. Intuition tells us that a better model should have a bigger <span class="math inline">\(r^2\)</span>.</p>
<pre class="language-julia"><code>round.([Glm.r2(iceMod1), Glm.r2(iceMod2)],
    digits = 3)</code></pre>
<pre class="output"><code>[0.719, 0.702]</code></pre>
<p>Hmm, <span class="math inline">\(r^2\)</span> is bigger for <code>iceMod1</code> than <code>iceMod2</code>. However, there are two problems with it: 1) the difference between the coefficients is quite small, and 2) <span class="math inline">\(r^2\)</span> gets easily inflated by any additional variable in the model. And I mean any, if you add, let’s say 10 random variables to the <code>ice</code> data frame and put them into model the coefficient of determination will go up even though this makes no sense (we know their real influence is 0). That is why we got an improved metrics called the adjusted coefficient of determination. This parameter (adj. <span class="math inline">\(r^2\)</span>) penalizes for every additional variable added to the model. Therefore the ‘noise’ variables will lower the adjusted <span class="math inline">\(r^2\)</span> whereas only truly impactful ones will be able to raise it.</p>
<pre class="language-julia"><code>round.([Glm.adjr2(iceMod1), Glm.adjr2(iceMod2)],
    digits = 3)</code></pre>
<pre class="output"><code>[0.687, 0.68]</code></pre>
<p><code>iceMod1</code> still explains more variability in <code>Cons</code> (ice cream consumption) but the magnitude of the difference dropped. This makes our decision even harder. Luckily, <code>Glm</code> has <code>ftest</code> function to help us determine if one model is significantly better than the other.</p>
<pre class="language-julia"><code>Glm.ftest(iceMod1.model, iceMod2.model)</code></pre>
<pre class="output"><code>F-test: 2 models fitted on 30 observations
───────────────────────────────────────────────────────────────
     DOF  ΔDOF     SSR    ΔSSR      R²      ΔR²      F*   p(&gt;F)
───────────────────────────────────────────────────────────────
[1]    5        0.0353          0.7190                         
[2]    4    -1  0.0374  0.0021  0.7021  -0.0169  1.5669  0.2218
───────────────────────────────────────────────────────────────</code></pre>
<p>The table contains two rows:</p>
<ul>
<li><code>[1]</code> - first model from the left (in <code>Glm.ftest</code> argument list)</li>
<li><code>[2]</code> - second model from the left (in <code>Glm.ftest</code> argument list)</li>
</ul>
<p>and a few columns:</p>
<ul>
<li><code>DOF</code> - degrees of freedom (more elements in formula, bigger <code>DOF</code>)</li>
<li><code>ΔDOF</code> - <code>DOF[1]</code> - <code>DOF[2]</code></li>
<li><code>SSR</code> - residual sum of squares (the smaller the better)</li>
<li><code>ΔSSR</code> - <code>SSR[1]</code> - <code>SSR[2]</code></li>
<li><code>R2</code> - coefficient of determination</li>
<li><code>ΔR2</code> - <code>R2[1]</code> - <code>R2[2]</code></li>
<li><code>F*</code> - F-Statistic (similar to the one we met in Section <a href="./compare_contin_data_one_way_anova.html#sec:compare_contin_data_one_way_anova">5.4</a>)</li>
<li><code>p(&gt;F)</code> - p-value for the comparison between the two models</li>
</ul>
<p>Based on the test we see that none of the models is clearly better from the other (p &gt; 0.05). Therefore, in line with <a href="https://en.wikipedia.org/wiki/Occam%27s_razor">Occam’s razor</a> principle (when two equally good explanations exist, choose the simpler one) we can safely pick <code>iceMod2</code> as our final model.</p>
<p>What we did here was the construction of a so called minimal adequate model (the smallest model that explains the greatest amount of variance in the dependent/outcome variable). We did this using top to bottom approach. We started with a ‘full’ model. Then we follow by removing explanatory variables (one by one) that do not contribute to the model (we start from highest p-value above 0.05) until only meaningful explanatory variables remain. The removal of the variables reflects our common sense, because usually we (or others that will use our model) do not want to spend time/money/energy on collecting data that are of no use for us.</p>
<p>OK, let’s inspect our minimal adequate model again.</p>
<pre class="language-julia"><code>[(cn, round(c, digits = 4)) for (cn, c) in
     zip(Glm.coefnames(iceMod2), Glm.coef(iceMod2))]</code></pre>
<pre class="output"><code>(&quot;(Intercept)&quot;, -0.1132)</code></pre>
<pre class="output"><code>(&quot;Income&quot;, 0.0035)</code></pre>
<pre class="output"><code>(&quot;Temp&quot;, 0.0035)</code></pre>
<p>We can see that for every extra dollar of <code>Income</code> our customer consumes 0.003 pint (~1.47 mL) of ice cream more. Roughly the same change is produced by each additional grade (in Fahrenheit) of temperature. So, a simultaneous increase in <code>Income</code> by 1 USD and <code>Temp</code> by 1 unit translates into roughly 0.003 + 0.003 = 0.006 (~2.94 mL) greater consumption of ice cream per person. Now, (remember you were to imagine you are an ice cream truck owner) you could use the model to make predictions (with <code>Glm.predict</code> as we did in Section <a href="./pred_simple_lin_reg.html#sec:pred_simple_lin_reg">8.2</a>) to your benefit (e.g. by preparing enough product for your customers on a hot day).</p>
<p>So the time passes by and one sunny day when you open a bottle of beer a drunk genie pops out of it. To compensate you for the lost beer he offers to fulfill one wish. He won’t give you cash right away since you will not be able to explain it to the tax office. Instead, he will give you the ability to control either <code>Income</code> or <code>Temp</code> variable at will. That way you will get your money and none is the wiser. Which one do you choose, answer quickly, before the genie changes his mind.</p>
<p>Hmm, now that’s a dilemma, but judging by the coefficients above it seems it doesn’t make much of a difference (both <code>Coef.</code>s are roughly equal to 0.0035). Or does it? Well, the <code>Coef.</code>s are similar, but we are comparing incomparable, i.e. dollars (<code>Income</code>) with degrees Fahrenheit (<code>Temp</code>) and their influence on <code>Cons</code>. We may however, <a href="https://en.wikipedia.org/wiki/Standardized_coefficient">standardize the coefficients</a> to overcome the problem.</p>
<pre class="language-julia"><code># fn from ch04
# how many std. devs is value above or below the mean
function getZScore(value::Real, mean::Real, sd::Real)::Float64
    return (value - mean)/sd
end

# adding new columns to the data frame
ice.ConsStand = getZScore.(
    ice.Cons, Stats.mean(ice.Cons), Stats.std(ice.Cons))
ice.IncomeStand = getZScore.(
    ice.Income, Stats.mean(ice.Income), Stats.std(ice.Income))
ice.TempStand = getZScore.(
    ice.Temp, Stats.mean(ice.Temp), Stats.std(ice.Temp))

iceMod2Stand = Glm.lm(
    Glm.@formula(ConsStand ~ IncomeStand + TempStand), ice)
iceMod2Stand</code></pre>
<pre class="output"><code>ConsStand ~ 1 + IncomeStand + TempStand

Coefficients:
────────────────────────────────────────────────────────────────────────────
                    Coef.  Std. Error      t  Pr(&gt;|t|)  Lower 95%  Upper 95%
────────────────────────────────────────────────────────────────────────────
(Intercept)  -7.92872e-17    0.103281  -0.00    1.0000  -0.211916   0.211916
IncomeStand   0.335122       0.111065   3.02    0.0055   0.107235   0.563009
TempStand     0.884442       0.111065   7.96    &lt;1e-07   0.656555   1.11233
────────────────────────────────────────────────────────────────────────────</code></pre>
<p>When expressed on the same scale (using <code>getZScore</code> function we met in Section <a href="./statistics_normal_distribution.html#sec:statistics_intro_distributions_package">4.6.2</a>) it becomes clear that the <code>Temp</code> (<code>Coef.</code> ~0.884) is a much more influential factor with regards to ice cream consumption (<code>Cons</code>) than <code>Income</code> (<code>Coef.</code> ~0.335). Therefore, we can be pretty sure that modifying the temperature by 1 standard deviation (which should not attract much attention) will bring you more money than modifying customers income by 1 standard deviation. Thanks genie.</p>
<p>To be continued …</p>


<div class="bottom-nav">
    <p id="nav-prev" style="text-align: left;">
        <a class="menu-level-2" href="./pred_simple_lin_reg.html"><b>8.2</b> Simple Linear Regression</a> <kbd>←</kbd>
        <span id="nav-next" style="float: right;">
            <kbd>→</kbd> <a class="menu-level-1" href="./appendix.html"><b></b> Appendix</a>
        </span>
    </p>
</div>


<div class="license">
    <br/>
  <br/>
  <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>
    Bartlomiej Lukaszuk
</div>
</div>
</div>
</body>
</html>