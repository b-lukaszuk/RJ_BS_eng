<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Bartlomiej Lukaszuk" />
  <title>Solutions - Comparisons of Continuous Data - Romeo and Julia</title>
  <link rel="stylesheet" href="./style.css"/>
    <script src="./mousetrap.min.js"></script>
    <style>
  @font-face {
    font-family: JuliaMono-Regular;
    src: url("./JuliaMono-Regular.woff2");
  }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <link rel="stylesheet" href="./github.min.css">
<script src="./highlight.min.js"></script>
<script src="./julia.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', (event) => {
    document.querySelectorAll('pre').forEach((el) => {
        if (!el.classList.contains('output')) {
            hljs.highlightElement(el);
        }
    });
});
</script>
 
</head>
<body>
<script>
function click_next() {
  var next = document.getElementById('nav-next');
  next.firstElementChild.nextElementSibling.click();
}
function click_prev() {
  var prev = document.getElementById('nav-prev');
  prev.firstElementChild.click();
}
Mousetrap.bind('right', click_next);
Mousetrap.bind('h', click_prev);
Mousetrap.bind('left', click_prev);
Mousetrap.bind('l', click_next);
</script>

<div class="books-container">
<aside class="books-menu">
<input type="checkbox" id="menu">
<label for="menu">☰</label>
<div class="books-title">
<a href="./">Romeo and Julia</a>
</div><br />
<span class="books-subtitle">
where Romeo is Basic Statistics
</span>
<div class="books-menu-content">
<li><a class="menu-level-1" href="./about.html"><b>1</b> About</a></li>
<li><a class="menu-level-1" href="./why_julia.html"><b>2</b> Why Julia</a></li>
<li><a class="menu-level-2" href="./julia_is_fast.html"><b>2.1</b> Julia is fast</a></li>
<li><a class="menu-level-2" href="./julia_is_simple.html"><b>2.2</b> Julia is simple</a></li>
<li><a class="menu-level-2" href="./jl_pleasure_to_write.html"><b>2.3</b> Pleasure to write</a></li>
<li><a class="menu-level-2" href="./jl_not_mainstream.html"><b>2.4</b> Not mainstream</a></li>
<li><a class="menu-level-2" href="./jl_open_source.html"><b>2.5</b> Julia is free</a></li>
<li><a class="menu-level-1" href="./julia_first_encounter.html"><b>3</b> Julia - first encounter</a></li>
<li><a class="menu-level-2" href="./julia_installation.html"><b>3.1</b> Installation</a></li>
<li><a class="menu-level-2" href="./julia_language_constructs.html"><b>3.2</b> Language Constructs</a></li>
<li><a class="menu-level-2" href="./julia_language_variables.html"><b>3.3</b> Variables</a></li>
<li><a class="menu-level-2" href="./julia_language_functions.html"><b>3.4</b> Functions</a></li>
<li><a class="menu-level-2" href="./julia_language_decision_making.html"><b>3.5</b> Decision Making</a></li>
<li><a class="menu-level-2" href="./julia_language_repetition.html"><b>3.6</b> Repetition</a></li>
<li><a class="menu-level-2" href="./julia_language_libraries.html"><b>3.7</b> Additional libraries</a></li>
<li><a class="menu-level-2" href="./julia_language_exercises.html"><b>3.8</b> Julia - Exercises</a></li>
<li><a class="menu-level-2" href="./julia_language_exercises_solutions.html"><b>3.9</b> Julia - Solutions</a></li>
<li><a class="menu-level-1" href="./statistics_intro.html"><b>4</b> Statistics - introduction</a></li>
<li><a class="menu-level-2" href="./statistics_intro_imports.html"><b>4.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./statistics_intro_probability_definition.html"><b>4.2</b> Probability - definition</a></li>
<li><a class="menu-level-2" href="./statistics_intro_probability_properties.html"><b>4.3</b> Probability - properties</a></li>
<li><a class="menu-level-2" href="./statistics_prob_theor_practice.html"><b>4.4</b> Probability - theory and..</a></li>
<li><a class="menu-level-2" href="./statistics_prob_distribution.html"><b>4.5</b> Probability distribution</a></li>
<li><a class="menu-level-2" href="./statistics_normal_distribution.html"><b>4.6</b> Normal distribution</a></li>
<li><a class="menu-level-2" href="./statistics_intro_hypothesis_testing.html"><b>4.7</b> Hypothesis testing</a></li>
<li><a class="menu-level-2" href="./statistics_intro_exercises.html"><b>4.8</b> Statistics intro - Exerc..</a></li>
<li><a class="menu-level-2" href="./statistics_intro_exercises_solutions.html"><b>4.9</b> Statistics intro - Solut..</a></li>
<li><a class="menu-level-1" href="./compare_contin_data.html"><b>5</b> Comparisons - continuous d..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_imports.html"><b>5.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_one_samp_ttest.html"><b>5.2</b> One sample Student’s t..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_two_samp_ttest.html"><b>5.3</b> Two samples Student’s ..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_one_way_anova.html"><b>5.4</b> One-way ANOVA</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_post_hoc_tests.html"><b>5.5</b> Post-hoc tests</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_multip_correction.html"><b>5.6</b> Multiplicity correction</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_exercises.html"><b>5.7</b> Exercises - Comparisons ..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_exercises_solutions.html"><b>5.8</b> Solutions - Comparisons ..</a></li>
<li><a class="menu-level-1" href="./appendix.html"><b></b> Appendix</a></li>
<li><a class="menu-level-1" href="./references.html"><b>6</b> References</a></li>
</div>
</aside>

<div class="books-content">
<h2 data-number="5.8" id="sec:compare_contin_data_exercises_solutions"><span class="header-section-number">5.8</span> Solutions - Comparisons of Continuous Data</h2>
<p>In this sub-chapter you will find exemplary solutions to the exercises from the previous section.</p>
<h3 data-number="5.8.1" id="sec:compare_contin_data_ex1_solution"><span class="header-section-number">5.8.1</span> Solution to Exercise 1</h3>
<p>First the sample and the 100’000 simulations:</p>
<pre class="language-julia"><code>Rand.seed!(321)
ex1sample = Rand.rand(Dsts.Normal(80, 20), 10)
ex1sampleSd = Stats.std(ex1sample)
ex1sampleSem = getSem(ex1sample)
ex1sampleMeans = [
    Stats.mean(Rand.rand(Dsts.Normal(80, 20), 10))
    for _ in 1:100_000]
ex1sampleMeansMean = Stats.mean(ex1sampleMeans)
ex1sampleMeansSd = Stats.std(ex1sampleMeans)</code></pre>
<p>The code doesn’t contain any new elements, so I will leave it to you to figure out what happened in the code snippet above.</p>
<p>And now, let’s move to the plot.</p>
<pre class="language-julia"><code>fig = Cmk.Figure()
Cmk.hist(fig[1, 1], ex1sampleMeans, bins=100, color=Cmk.RGBAf(0, 0, 1, 0.3),
    axis=(;
        title=&quot;Histogram of 100&#39;000 sample means&quot;,
        xlabel=&quot;Adult human body weight [kg]&quot;,
        ylabel=&quot;Count&quot;))
Cmk.ylims!(0, 4000)
Cmk.vlines!(fig[1, 1], 80,
    ymin=0.0, ymax=0.85, color=&quot;black&quot;, linestyle=:dashdot)
Cmk.text!(fig[1, 1], 81, 1000, text=&quot;population mean = 80&quot;)
Cmk.bracket!(fig[1, 1],
    ex1sampleMeansMean - ex1sampleMeansSd / 2, 3500,
    ex1sampleMeansMean + ex1sampleMeansSd / 2, 3500,
    style=:square
)
Cmk.text!(fig[1, 1], 72.5, 3700,
    text=&quot;sample means sd = 6.33&quot;)
Cmk.text!(fig[1, 1], 90, 3200,
    text=&quot;sample sd = 17.32&quot;)
Cmk.text!(fig[1, 1], 90, 3000,
    text=&quot;sample sd = 5.48&quot;)
fig</code></pre>
<p>This produces the following graph.</p>
<figure>
<img src="./images/histCh05Ex1.png" id="fig:histCh05Ex1" alt="Figure 17: Histogram of drawing 100’000 random samples from a population with \mu = 80 and \sigma = 20." /><figcaption aria-hidden="true">Figure 17: Histogram of drawing 100’000 random samples from a population with <span class="math inline">\(\mu = 80\)</span> and <span class="math inline">\(\sigma = 20\)</span>.</figcaption>
</figure>
<p>The graph clearly demonstrates that a better approximation of the samples means sd is <code>sem</code> and not <code>sd</code> (as stated in Section <a href="./compare_contin_data_one_samp_ttest.html#sec:compare_contin_data_one_samp_ttest">5.2</a>).</p>
<p>I’m not gonna explain the code snippet above in great detail since this is a warm up exercise, and <a href="https://docs.makie.org/stable/tutorials/">the tutorials</a> (e.g. the basic tutorial) and the documentation for the plotting functions (see the links in Section <a href="./compare_contin_data_exercises.html#sec:compare_contin_data_ex1">5.7.1</a>) are pretty good. Moreover, we already used <code>CairoMakie</code> plotting functions in Section <a href="./statistics_prob_distribution.html#sec:statistics_prob_distribution">4.5</a>. Still, a few quick notes are in order.</p>
<p>First of all, drawing a graph like that is not an enormous feat, you just need some knowledge (you read the tutorial and the function docs, right?). The rest is just patience and replication of the examples. Ah yes, I forgot about try and error (that happens from time to time in my case). If an error happens, do not panic try to read the error’s message and think what it tells you).</p>
<p>It is always a good idea to annotate the graph, add the title, x- and y-axis labels (to make the reader’s, and your own, reasoning easier). Figures are developed from top to bottom (in the code), layer after layer (top line of code -&gt; bottom layer, next line of code places a layer above the previous layer). First function (<code>fig</code> and <code>Cmk.hist</code>) creates the figure, the following functions (e.g. <code>Cmk.text!</code> and <code>Cmk.vlines</code>), write/paint something on the previous layers. After some time and tweaking you should be able to produce quite pleasing figures (just remember, patience is the key). One more point, instead of typing strings by hand (like <code>text="sample sd = 17.32"</code>) you may let Julia do that by using <a href="https://docs.julialang.org/en/v1/manual/strings/#string-interpolation">strings interpolation</a>, like <code>text="sample sd = $(round(ex1sampleSd, digits=2))"</code>(with time you will appreciate the convenience of this method).</p>
<h3 data-number="5.8.2" id="sec:compare_contin_data_ex2_solution"><span class="header-section-number">5.8.2</span> Solution to Exercise 2</h3>
<p>First let’s start with the functions we developed in Section <a href="./statistics_intro.html#sec:statistics_intro">4</a> (and its subsections). We already now them, so I will not explain them here.</p>
<pre class="language-julia"><code>function getCounts(v::Vector{T})::Dict{T,Int} where {T}
    counts::Dict{T,Int} = Dict()
    for elt in v
        counts[elt] = get(counts, elt, 0) + 1
    end
    return counts
end

function getProbs(counts::Dict{T,Int})::Dict{T,Float64} where {T}
    total::Int = sum(values(counts))
    return Dict(k =&gt; v / total for (k, v) in counts)
end

function getSortedKeysVals(d::Dict{T1,T2})::Tuple{
    Vector{T1},Vector{T2}} where {T1,T2}

    sortedKeys::Vector{T1} = keys(d) |&gt; collect |&gt; sort
    sortedVals::Vector{T2} = [d[k] for k in sortedKeys]
    return (sortedKeys, sortedVals)
end</code></pre>
<p>Now, time to define <code>getLstatistic</code> based on what we learned in Section <a href="./compare_contin_data_one_way_anova.html#sec:compare_contin_data_one_way_anova">5.4</a> (note, the function uses <code>getAbsGroupDiffsAroundOverallMean</code> and <code>getAbsPointDiffsFromGroupMeans</code> that we developed in that section).</p>
<pre class="language-julia"><code>function getLStatistic(v1::Vector{&lt;:Real}, v2::Vector{&lt;:Real})::Float64
    absDiffsOverallMean::Vector{&lt;:Real} =
        getAbsGroupDiffsFromOverallMean(v1, v2)
    absDiffsGroupMean::Vector{&lt;:Real} =
        getAbsPointDiffsFromGroupMeans(v1, v2)
    return Stats.mean(absDiffsOverallMean) / Stats.mean(absDiffsGroupMean)
end</code></pre>
<p>OK, that was easy, after all we practically did it all before, we only needed to look for it in the previous chapters. Now, the function to determine the distribution.</p>
<pre class="language-julia"><code>function getLStatisticsUnderH0(
    popMean::Real, popSd::Real,
    nPerGroup::Int=4, nIter::Int=1_000_000)::Vector{Float64}

    v1::Vector{Float64} = []
    v2::Vector{Float64} = []
    result::Vector{Float64} = zeros(nIter)

    for i in 1:nIter
        v1 = Rand.rand(Dsts.Normal(popMean, popSd), nPerGroup)
        v2 = Rand.rand(Dsts.Normal(popMean, popSd), nPerGroup)
        result[i] = getLStatistic(v1, v2)
    end

    return result
end</code></pre>
<p>This one is slightly more complicated so I think a bit of explanation is in order here. First we initialize some variables that we will use later. For instance, <code>v1</code> and <code>v2</code> will hold random samples drawn from a population of interest (<code>Dsts.Normal(popMean, popSd)</code>) and will change with each iteration. The vector <code>result</code> is initialized with <code>0</code>s and will hold the <code>LStatistic</code> calculated during each iteration for <code>v1</code> and <code>v2</code>. The result vector is returned by the function. Later on we will be able to use it to <code>getCounts</code> and <code>getProbs</code> for the L-Statistics. This should work just fine. However, if we slightly modify our function (<code>getLStatisticsUnderH0</code>), we could use it not only with the L-Statistic but also F-Statistic (optional points in this task) or any other statistic of interest. Observe</p>
<pre class="language-julia"><code># getXStatFn signature: fnName(::Vector{&lt;:Real}, ::Vector{&lt;:Real})::Float64
function getXStatisticsUnderH0(
    getXStatFn::Function,
    popMean::Real, popSd::Real,
    nPerGroup::Int=4, nIter::Int=1_000_000)::Vector{Float64}

    v1::Vector{Float64} = []
    v2::Vector{Float64} = []
    result::Vector{Float64} = zeros(nIter)

    for i in 1:nIter
        v1 = Rand.rand(Dsts.Normal(popMean, popSd), nPerGroup)
        v2 = Rand.rand(Dsts.Normal(popMean, popSd), nPerGroup)
        result[i] = getXStatFn(v1, v2)
    end

    return result
end</code></pre>
<p>Here, instead of <code>getLStatisticsUnderH0</code> we named the function <code>getXStatisticsUnderH0</code>, where <code>X</code> is any statistic we can come up with. The function that calculates our statistic of interest is passed as a first argument to <code>getXStatisticsUnderH0</code> (<code>getXStatFn</code>). The <code>getXStatFn</code> should work just fine, if it accepts two vectors (<code>::Vector{&lt;:Real}</code>) and returns <code>Float64</code> (the statistic) of interest. Both those assumptions are fulfilled by <code>getLStatistic</code> (defined above) and <code>getFStatistic</code> defined in Section <a href="./compare_contin_data_one_way_anova.html#sec:compare_contin_data_one_way_anova">5.4</a>. To use our <code>getXStatisticsUnderH0</code> we would type, e.g.: <code>getXStatisticsUnderH0(getLStatistic, 25, 3, 4)</code> instead of <code>getLStatisticsUnderH0(25, 3, 4)</code> that we would have used for <code>getLStatisticsUnderH0</code> defined above (more typing, but greater flexibility, and the result would be the same).</p>
<p>Now, to get a distribution of interest we use the following function</p>
<pre class="language-julia"><code># getXStatFn signature: fnName(::Vector{&lt;:Real}, ::Vector{&lt;:Real})::Float64
function getXDistUnderH0(getXStatFn::Function,
    mean::Real, sd::Real,
    nPerGroup::Int=4)::Dict{Float64,Float64}

    xStats::Vector{&lt;:Float64} = getXStatisticsUnderH0(
        getXStatFn, mean, sd, nPerGroup)
    xStats = round.(xStats, digits=1)
    xCounts::Dict{Float64,Int} = getCounts(xStats)
    xProbs::Dict{Float64,Float64} = getProbs(xCounts)

    return xProbs
end</code></pre>
<p>First, we calculate the statistics of interest (<code>xStats</code>), then we round the statistics to a 1 decimal point (<code>round.(xStats, digits=1)</code>). This is necessary, since in a moment we will use <code>getCounts</code> so we need some repetitions in our <code>xStats</code> vector (e.g. 1.283333331 and 1.283333332 will, both get rounded to 1.3 and the count for this value of the statistic will be 2). Once we got the counts, we change them to probabilities (fraction of times that the given value of the statistic occurred) with <code>getProbs</code>.</p>
<p>Now we can finally, use them to estimate the probability that the L-statistic greater than <code>LStatisticEx2</code> = 1.28 occurred by chance.</p>
<pre class="language-julia"><code>Rand.seed!(321)
lprobs = getXDistUnderH0(getLStatistic, 25, 3)
lprobsGTLStatisticEx2 = [v for (k, v) in lprobs if k &gt; LStatisticEx2]
lStatProb = sum(lprobsGTLStatisticEx2)</code></pre>
<p>0.045378999999999996</p>
<p>The estimated probability for our L-Statistic is 0.045 which is pretty close to the probability obtained for the F-Statistic (<code>Htests.OneWayANOVATest(ex2BwtsWater, ex2BwtsDrugY) |&gt; Htests.pvalue</code> = 0.043) (and well it should).</p>
<p>OK, now it’s time to draw some plots. First, let’s get the values for x- and y-axes</p>
<pre class="language-julia"><code>Rand.seed!(321)
# L distributions
lxs1, lys1 = getXDistUnderH0(getLStatistic, 25, 3) |&gt; getSortedKeysVals
lxs2, lys2 = getXDistUnderH0(getLStatistic, 100, 50) |&gt; getSortedKeysVals
lxs3, lys3 = getXDistUnderH0(getLStatistic, 25, 3, 8) |&gt; getSortedKeysVals
# F distribution
fxs1, fys1 = getXDistUnderH0(getFStatistic, 25, 3) |&gt; getSortedKeysVals</code></pre>
<p>No, big deal L-Distributions start with <code>l</code>, the classical F-Distribution starts with <code>f</code>. BTW. Notice that thanks to <code>getXDistUnderH0</code> we didn’t have to write two almost identical functions (<code>getLDistUnderH0</code> and <code>getFDistUnderH0</code>).</p>
<p>OK, let’s place them on the graph</p>
<pre class="language-julia"><code>fig = Cmk.Figure()
ax1, l1 = Cmk.lines(fig[1, 1], fxs1, fys1, color=&quot;red&quot;,
    axis=(;
        title=&quot;F-Distribution (red) and L-Distribution (blue)&quot;,
        xlabel=&quot;Value of the statistic&quot;,
        ylabel=&quot;Probability distribution&quot;))
l2 = Cmk.lines!(fig[1, 1], lxs1, lys1, color=&quot;blue&quot;)
sc1 = Cmk.scatter!(fig[1, 1], lxs2, lys2, color=&quot;blue&quot;, marker=:circle)
sc2 = Cmk.scatter!(fig[1, 1], lxs3, lys3, color=&quot;blue&quot;, marker=:xcross)
Cmk.vlines!(fig[1, 1], LStatisticEx2, color=&quot;lightblue&quot;, type=:dashdot)
Cmk.text!(fig[1, 1], 1.35, 0.1,
    text=&quot;L-Statistic = 1.28&quot;)
Cmk.xlims!(0, 4)
Cmk.ylims!(0, 0.25)
Cmk.axislegend(ax1,
    [l1, l2, sc1, sc2],
    [
    &quot;F-Statistic(1, 6) [Dsts.Normal(25, 3), n = 4]&quot;,
    &quot;L-Statistic [Dsts.Normal(25, 3), n = 4]&quot;,
    &quot;L-Statistic [Dsts.Normal(100, 50), n = 4]&quot;,
    &quot;L-Statistic [Dsts.Normal(25, 3), n = 8]&quot;
    ],
    &quot;Distributions
(num groups = 2,
n - num observations per gorup)&quot;,
    position=:rt)
fig</code></pre>
<p>Behold</p>
<figure>
<img src="./images/fAndLDistCh05Ex2.png" id="fig:fAndLDistCh05Ex2" alt="Figure 18: Experimental F- and L-Distributions." /><figcaption aria-hidden="true">Figure 18: Experimental F- and L-Distributions.</figcaption>
</figure>
<p>Wow, what a beauty.</p>
<p>A few points of notice. Before, we calculated the probability (<code>lStatProb</code>) of getting the L-Statistic value greater than the vertical light blue line (the area under the blue curve to the right of that line). This is a one tail probability only. Interestingly, for the L-Distribution the mean and sd in the population of origin are not that important (blue circles for <code>Dsts.Normal(100, 50)</code> lie exactly on the blue line for <code>Dsts.Normal(25, 3)</code>). However, the number of groups and the number of observations per group affect the shape of the distribution (blue xcrosses for <code>Dsts.Normal(25, 3) n = 8</code> diverge from the blue curve for <code>Dsts.Normal(25, 3) n = 4</code>).</p>
<p>The same is true for the F-Distribution. That is why the F-Distribution depends only on the degrees of freedom (<code>Dsts.FDist(dfGroup, dfResidual)</code>). The degrees of freedom depend on the number of groups and the number of observations per group.</p>
<h3 data-number="5.8.3" id="sec:compare_contin_data_ex3_solution"><span class="header-section-number">5.8.3</span> Solution to Exercise 3</h3>
<p>OK, let’s start with functions for checking the assumptions</p>
<pre class="language-julia"><code>function areAllDistributionsNormal(vects::Vector{&lt;:Vector{&lt;:Real}})::Bool
    return [Pg.normality(v).pval[1] for v in vects] |&gt;
        pvals -&gt; map(pv -&gt; pv &gt; 0.05, pvals) |&gt;
        all
end

function areAllVariancesEqual(vects::Vector{&lt;:Vector{&lt;:Real}})
    return Htests.FlignerKilleenTest(vects...) |&gt;
        Htests.pvalue |&gt; pv -&gt; pv &gt; 0.05
end</code></pre>
<p>The functions above are basically just wrappers around the code we wrote in Section <a href="./compare_contin_data_post_hoc_tests.html#sec:compare_contin_data_post_hoc_tests">5.5</a>. Now, time for <code>getPValUnpairedTest</code></p>
<pre class="language-julia"><code>function getPValUnpairedTest(
    v1::Vector{&lt;:Real}, v2::Vector{&lt;:Real})::Float64

    normality::Bool = areAllDistributionsNormal([v1, v2])
    homogeneity::Bool = areAllVariancesEqual([v1, v2])

    return (
        (normality &amp;&amp; homogeneity) ? Htests.EqualVarianceTTest(v1, v2) :
        (normality) ? Htests.UnequalVarianceTTest(v1, v2) :
        Htests.MannWhitneyUTest(v1,v2)
        ) |&gt; Htests.pvalue
end</code></pre>
<p>The code is rather self-explanatory, of course if you remember the ternary expression from Section <a href="./julia_language_decision_making.html#sec:ternary_expression">3.5.2</a> and Section <a href="./julia_language_exercises_solutions.html#sec:julia_language_exercise4_solution">3.9.4</a>.</p>
<p>Let’s test our newly created function with the data from Section <a href="./compare_contin_data_two_samp_ttest.html#sec:compare_contin_data_unpaired_ttest">5.3.2</a> (<code>miceBwt</code>)</p>
<pre class="language-julia"><code>getPValUnpairedTest([miceBwt[!, n] for n in names(miceBwt)]...) |&gt;
x -&gt; round(x, digits=4)</code></pre>
<p>0.0804</p>
<p>The p-value is the same as in Section <a href="./compare_contin_data_two_samp_ttest.html#sec:compare_contin_data_unpaired_ttest">5.3.2</a> (as it should be), but this time we didn’t have to explicitly check the assumptions before applying the appropriate test.</p>
<h3 data-number="5.8.4" id="sec:compare_contin_data_ex4_solution"><span class="header-section-number">5.8.4</span> Solution to Exercise 4</h3>
<p>First, let’s start with a helper function that will return us the all the possible pairs from a vector.</p>
<pre class="language-julia"><code>function getUniquePairs(names::Vector{T})::Vector{Tuple{T,T}} where {T}

    @assert (length(names) &gt;= 2) &quot;the input must be of length &gt;= 2&quot;

    uniquePairs::Vector{Tuple{T,T}} =
        Vector{Tuple{T,T}}(undef, binomial(length(names), 2))
    currInd::Int = 1

    for i in eachindex(names)[1:(end-1)]
        for j in eachindex(names)[(i+1):end]
            uniquePairs[currInd] = (names[i], names[j])
            currInd += 1
        end
    end

    return uniquePairs
end</code></pre>
<p>The function is generic, so it can be applied to vector of any type, here designed as <code>{T}</code>. It starts by initializing an empty vector (<code>uniquePairs</code>) to hold the results. The initialization takes the following form: <code>Vector{typeOfItElements}(iniaialValues, lengthOfTheVector)</code>. The vector is filled with <code>undef</code>s (undefined values, some garbage) as placeholders. The size of the new vector is calculated by the <a href="https://docs.julialang.org/en/v1/base/math/#Base.binomial">binomial</a> function. It is applied in the form <code>binomial(numValuesToChooseFrom, numValsPerGroup)</code> and returns the number of possible groups of a given size. The rest is just iteration (<code>for</code> loops) over the indexes (<code>eachindex</code>) of the <code>names</code> vector to get all the possible pairs. Let’s quickly check if the function works as expected.</p>
<pre class="language-julia"><code>(
getUniquePairs([10, 20]),
getUniquePairs([1.1, 2.2, 3.3]),
getUniquePairs([&quot;w&quot;, &quot;x&quot;, &quot;y&quot;, &quot;z&quot;]),
)</code></pre>
<pre class="output"><code>([(10, 20)],
 [(1.1, 2.2), (1.1, 3.3), (2.2, 3.3)],
 [(&quot;w&quot;, &quot;x&quot;), (&quot;w&quot;, &quot;y&quot;), (&quot;w&quot;, &quot;z&quot;), (&quot;x&quot;, &quot;y&quot;), (&quot;x&quot;, &quot;z&quot;), (&quot;y&quot;, &quot;z&quot;)])</code></pre>
<p>OK, now it’s time for <code>getPValsUnpairedTests</code></p>
<pre class="language-julia"><code># df - DataFrame: each column continuous variable
# returns uncorrected p-values
function getPValsUnpairedTests(
    df::Dfs.DataFrame)::Dict{Tuple{String,String},Float64}

    pairs::Vector{Tuple{String,String}} = getUniquePairs(names(df))
    pvals::Vector{Float64} = [
        getPValUnpairedTest(df[!, a], df[!, b])
        for (a, b) in pairs
    ]

    return Dict(pairs[i] =&gt; pvals[i] for i in eachindex(pairs))
end</code></pre>
<p>First, we obtain the pairs of group names that we will compare later (<code>pairs</code>). In the next few lines we use for comprehension to obtain the p-values. Since each element of <code>pairs</code> vector is a tuple (e.g. <code>[("spA", "spB"), etc.]</code>) we assign its elements to <code>a</code> and <code>b</code> (<code>for (a, b)</code>) and pass them to <code>df</code> to get the values of interest (e.g. <code>df[!, a]</code>). The values are send to <code>getPValUnpairedTest</code> from the previous section. We terminate (<code>return</code>) with another comprehension that creates a dictionary with the desired result.</p>
<p>Let’s see how the function works and compare the results with the ones we obtained in Section <a href="./compare_contin_data_post_hoc_tests.html#sec:compare_contin_data_post_hoc_tests">5.5</a>.</p>
<pre class="language-julia"><code>getPValsUnpairedTests(miceBwtABC)</code></pre>
<pre class="output"><code>Dict{Tuple{String, String}, Float64} with 3 entries:
  (&quot;spA&quot;, &quot;spB&quot;) =&gt; 0.0251115
  (&quot;spA&quot;, &quot;spC&quot;) =&gt; 0.000398545
  (&quot;spB&quot;, &quot;spC&quot;) =&gt; 0.0493322</code></pre>
<p>OK, the uncorrected p-values are the same.</p>
<p>Now, the improved version.</p>
<pre class="language-julia"><code># df - DataFrame: each column continuous variable
# returns corrected p-values
function getPValsUnpairedTests(
    df::Dfs.DataFrame,
    multCorr::Type{M}
)::Dict{Tuple{String,String},Float64} where {M&lt;:Mt.PValueAdjustment}

    pairs::Vector{Tuple{String,String}} = getUniquePairs(names(df))
    pvals::Vector{Float64} = [
        getPValUnpairedTest(df[!, a], df[!, b])
        for (a, b) in pairs
    ]
    pvals = Mt.adjust(pvals, multCorr())

    return Dict(pairs[i] =&gt; pvals[i] for i in eachindex(pairs))
end</code></pre>
<p>Don’t worry about the strange type declarations like <code>::Type{M}</code> and <code>where {M&lt;:Mt.PValueAdjustment}</code>. I added them for the sake of consistency (after reading the code in <a href="https://github.com/juliangehring/MultipleTesting.jl">the package repo</a> and some try and error). When properly called, the function should work equally well without those parts.</p>
<p>Anyway, it wasn’t that bad, we basically just added a small piece of code (<code>multCorr</code> in the arguments list and <code>pvals = Mt.adjust(pvals, multCorr())</code> in the function body) similar to the one in Section <a href="./compare_contin_data_multip_correction.html#sec:compare_contin_data_multip_correction">5.6</a>.</p>
<p>Let’s see how it works.</p>
<pre class="language-julia"><code># the default Bonferroni correction
getPValsUnpairedTests(miceBwtABC, Mt.Bonferroni)</code></pre>
<pre class="output"><code>Dict{Tuple{String, String}, Float64} with 3 entries:
  (&quot;spA&quot;, &quot;spB&quot;) =&gt; 0.0753345
  (&quot;spA&quot;, &quot;spC&quot;) =&gt; 0.00119563
  (&quot;spB&quot;, &quot;spC&quot;) =&gt; 0.147997</code></pre>
<p>That looks quite alright. Time for one more swing.</p>
<pre class="language-julia"><code># Benjamini-Hochberg correction
getPValsUnpairedTests(miceBwtABC, Mt.BenjaminiHochberg)</code></pre>
<pre class="output"><code>Dict{Tuple{String, String}, Float64} with 3 entries:
  (&quot;spA&quot;, &quot;spB&quot;) =&gt; 0.0376673
  (&quot;spA&quot;, &quot;spC&quot;) =&gt; 0.00119563
  (&quot;spB&quot;, &quot;spC&quot;) =&gt; 0.0493322</code></pre>
<p>Again, the p-values appear to be the same as those we saw in Section <a href="./compare_contin_data_multip_correction.html#sec:compare_contin_data_multip_correction">5.6</a>.</p>
<p>To be continued…</p>


<div class="bottom-nav">
    <p id="nav-prev" style="text-align: left;">
        <a class="menu-level-2" href="./compare_contin_data_exercises.html"><b>5.7</b> Exercises - Comparisons ..</a> <kbd>←</kbd>
        <span id="nav-next" style="float: right;">
            <kbd>→</kbd> <a class="menu-level-1" href="./appendix.html"><b></b> Appendix</a>
        </span>
    </p>
</div>


<div class="license">
    <br/>
  <br/>
  <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>
    Bartlomiej Lukaszuk
</div>
</div>
</div>
</body>
</html>