<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Bartlomiej Lukaszuk" />
  <title>Solutions - Association - Romeo and Julia, where Romeo is Basic Statistics</title>
  <link rel="stylesheet" href="./style.css"/>
    <script src="./mousetrap.min.js"></script>
    <style>
  @font-face {
    font-family: JuliaMono-Regular;
    src: url("./JuliaMono-Regular.woff2");
  }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <link rel="stylesheet" href="./github.min.css">
<script src="./highlight.min.js"></script>
<script src="./julia.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', (event) => {
    document.querySelectorAll('pre').forEach((el) => {
        if (!el.classList.contains('output')) {
            hljs.highlightElement(el);
        }
    });
});
</script>
 
</head>
<body>
<script>
function click_next() {
  var next = document.getElementById('nav-next');
  next.firstElementChild.nextElementSibling.click();
}
function click_prev() {
  var prev = document.getElementById('nav-prev');
  prev.firstElementChild.click();
}
Mousetrap.bind('right', click_next);
Mousetrap.bind('h', click_prev);
Mousetrap.bind('left', click_prev);
Mousetrap.bind('l', click_next);
</script>

<div class="books-container">
<aside class="books-menu">
<input type="checkbox" id="menu">
<label for="menu">☰</label>
<div class="books-title">
<a href="./">Romeo and Julia, where Romeo is Basic Statistics</a>
</div><br />
<span class="books-subtitle">

</span>
<div class="books-menu-content">
<li><a class="menu-level-1" href="./about.html"><b>1</b> About</a></li>
<li><a class="menu-level-1" href="./why_julia.html"><b>2</b> Why Julia</a></li>
<li><a class="menu-level-2" href="./julia_is_fast.html"><b>2.1</b> Julia is fast</a></li>
<li><a class="menu-level-2" href="./julia_is_simple.html"><b>2.2</b> Julia is simple</a></li>
<li><a class="menu-level-2" href="./jl_pleasure_to_write.html"><b>2.3</b> Pleasure to write</a></li>
<li><a class="menu-level-2" href="./jl_not_mainstream.html"><b>2.4</b> Not mainstream</a></li>
<li><a class="menu-level-2" href="./jl_open_source.html"><b>2.5</b> Julia is free</a></li>
<li><a class="menu-level-1" href="./julia_first_encounter.html"><b>3</b> Julia - first encounter</a></li>
<li><a class="menu-level-2" href="./julia_installation.html"><b>3.1</b> Installation</a></li>
<li><a class="menu-level-2" href="./julia_language_constructs.html"><b>3.2</b> Language Constructs</a></li>
<li><a class="menu-level-2" href="./julia_language_variables.html"><b>3.3</b> Variables</a></li>
<li><a class="menu-level-2" href="./julia_language_functions.html"><b>3.4</b> Functions</a></li>
<li><a class="menu-level-2" href="./julia_language_decision_making.html"><b>3.5</b> Decision Making</a></li>
<li><a class="menu-level-2" href="./julia_language_repetition.html"><b>3.6</b> Repetition</a></li>
<li><a class="menu-level-2" href="./julia_language_libraries.html"><b>3.7</b> Additional libraries</a></li>
<li><a class="menu-level-2" href="./julia_language_exercises.html"><b>3.8</b> Julia - Exercises</a></li>
<li><a class="menu-level-2" href="./julia_language_exercises_solutions.html"><b>3.9</b> Julia - Solutions</a></li>
<li><a class="menu-level-1" href="./statistics_intro.html"><b>4</b> Statistics - introduction</a></li>
<li><a class="menu-level-2" href="./statistics_intro_imports.html"><b>4.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./statistics_intro_probability_definition.html"><b>4.2</b> Probability - definition</a></li>
<li><a class="menu-level-2" href="./statistics_intro_probability_properties.html"><b>4.3</b> Probability - properties</a></li>
<li><a class="menu-level-2" href="./statistics_prob_theor_practice.html"><b>4.4</b> Probability - theory and..</a></li>
<li><a class="menu-level-2" href="./statistics_prob_distribution.html"><b>4.5</b> Probability distribution</a></li>
<li><a class="menu-level-2" href="./statistics_normal_distribution.html"><b>4.6</b> Normal distribution</a></li>
<li><a class="menu-level-2" href="./statistics_intro_hypothesis_testing.html"><b>4.7</b> Hypothesis testing</a></li>
<li><a class="menu-level-2" href="./statistics_intro_exercises.html"><b>4.8</b> Statistics intro - Exerc..</a></li>
<li><a class="menu-level-2" href="./statistics_intro_exercises_solutions.html"><b>4.9</b> Statistics intro - Solut..</a></li>
<li><a class="menu-level-1" href="./compare_contin_data.html"><b>5</b> Comparisons - continuous d..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_imports.html"><b>5.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_one_samp_ttest.html"><b>5.2</b> One sample Student’s t..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_two_samp_ttest.html"><b>5.3</b> Two samples Student’s ..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_one_way_anova.html"><b>5.4</b> One-way ANOVA</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_post_hoc_tests.html"><b>5.5</b> Post-hoc tests</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_multip_correction.html"><b>5.6</b> Multiplicity correction</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_exercises.html"><b>5.7</b> Exercises - Comparisons ..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_exercises_solutions.html"><b>5.8</b> Solutions - Comparisons ..</a></li>
<li><a class="menu-level-1" href="./compare_categ_data.html"><b>6</b> Comparisons - categorical ..</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_imports.html"><b>6.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_flashback.html"><b>6.2</b> Flashback</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_chisq_test.html"><b>6.3</b> Chi squared test</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_fisher_exact_text.html"><b>6.4</b> Fisher’s exact test</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_bigger_table.html"><b>6.5</b> Bigger table</a></li>
<li><a class="menu-level-2" href="./compare_categ_test_for_independence.html"><b>6.6</b> Test for independence</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_exercises.html"><b>6.7</b> Exercises - Comparisons ..</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_exercises_solutions.html"><b>6.8</b> Solutions - Comparisons ..</a></li>
<li><a class="menu-level-1" href="./assoc_pred.html"><b>7</b> Association and Prediction</a></li>
<li><a class="menu-level-2" href="./assoc_pred_imports.html"><b>7.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./assoc_pred_lin_relation.html"><b>7.2</b> Linear relation</a></li>
<li><a class="menu-level-2" href="./assoc_pred_covariance.html"><b>7.3</b> Covariance</a></li>
<li><a class="menu-level-2" href="./assoc_pred_correlation.html"><b>7.4</b> Correlation</a></li>
<li><a class="menu-level-2" href="./assoc_pred_corr_pitfalls.html"><b>7.5</b> Correlation Pitfalls</a></li>
<li><a class="menu-level-2" href="./assoc_pred_simple_lin_reg.html"><b>7.6</b> Simple Linear Regression</a></li>
<li><a class="menu-level-2" href="./assoc_pred_multiple_lin_reg.html"><b>7.7</b> Multiple Linear Regressi..</a></li>
<li><a class="menu-level-2" href="./assoc_pred_exercises.html"><b>7.8</b> Exercises - Association ..</a></li>
<li><a class="menu-level-2" href="./assoc_pred_exercises_solutions.html"><b>7.9</b> Solutions - Association</a></li>
<li><a class="menu-level-1" href="./time_to_say_goodbye.html"><b>8</b> Time to say goodbye</a></li>
</div>
</aside>

<div class="books-content">
<h2 data-number="7.9" id="sec:assoc_pred_exercises_solutions"><span class="header-section-number">7.9</span> Solutions - Association</h2>
<p>In this sub-chapter you will find exemplary solutions to the exercises from the previous section.</p>
<h3 data-number="7.9.1" id="sec:assoc_pred_ex1_solution"><span class="header-section-number">7.9.1</span> Solution to Exercise 1</h3>
<p>Let’s write <code>getRanks</code>, but let’s start simple and use it on a sorted vector <code>[100, 500, 1000]</code> without ties. In this case the body of <code>getRanks</code> function would be something like.</p>
<pre class="language-julia"><code># for now the function is without types
function getRanksVer1(v)
    # or: ranks = collect(1:length(v))
    ranks = collect(eachindex(v))
    return ranks
end

getRanksVer1([100, 500, 1000])</code></pre>
<pre class="output"><code>[1, 2, 3]</code></pre>
<p>Time to complicate stuff a bit by adding some ties in numbers.</p>
<pre class="language-julia"><code># for now the function is without types
function getRanksVer2(v)
    initialRanks = collect(eachindex(v))
    finalRanks = zeros(length(v))
    for i in eachindex(v)
        indicesInV = findall(x -&gt; x == v[i], v)
        finalRanks[i] = Stats.mean(initialRanks[indicesInV])
    end
    return finalRanks
end

(
    getRanksVer2([100, 500, 500, 1000]),
    getRanksVer2([100, 500, 500, 500, 1000])
)</code></pre>
<pre class="output"><code>([1.0, 2.5, 2.5, 4.0],
 [1.0, 3.0, 3.0, 3.0, 5.0])</code></pre>
<p>The <code>findall</code> function accepts a function (here <code>x -&gt; x == v[i]</code>) and a vector (here <code>v</code>). Next, it runs the function on every element of the vector and returns the indices for which the result was <code>true</code>. Here we are looking for elements in <code>v</code> that are equal to the currently examined (<code>v[i]</code>) element of <code>v</code>. Then, we use <code>indicesInV</code> to get the <code>initialRanks</code>. The <code>initialRanks[indicesInV]</code> returns a <code>Vector</code> that contains one or more (if ties occur) <code>initialRanks</code> for a given element of <code>v</code>. Finally, we calculate the average rank for a given number in <code>v</code> by using <code>Stats.mean</code>. The function may be sub-optimall as for <code>[100, 500, 500, 1000]</code> the average rank for <code>500</code> is calculated twice (once for <code>500</code> at index 2 and once for <code>500</code> at index 3) and for <code>[100, 500, 500, 500, 1000]</code> the average rank for <code>500</code> is calculated three times. Still, we are more concerned with the correct result and not the efficiency (assuming that the function is fast enough) so we will leave it as it is.</p>
<p>Now, the final tweak. The input vector is shuffled.</p>
<pre class="language-julia"><code># for now the function is without types
function getRanksVer3(v)
    sortedV = collect(sort(v))
    initialRanks = collect(eachindex(sortedV))
    finalRanks = zeros(length(v))
    for i in eachindex(v)
        indicesInSortedV = findall(x -&gt; x == v[i], sortedV)
        finalRanks[i] = Stats.mean(initialRanks[indicesInSortedV])
    end
    return finalRanks
end

(
    getRanksVer3([500, 100, 1000]),
    getRanksVer3([500, 100, 500, 1000]),
    getRanksVer3([500, 100, 500, 1000, 500])
)</code></pre>
<pre class="output"><code>([2.0, 1.0, 3.0],
 [2.5, 1.0, 2.5, 4.0],
 [3.0, 1.0, 3.0, 5.0, 3.0])</code></pre>
<p>Here, we let the built in function <code>sort</code> to arrange the numbers from <code>v</code> in the ascending order. Then for each number from <code>v</code> we get its indices in <code>sortedV</code> and their corresponding ranks based on that (<code>initialRanks[indicesInSortedV]</code>). As in <code>getRanksVer2</code> the latter is used to calculate their average.</p>
<p>OK, time for cleanup + adding some types for future references (before we forget them).</p>
<pre class="language-julia"><code>function getRanks(v::Vector{&lt;:Real})::Vector{&lt;:Float64}
    sortedV::Vector{&lt;:Real} = collect(sort(v))
    initialRanks::Vector{&lt;:Int} = collect(eachindex(sortedV))
    finalRanks::Vector{&lt;:Float64} = zeros(length(v))
    for i in eachindex(v)
        indicesInSortedV = findall(x -&gt; x == v[i], sortedV)
        finalRanks[i] = Stats.mean(initialRanks[indicesInSortedV])
    end
    return finalRanks
end

(
    getRanks([100, 500, 1000]),
    getRanks([100, 500, 500, 1000]),
    getRanks([500, 100, 1000]),
    getRanks([500, 100, 500, 1000]),
    getRanks([500, 100, 500, 1000, 500])
)</code></pre>
<pre class="output"><code>([1.0, 2.0, 3.0],
 [1.0, 2.5, 2.5, 4.0],
 [2.0, 1.0, 3.0],
 [2.5, 1.0, 2.5, 4.0],
 [3.0, 1.0, 3.0, 5.0, 3.0])</code></pre>
<p>After long last we can define <code>getSpearmCorAndPval</code> and apply it to <code>animals</code> data frame.</p>
<pre class="language-julia"><code>function getSpearmCorAndPval(
    v1::Vector{&lt;:Real}, v2::Vector{&lt;:Real})::Tuple{Float64, Float64}
    return getCorAndPval(getRanks(v1), getRanks(v2))
end

getSpearmCorAndPval(animals.Body, animals.Brain)</code></pre>
<pre class="output"><code>(0.7162994456021085, 1.8128636948722132e-5)</code></pre>
<p>The result appears to reflect the general relationship well (compare with Figure 34).</p>
<h3 data-number="7.9.2" id="sec:assoc_pred_ex2_solution"><span class="header-section-number">7.9.2</span> Solution to Exercise 2</h3>
<p>The solution should be quite simple assuming you did solve exercise 4 from ch05 (see Section <a href="./compare_contin_data_exercises.html#sec:compare_contin_data_ex4">5.7.4</a> and Section <a href="./compare_contin_data_exercises_solutions.html#sec:compare_contin_data_ex4_solution">5.8.4</a>) and exercise 5 from ch06 (see Section <a href="./compare_categ_data_exercises.html#sec:compare_categ_data_ex5">6.7.5</a> and Section <a href="./compare_categ_data_exercises_solutions.html#sec:compare_categ_data_ex5_solution">6.8.5</a>).</p>
<p>Let’s start with the helper functions, <code>getUniquePairs</code> (Section <a href="./compare_contin_data_exercises_solutions.html#sec:compare_contin_data_ex4_solution">5.8.4</a>) and <code>getSortedKeysVals</code> (Section <a href="./statistics_prob_distribution.html#sec:statistics_prob_distribution">4.5</a>) that we developed previously. For your convenience I paste them below.</p>
<pre><code>function getUniquePairs(names::Vector{T})::Vector{Tuple{T,T}} where T
    @assert (length(names) &gt;= 2) &quot;the input must be of length &gt;= 2&quot;
    uniquePairs::Vector{Tuple{T,T}} =
        Vector{Tuple{T,T}}(undef, binomial(length(names), 2))
    currInd::Int = 1
    for i in eachindex(names)[1:(end-1)]
        for j in eachindex(names)[(i+1):end]
            uniquePairs[currInd] = (names[i], names[j])
            currInd += 1
        end
    end
    return uniquePairs
end

function getSortedKeysVals(d::Dict{T1,T2})::Tuple{
    Vector{T1},Vector{T2}} where {T1,T2}
    sortedKeys::Vector{T1} = keys(d) |&gt; collect |&gt; sort
    sortedVals::Vector{T2} = [d[k] for k in sortedKeys]
    return (sortedKeys, sortedVals)
end</code></pre>
<p>Now, time to get all possible ‘raw’ correlations.</p>
<pre class="language-julia"><code>function getAllCorsAndPvals(
    df::Dfs.DataFrame, colsNames::Vector{String}
)::Dict{Tuple{String,String},Tuple{Float64,Float64}}

    uniquePairs::Vector{Tuple{String,String}} = getUniquePairs(colsNames)
    allCors::Dict{Tuple{String,String},Tuple{Float64,Float64}} = Dict(
        (n1, n2) =&gt; getCorAndPval(df[!, n1], df[!, n2]) for (n1, n2)
        in
        uniquePairs)

    return allCors
end</code></pre>
<pre class="output"><code>getAllCorsAndPvals (generic function with 1 method)</code></pre>
<p>We start by getting the <code>uniquePairs</code> for the columns of interest <code>colNames</code>. Then we use dictionary comprehension to get our result. We iterate through each pair <code>for (n1, n2) in uniquePairs</code>. Each <code>uniquePair</code> is composed of a tuple <code>(n1, n2)</code>, where <code>n1</code> - name1, <code>n2</code> - name2. While traversing the <code>uniquePairs</code> we calculate the correlations and p-values (<code>getCorAndPval</code>) by selecting columns of interest (<code>df[:, n1]</code> and <code>df[:, n2]</code>). And that’s it. Let’s see how it works and how many false positives we got (remember, we expect 2 or 3).</p>
<pre class="language-julia"><code>allCorsPvals = getAllCorsAndPvals(bogusCors, letters)
falsePositves = (map(t -&gt; t[2], values(allCorsPvals)) .&lt;= 0.05) |&gt; sum
falsePositves</code></pre>
<p>3</p>
<p>First, we extract the values from our dictionary with <code>values(allCorsPvals)</code>. The values are a vector of tuples [<code>(cor, pval)</code>]. To get p-values alone, we use map function that takes every tuple (<code>t</code>) and returns its second element (<code>t[2]</code>). Finally, we compare the p-values with our cutoff level for type 1 error (<span class="math inline">\(\alpha = 0.05\)</span>). And sum the <code>Bool</code>s (each <code>true</code> is counted as 1, and each <code>false</code> as 0).</p>
<p>Anyway, as expected we got 3 false positives. All that’s left to do is to apply the multiplicity correction.</p>
<pre class="language-julia"><code>function adjustPvals(
    corsAndPvals::Dict{Tuple{String,String},Tuple{Float64,Float64}},
    adjMeth::Type{M}
)::Dict{Tuple{String,String},Tuple{Float64,Float64}} where
    {M&lt;:Mt.PValueAdjustment}

    ks, vs = getSortedKeysVals(corsAndPvals)
    cors::Vector{&lt;:Float64} = map(t -&gt; t[1], vs)
    pvals::Vector{&lt;:Float64} = map(t -&gt; t[2], vs)
    adjustedPVals::Vector{&lt;:Float64} = Mt.adjust(pvals, adjMeth())
    newVs::Vector{Tuple{Float64,Float64}} = collect(
        zip(cors, adjustedPVals))

    return Dict(ks[i] =&gt; newVs[i] for i in eachindex(ks))
end</code></pre>
<pre class="output"><code>adjustPvals (generic function with 1 method)</code></pre>
<p>The code is rather self explanatory and relies on step by step operations: 1) getting our p-values (<code>pvals</code>), 2) applying an adjustment method (<code>adjMeth</code>) on them (<code>Mt.adjust</code>), and 3) combining the adjusted p-values (<code>adjustedPVals</code>) with <code>cors</code> again. For that last purpose we use <code>zip</code> function we met in Section <a href="./compare_categ_data_exercises_solutions.html#sec:compare_categ_data_ex1_solution">6.8.1</a>. Finally we recreate a dictionary using comprehension. Time for some tests.</p>
<pre class="language-julia"><code>allCorsPvalsAdj = adjustPvals(allCorsPvals, Mt.BenjaminiHochberg)
falsePositves = (map(t -&gt; t[2], values(allCorsPvalsAdj)) .&lt;= 0.05) |&gt; sum
falsePositves</code></pre>
<p>0</p>
<p>We cannot expect a multiplicity correction to be a 100% error-proof solution. Still, it’s better than doing nothing and in our case it did the trick, we got rid of false positives.</p>
<h3 data-number="7.9.3" id="sec:assoc_pred_ex3_solution"><span class="header-section-number">7.9.3</span> Solution to Exercise 3</h3>
<p>Let’s start by writing a function to get a correlation matrix. We could use for that <a href="https://docs.julialang.org/en/v1/stdlib/Statistics/#Statistics.cor">Stats.cor</a> like so <code>Stats.cor(bogusCors)</code>. But since we need to add significance markers then the p-values for the correlations are indispensable. As far as I’m aware the package does not have it, then we will write a function of our own.</p>
<pre class="language-julia"><code>function getCorsAndPvalsMatrix(
    df::Dfs.DataFrame,
    colNames::Vector{String})::Array{&lt;:Tuple{Float64, Float64}}

    len::Int = length(colNames)
    corsPvals::Dict{Tuple{String,String},Tuple{Float64,Float64}} =
        getAllCorsAndPvals(df, colNames)
    mCorsPvals::Array{Tuple{Float64,Float64}} = fill((0.0, 0.0), len, len)

    for cn in eachindex(colNames) # cn - column number
        for rn in eachindex(colNames) # rn - row number
            corPval = (
                haskey(corsPvals, (colNames[rn], colNames[cn])) ?
                corsPvals[(colNames[rn], colNames[cn])] :
                get(corsPvals, (colNames[cn], colNames[rn]), (1, 1))
            )
            mCorsPvals[rn, cn] = corPval
        end
    end

    return mCorsPvals
end</code></pre>
<pre class="output"><code>getCorsAndPvalsMatrix (generic function with 1 method)</code></pre>
<p>The function <code>getCorsAndPvalsMatrix</code> uses <code>getAllCorsAndPvals</code> we developed previously (Section <a href="./assoc_pred_exercises_solutions.html#sec:assoc_pred_ex2_solution">7.9.2</a>). Then we define the matrix (our result), we initialize it with the <a href="https://docs.julialang.org/en/v1/base/arrays/#Base.fill">fill function</a> that takes an initial value and returns an array of a given size filled with that value (<code>(0.0, 0.0)</code>). Next, we replace the initial values in <code>mCorsPvals</code> with the correct ones by using two <code>for</code> loops. Inside them we extract a tuple (<code>corPval</code>) from the unique <code>corsPvals</code>. First, we test if a <code>corPval</code> for a given two variables (e.g. “a” and “b”) is in the dictionary <code>corsPvals</code> (<code>haskey</code> etc.). If so then we insert it into the <code>mCorsPvals</code>. If not, then we search in <code>corsPvals</code> by its reverse (so, e.g. “b” and “a”) with <code>get(corsPvals, (colNames[cn], colNames[rn]), etc.)</code>. If that combination is not present then we are looking for the correlation of a variable with itself (e.g. “a” and “a”) which is equal to <code>(1, 1)</code> (for correlation coefficient and p-value, respectively). Once we are done we return our <code>mCorsPvals</code> matrix (aka <code>Array</code>). Time to give it a test run.</p>
<pre class="language-julia"><code>getCorsAndPvalsMatrix(bogusCors, [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;])</code></pre>
<pre class="output"><code>3×3 Matrix{Tuple{Float64, Float64}}:
 (1.0, 1.0)             (0.194, 0.591239)      (-0.432251, 0.212195)
 (0.194, 0.591239)      (1.0, 1.0)             (-0.205942, 0.568128)
 (-0.432251, 0.212195)  (-0.205942, 0.568128)  (1.0, 1.0)</code></pre>
<p>The numbers seem to be OK. In the future, you may consider changing the function so that the p-values are adjusted, e.g. by using <code>Mt.BenjaminiHochberg</code> correction, but here we need some statistical significance for our heatmap so we will leave it as it is.</p>
<p>Now, let’s move to drawing a plot.</p>
<pre><code>mCorsPvals = getCorsAndPvalsMatrix(bogusCors, letters)
cors = map(t -&gt; t[1], mCorsPvals)
pvals = map(t -&gt; t[2], mCorsPvals)
nRows, _ = size(cors) # same num of rows and cols in our matrix
xs = repeat(1:nRows, inner=nRows)
ys = repeat(1:nRows, outer=nRows)[end:-1:1]

fig = Cmk.Figure()
ax, hm = Cmk.heatmap(fig[1, 1], xs, ys, [cors...],
    colormap=:RdBu, colorrange=(-1, 1),
    axis=(;
        xticks=(1:1:nRows, letters[1:nRows]),
        yticks=(1:1:nRows, letters[1:nRows][end:-1:1])
    ))
Cmk.hlines!(fig[1, 1], 1.5:1:nRows, color=&quot;black&quot;, linewidth=0.25)
Cmk.vlines!(fig[1, 1], 1.5:1:nRows, color=&quot;black&quot;, linewidth=0.25)
Cmk.Colorbar(fig[:, end+1], hm)
fig</code></pre>
<p>We begin by preparing the necessary helper variables (<code>mCorsPvals</code>, <code>cors</code>, <code>pvals</code>, <code>nRows</code>, <code>xs</code>, <code>ys</code>). The last two are the coordinates of the centers of squares on the X- and Y-axis. The <code>cors</code> will be flattened row by row using <code>[cors...]</code> syntax. For your information <code>repeat([1, 2], inner = 2)</code> returns <code>[1, 1, 2, 2]</code> and <code>repeat([1, 2], outer = 2)</code> returns <code>[1, 2, 1, 2]</code>. The <code>ys</code> vector is then reversed with <code>[end:-1:1]</code> to make it reflect better the order of correlations in <code>cors</code> (left to right, row by row). The same goes for <code>yticks</code> below. The above was determined to be the right option by trial and error. The next important parameter is <code>colorrange=(-1, 1)</code> it ensures that <code>-1</code> is always the leftmost color (red) from the <code>:RdBu</code> colormap and <code>1</code> is always the rightmost color (blue) from the colormap. Without it the colors would be set to <code>minimum(cors)</code> and <code>maximum(cors)</code> which we do not want since the <code>minimum</code> will change from matrix to matrix. Over our heatmap we overlay the grid (<code>hlines!</code> and <code>vlines!</code>) to make the squares separate better from each other. The centers of the squares are at integers, and the edges are at halves, that’s why we start the ticks at <code>1.5</code>. Finally, we add <code>Colorbar</code> as they did in the docs for <code>Cmk.heatmap</code>. The result of this code is visible in Figure 33 from the previous section.</p>
<p>OK, let’s add the correlation coefficients and statistical significance markers. But first, two little helper functions.</p>
<pre class="language-julia"><code>function getColorForCor(corCoeff::Float64)::String
    @assert (0 &lt;= abs(corCoeff) &lt;= 1) &quot;abc(corCoeff) must be in range [0-1]&quot;
    return (abs(corCoeff) &gt;= 0.65) ? &quot;white&quot; : &quot;black&quot;
end

function getMarkerForPval(pval::Float64)::String
    @assert (0 &lt;= pval &lt;= 1) &quot;probability must be in range [0-1]&quot;
    return (pval &lt;= 0.05) ? &quot;#&quot; : &quot;&quot;
end</code></pre>
<pre class="output"><code>getMarkerForPval (generic function with 1 method)</code></pre>
<p>As you can see <code>getColorForCor</code> returns a color (“white” or “black”) for a given value of correlation coefficient (white color will make it easier to read the correlation coefficient on a dark red/blue background of a square). On the other hand <code>getMarkerForPval</code> returns a marker (“#”) when a pvalue is below our customary cutoff level for type I error.</p>
<pre><code>fig = Cmk.Figure()
ax, hm = Cmk.heatmap(fig[1, 1], xs, ys, [cors...],
    colormap=:RdBu, colorrange=(-1, 1),
    axis=(;
        xticks=(1:1:nRows, letters[1:nRows]),
        yticks=(1:1:nRows, letters[1:nRows][end:-1:1])
    ))
Cmk.text!(fig[1, 1], xs, ys,
    text=string.(round.([cors...], digits=2)) .*
        getMarkerForPval.([pvals...]),
    align=(:center, :center),
    color=getColorForCor.([cors...]))
Cmk.hlines!(fig[1, 1], 1.5:1:nRows, color=&quot;black&quot;, linewidth=0.25)
Cmk.vlines!(fig[1, 1], 1.5:1:nRows, color=&quot;black&quot;, linewidth=0.25)
Cmk.Colorbar(fig[:, end+1], hm)
fig</code></pre>
<p>The only new element here is <code>Cmk.text!</code> function, but since we used it a couple of times throughout this book, then I will leave the explanation of how the code piece works to you. Anyway, the result is to be found below.</p>
<figure>
<img src="./images/ch07ex3v2.png" id="fig:ch07ex3v2" alt="Figure 37: Correlation heatmap for data in bogusCors with the coefficients and significance markers." /><figcaption aria-hidden="true">Figure 37: Correlation heatmap for data in <code>bogusCors</code> with the coefficients and significance markers.</figcaption>
</figure>
<p>It looks good. Also the number of significance markers is right. Previously (Section <a href="./assoc_pred_exercises_solutions.html#sec:assoc_pred_ex2_solution">7.9.2</a>) we said we got 3 significant correlations (based on ‘raw’ p-values). Since, the upper right triangle of the heatmap is a mirror reflection of the lower left triangle, then we should see 6 significance markers altogether. As a final step (that I leave to you) we could enclose the code from this task into a neat function named, e.g. <code>drawHeatmap</code>.</p>
<h3 data-number="7.9.4" id="sec:assoc_pred_ex4_solution"><span class="header-section-number">7.9.4</span> Solution to Exercise 4</h3>
<p>OK, the code for this task is quite straightforward so let’s get right to it.</p>
<pre><code>function drawDiagPlot(
    reg::Glm.StatsModels.TableRegressionModel,
    byCol::Bool = true)::Cmk.Figure
    dim::Vector{&lt;:Int} = (byCol ? [1, 2] : [2, 1])
    res::Vector{&lt;:Float64} = Glm.residuals(reg)
    pred::Vector{&lt;:Float64} = Glm.predict(reg)
    form::String = string(Glm.formula(reg))
    fig = Cmk.Figure(size=(800, 800))
    Cmk.scatter(fig[1, 1], pred, res,
        axis=(;
            title=&quot;Residuals vs Fitted\n&quot; * form,
            xlabel=&quot;Fitted values&quot;,
            ylabel=&quot;Residuals&quot;)
    )
    Cmk.hlines!(fig[1, 1], 0, linestyle=:dash, color=&quot;gray&quot;)
    Cmk.qqplot(fig[dim...],
        Dsts.Normal(0, 1),
        getZScore.(res, Stats.mean(res), Stats.std(res)),
        qqline=:identity,
        axis=(;
            title=&quot;Normal Q-Q\n&quot; * form,
            xlabel=&quot;Theoretical Quantiles&quot;,
            ylabel=&quot;Standarized residuals&quot;)
    )
    return fig
end</code></pre>
<p>We begin with extracting residuals (<code>res</code>) and predicted (<code>pred</code>) values from our model (<code>reg</code>). Additionally, we extract the formula (<code>form</code>) as a string. Then, we prepare a scatter plot (<code>Cmk.scatter</code>) with <code>pred</code> and <code>res</code> placed on X- and Y-axis, respectively. Next, we add a horizontal line (<code>Cmk.hlines!</code>) at 0 on Y-axis (the points should be randomly scattered around it). All that’s left to do is to build the required Q-Q plot (<code>qqplot</code>) with X-axis that contains the theoretical <a href="https://en.wikipedia.org/wiki/Normal_distribution#Standard_normal_distribution">standard normal distribution</a> (<code>Dsts.Normal(0, 1)</code>) and Y-axis with the standardized (<code>getZScore</code>) residuals (<code>res</code>). We also add <code>qqline=:identity</code> (here, identity means x = y) to facilitate the interpretation [if two distributions (on X- and Y-axis)] are alike then the points should lie roughly on the line. Since the visual impression we get may depend on the spacial arrangement (stretching or tightening of the points on a graph) our function enables us to choose (<code>byCol</code>) between column (<code>true</code>) and row (<code>false</code>) alignment of the subplots.</p>
<p>For a change let’s test our function on the <code>iceMod2</code> from Section <a href="./assoc_pred_multiple_lin_reg.html#sec:assoc_pred_multiple_lin_reg">7.7</a>. Behold the result of <code>drawDiagPlot(iceMod2, false)</code>.</p>
<figure>
<img src="./images/ch07ex4v2.png" id="fig:ch07ex4v2" alt="Figure 38: Diagnostic plot for regression model (iceMod2)." /><figcaption aria-hidden="true">Figure 38: Diagnostic plot for regression model (iceMod2).</figcaption>
</figure>
<p>Hmm, I don’t know about you but to me the bottom panel looks rather normal. However, the top panel seems to display a wave (‘w’) pattern. This may be a sign of auto-correlation (explanation in a moment) and translate into instability of the estimation error produced by the model across the values of the explanatory variable(s). The error will display a wave pattern (once bigger once smaller). Now we got a choice, either we leave this model as it is (and we bear the consequences) or we try to find a better one.</p>
<p>To understand what the auto-correlation means in our case let’s do a thought experiment. Right now in the room that I am sitting the temperature is equal to 20 degrees of Celsius (68 deg. Fahrenheit). Which one is the more probable value of the temperature in 1 minute from now: 0 deg. Cels. (32 deg. Fahr.) or 21 deg. Cels. (70 deg. Fahr.)? I guess the latter is the more reasonable option. That is because the temperature one minute from now is a derivative of the temperature in the present (i.e. both values are correlated).</p>
<p>The same might be true for <a href="https://vincentarelbundock.github.io/Rdatasets/doc/Ecdat/Icecream.html">Icecream</a> data frame, since it contains <code>Temp</code> column that we used in our model (<code>iceMod2</code>). We could try to remedy this by removing (kind of) the auto-correlation, e.g. with <code>ice2 = ice[2:end, :]</code> and <code>ice2.TempDiff = ice.Temp[1:(end-1)] .- ice.Temp[2:end]</code> and building our model a new. This is what we will do in the next exercise (although we will try to automate the process a bit).</p>
<h3 data-number="7.9.5" id="sec:assoc_pred_ex5_solution"><span class="header-section-number">7.9.5</span> Solution to Exercise 5</h3>
<p>Let’s start with a few helper functions.</p>
<pre class="language-julia"><code>function getLinMod(
    df::Dfs.DataFrame,
    y::String, xs::Vector{&lt;:String}
    )::Glm.StatsModels.TableRegressionModel
    return Glm.lm(Glm.term(y) ~ sum(Glm.term.(xs)), df)
end

function getPredictorsPvals(
    m::Glm.StatsModels.TableRegressionModel)::Vector{&lt;:Float64}
    allPvals::Vector{&lt;:Float64} = Glm.coeftable(m).cols[4]
    # 1st pvalue is for the intercept
    return allPvals[2:end]
end

function getIndsEltsNotEqlM(v::Vector{&lt;:Real}, m::Real)::Vector{&lt;:Int}
    return findall(x -&gt; !isapprox(x, m), v)
end</code></pre>
<p>We begin with <code>getLinMod</code> that accepts a data frame (<code>df</code>), name of the dependent variable (<code>y</code>) and names of the independent/predictor variables (<code>xs</code>). Based on the inputs it creates the model programmatically using <code>Glm.term</code>.</p>
<p>Next, we go with <code>getPredictorsPvals</code> that returns the p-values corresponding to a model’s coefficients.</p>
<p>Then, we define <code>getIndsEltsNotEqlM</code> that we will use to filter out the highest p-value from our model.</p>
<p>OK, time for the main actor of the show.</p>
<pre class="language-julia"><code># returns minimal adequate (linear) model
function getMinAdeqMod(
    df::Dfs.DataFrame, y::String, xs::Vector{&lt;:String}
    )::Glm.StatsModels.TableRegressionModel

    preds::Vector{&lt;:String} = copy(xs)
    mod::Glm.StatsModels.TableRegressionModel = getLinMod(df, y, preds)
    pvals::Vector{&lt;:Float64} = getPredictorsPvals(mod)
    maxPval::Float64 = maximum(pvals)
    inds::Vector{&lt;:Int} = getIndsEltsNotEqlM(pvals, maxPval)

    for _ in xs
        if (maxPval &lt;= 0.05)
            break
        end
        if (length(preds) == 1 &amp;&amp; maxPval &gt; 0.05)
            mod = Glm.lm(Glm.term(y) ~ Glm.term(1), df)
            break
        end
        preds = preds[inds]
        mod = getLinMod(df, y, preds)
        pvals = getPredictorsPvals(mod)
        maxPval = maximum(pvals)
        inds = getIndsEltsNotEqlM(pvals, maxPval)
    end

    return mod
end</code></pre>
<p>We begin with defining the necessary variables that we will update in a for loop. The variables are: predictors (<code>preds</code>), linear model (<code>mod</code>), p-values for the model’s coefficients (<code>pvals</code>), maximum p-value (<code>maxPval</code>) and indices of predictors that we will leave in our model (<code>inds</code>). We start each iteration (<code>for _ in xs</code>) by checking if we already reached our minimal adequate model. To that end we make sure that all the remaining coefficients are statistically significant (<code>if (maxPval &lt;= 0.05)</code>) or if we run out of the explanatory variables (<code>length(preds) == 1 &amp;&amp; maxPval &gt; 0.05</code>) we return our default (<code>y ~ 1</code>) model (the intercept of this model is equal to <code>Stats.mean(y)</code>). If not then we remove one predictor variable from the model (<code>preds = preds[inds]</code>) and update the remaining helper variables (<code>mod</code>, <code>pvals</code>, <code>maxPval</code>, <code>inds</code>). And that’s it, let’s see how it works.</p>
<pre class="language-julia"><code>ice2mod = getMinAdeqMod(ice2, names(ice2)[1], names(ice2)[2:end])
ice2mod
</code></pre>
<pre class="output"><code>───────────────────────────────────────────────────────────────────────
               Coef.  Std. Error      t  Pr(&gt;|t|)  Lower 95%  Upper 95%
───────────────────────────────────────────────────────────────────────
(Intercept)  -0.0672      0.0988  -0.68    0.5024    -0.2707     0.1363
Income        0.0031      0.001    2.99    0.0062     0.001      0.0053
Temp          0.0032      0.0004   7.99    &lt;1e-99     0.0024     0.004
TempDiff     -0.0022      0.0007  -2.93    0.0071    -0.0037    -0.0006
───────────────────────────────────────────────────────────────────────</code></pre>
<p>It appears to work as expected. Let’s compare it with a full model.</p>
<pre class="language-julia"><code>ice2FullMod = getLinMod(ice2, names(ice2)[1], names(ice2)[2:end])

Glm.ftest(ice2FullMod.model, ice2mod.model)</code></pre>
<pre class="output"><code>F-test: 2 models fitted on 29 observations
───────────────────────────────────────────────────────────────
     DOF  ΔDOF     SSR    ΔSSR      R²      ΔR²      F*   p(&gt;F)
───────────────────────────────────────────────────────────────
[1]   10        0.0193          0.8450                         
[2]    5    -5  0.0227  0.0034  0.8179  -0.0272  0.7019  0.6285
───────────────────────────────────────────────────────────────</code></pre>
<p>It looks good as well. We reduced the number of explanatory variables while maintaining comparable (p &gt; 0.05) explanatory power of our model.</p>
<p>Time to check the assumptions with our diagnostic plot (<code>drawDiagPlot</code> from Section <a href="./assoc_pred_exercises_solutions.html#sec:assoc_pred_ex1_solution">7.9.1</a>).</p>
<figure>
<img src="./images/ch07ex5.png" id="fig:ch07ex5" alt="Figure 39: Diagnostic plot for regression model (ice2mod)." /><figcaption aria-hidden="true">Figure 39: Diagnostic plot for regression model (ice2mod).</figcaption>
</figure>
<p>To me, the plot has slightly improved and since I run out of ideas how to make our model even better I’ll leave it as it is.</p>
<p>Now, let’s compare our <code>ice2mod</code>, that aimed to counteract the auto-correlation, with its predecessor (<code>iceMod2</code>). We will focus on the explanatory powers (adjusted <span class="math inline">\(r^2\)</span>, the higher the better)</p>
<pre class="language-julia"><code>(
    Glm.adjr2(iceMod2),
    Glm.adjr2(ice2mod)
)</code></pre>
<pre class="output"><code>(0.6799892012945553, 0.796000295561351)</code></pre>
<p>and the average prediction errors (the lower the better).</p>
<pre class="language-julia"><code>(
    getAvgEstimError(iceMod2),
    getAvgEstimError(ice2mod)
)</code></pre>
<pre class="output"><code>(0.026114993652645798, 0.022116071809225545)</code></pre>
<p>Again, it appears that we managed to improve our model’s prediction power at a cost of slightly more difficult interpretation (go ahead examine the output tables for <code>Income + Temp + TempDiff</code> vs. <code>Income + Temp</code> and explain to yourself how each variable influences the value of <code>Cons</code>). This is usually the case, the less straightforward the model, the less intuitive is its interpretation.</p>
<p>At a very long last we may check how our <code>getMinAdeqMod</code> will behave when there are no meaningful explanatory variables.</p>
<pre class="language-julia"><code>getMinAdeqMod(ice2, &quot;Cons&quot;, [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;])</code></pre>
<pre class="output"><code>Cons ~ 1

Coefficients:
────────────────────────────────────────────────────────────────────────
                Coef.  Std. Error      t  Pr(&gt;|t|)  Lower 95%  Upper 95%
────────────────────────────────────────────────────────────────────────
(Intercept)  0.358517    0.012397  28.92    &lt;1e-21   0.333123   0.383911
────────────────────────────────────────────────────────────────────────</code></pre>
<p>In that case (no meaningful explanatory variables) our best estimate of <code>y</code> (here <code>Cons</code>) is the variable’s average (<code>Stats.mean(ice2.Cons)</code>) which is returned as the <code>Coef.</code> for <code>(Intercept)</code>. In that case <code>Std. Error</code> is just the standard error of the mean that we met in Section <a href="./compare_contin_data_one_samp_ttest.html#sec:compare_contin_data_one_samp_ttest">5.2</a> (compare with <code>getSem(ice2.Cons)</code>).</p>


<div class="bottom-nav">
    <p id="nav-prev" style="text-align: left;">
        <a class="menu-level-2" href="./assoc_pred_exercises.html"><b>7.8</b> Exercises - Association ..</a> <kbd>←</kbd>
        <span id="nav-next" style="float: right;">
             
        </span>
    </p>
</div>


<div class="license">
    <br/>
  <br/>
  <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>
    Bartlomiej Lukaszuk
</div>
</div>
</div>
</body>
</html>