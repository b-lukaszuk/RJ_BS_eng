<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Bartlomiej Lukaszuk" />
  <title>Solutions - Prediction - Romeo and Julia, where Romeo is Basic Statistics</title>
  <link rel="stylesheet" href="./style.css"/>
    <script src="./mousetrap.min.js"></script>
    <style>
  @font-face {
    font-family: JuliaMono-Regular;
    src: url("./JuliaMono-Regular.woff2");
  }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <link rel="stylesheet" href="./github.min.css">
<script src="./highlight.min.js"></script>
<script src="./julia.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', (event) => {
    document.querySelectorAll('pre').forEach((el) => {
        if (!el.classList.contains('output')) {
            hljs.highlightElement(el);
        }
    });
});
</script>
 
</head>
<body>
<script>
function click_next() {
  var next = document.getElementById('nav-next');
  next.firstElementChild.nextElementSibling.click();
}
function click_prev() {
  var prev = document.getElementById('nav-prev');
  prev.firstElementChild.click();
}
Mousetrap.bind('right', click_next);
Mousetrap.bind('h', click_prev);
Mousetrap.bind('left', click_prev);
Mousetrap.bind('l', click_next);
</script>

<div class="books-container">
<aside class="books-menu">
<input type="checkbox" id="menu">
<label for="menu">☰</label>
<div class="books-title">
<a href="./">Romeo and Julia, where Romeo is Basic Statistics</a>
</div><br />
<span class="books-subtitle">

</span>
<div class="books-menu-content">
<li><a class="menu-level-1" href="./about.html"><b>1</b> About</a></li>
<li><a class="menu-level-1" href="./why_julia.html"><b>2</b> Why Julia</a></li>
<li><a class="menu-level-2" href="./julia_is_fast.html"><b>2.1</b> Julia is fast</a></li>
<li><a class="menu-level-2" href="./julia_is_simple.html"><b>2.2</b> Julia is simple</a></li>
<li><a class="menu-level-2" href="./jl_pleasure_to_write.html"><b>2.3</b> Pleasure to write</a></li>
<li><a class="menu-level-2" href="./jl_not_mainstream.html"><b>2.4</b> Not mainstream</a></li>
<li><a class="menu-level-2" href="./jl_open_source.html"><b>2.5</b> Julia is free</a></li>
<li><a class="menu-level-1" href="./julia_first_encounter.html"><b>3</b> Julia - first encounter</a></li>
<li><a class="menu-level-2" href="./julia_installation.html"><b>3.1</b> Installation</a></li>
<li><a class="menu-level-2" href="./julia_language_constructs.html"><b>3.2</b> Language Constructs</a></li>
<li><a class="menu-level-2" href="./julia_language_variables.html"><b>3.3</b> Variables</a></li>
<li><a class="menu-level-2" href="./julia_language_functions.html"><b>3.4</b> Functions</a></li>
<li><a class="menu-level-2" href="./julia_language_decision_making.html"><b>3.5</b> Decision Making</a></li>
<li><a class="menu-level-2" href="./julia_language_repetition.html"><b>3.6</b> Repetition</a></li>
<li><a class="menu-level-2" href="./julia_language_libraries.html"><b>3.7</b> Additional libraries</a></li>
<li><a class="menu-level-2" href="./julia_language_exercises.html"><b>3.8</b> Julia - Exercises</a></li>
<li><a class="menu-level-2" href="./julia_language_exercises_solutions.html"><b>3.9</b> Julia - Solutions</a></li>
<li><a class="menu-level-1" href="./statistics_intro.html"><b>4</b> Statistics - introduction</a></li>
<li><a class="menu-level-2" href="./statistics_intro_imports.html"><b>4.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./statistics_intro_probability_definition.html"><b>4.2</b> Probability - definition</a></li>
<li><a class="menu-level-2" href="./statistics_intro_probability_properties.html"><b>4.3</b> Probability - properties</a></li>
<li><a class="menu-level-2" href="./statistics_prob_theor_practice.html"><b>4.4</b> Probability - theory and..</a></li>
<li><a class="menu-level-2" href="./statistics_prob_distribution.html"><b>4.5</b> Probability distribution</a></li>
<li><a class="menu-level-2" href="./statistics_normal_distribution.html"><b>4.6</b> Normal distribution</a></li>
<li><a class="menu-level-2" href="./statistics_intro_hypothesis_testing.html"><b>4.7</b> Hypothesis testing</a></li>
<li><a class="menu-level-2" href="./statistics_intro_exercises.html"><b>4.8</b> Statistics intro - Exerc..</a></li>
<li><a class="menu-level-2" href="./statistics_intro_exercises_solutions.html"><b>4.9</b> Statistics intro - Solut..</a></li>
<li><a class="menu-level-1" href="./compare_contin_data.html"><b>5</b> Comparisons - continuous d..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_imports.html"><b>5.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_one_samp_ttest.html"><b>5.2</b> One sample Student’s t..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_two_samp_ttest.html"><b>5.3</b> Two samples Student’s ..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_one_way_anova.html"><b>5.4</b> One-way ANOVA</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_post_hoc_tests.html"><b>5.5</b> Post-hoc tests</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_multip_correction.html"><b>5.6</b> Multiplicity correction</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_exercises.html"><b>5.7</b> Exercises - Comparisons ..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_exercises_solutions.html"><b>5.8</b> Solutions - Comparisons ..</a></li>
<li><a class="menu-level-1" href="./compare_categ_data.html"><b>6</b> Comparisons - categorical ..</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_imports.html"><b>6.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_flashback.html"><b>6.2</b> Flashback</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_chisq_test.html"><b>6.3</b> Chi squared test</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_fisher_exact_text.html"><b>6.4</b> Fisher’s exact test</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_bigger_table.html"><b>6.5</b> Bigger table</a></li>
<li><a class="menu-level-2" href="./compare_categ_test_for_independence.html"><b>6.6</b> Test for independence</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_exercises.html"><b>6.7</b> Exercises - Comparisons ..</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_exercises_solutions.html"><b>6.8</b> Solutions - Comparisons ..</a></li>
<li><a class="menu-level-1" href="./association.html"><b>7</b> Association</a></li>
<li><a class="menu-level-2" href="./association_imports.html"><b>7.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./association_lin_relation.html"><b>7.2</b> Linear relation</a></li>
<li><a class="menu-level-2" href="./association_covariance.html"><b>7.3</b> Covariance</a></li>
<li><a class="menu-level-2" href="./association_correlation.html"><b>7.4</b> Correlation</a></li>
<li><a class="menu-level-2" href="./association_corr_pitfalls.html"><b>7.5</b> Correlation Pitfalls</a></li>
<li><a class="menu-level-2" href="./association_exercises.html"><b>7.6</b> Exercises - Association</a></li>
<li><a class="menu-level-2" href="./association_exercises_solutions.html"><b>7.7</b> Solutions - Association</a></li>
<li><a class="menu-level-1" href="./prediction.html"><b>8</b> Prediction</a></li>
<li><a class="menu-level-2" href="./prediction_imports.html"><b>8.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./pred_simple_lin_reg.html"><b>8.2</b> Simple Linear Regression</a></li>
<li><a class="menu-level-2" href="./pred_multiple_lin_reg.html"><b>8.3</b> Multiple Linear Regressi..</a></li>
<li><a class="menu-level-2" href="./prediction_exercises.html"><b>8.4</b> Exercises - Prediction</a></li>
<li><a class="menu-level-2" href="./prediction_exercises_solutions.html"><b>8.5</b> Solutions - Prediction</a></li>
<li><a class="menu-level-1" href="./appendix.html"><b></b> Appendix</a></li>
<li><a class="menu-level-1" href="./references.html"><b>9</b> References</a></li>
</div>
</aside>

<div class="books-content">
<h2 data-number="8.5" id="sec:prediction_exercises_solutions"><span class="header-section-number">8.5</span> Solutions - Prediction</h2>
<p>In this sub-chapter you will find exemplary solutions to the exercises from the previous section.</p>
<h3 data-number="8.5.1" id="sec:prediction_ex1_solution"><span class="header-section-number">8.5.1</span> Solution to Exercise 1</h3>
<p>OK, the code for this task is quite straightforward so let’s get right to it.</p>
<pre>
function drawDiagPlot(
    reg::Glm.StatsModels.TableRegressionModel,
    byCol::Bool = true)::Cmk.Figure
    dim::Vector{<:Int} = (byCol ? [1, 2] : [2, 1])
    res::Vector{<:Float64} = Glm.residuals(reg)
    pred::Vector{<:Float64} = Glm.predict(reg)
    form::String = string(Glm.formula(reg))
    fig = Cmk.Figure(size=(800, 800))
    Cmk.scatter(fig[1, 1], pred, res,
        axis=(;
            title="Residuals vs Fitted\n" * form,
            xlabel="Fitted values",
            ylabel="Residuals")
    )
    Cmk.hlines!(fig[1, 1], 0, linestyle=:dash, color="gray")
    Cmk.qqplot(fig[dim...],
        Dsts.Normal(0, 1),
        getZScore.(res, Stats.mean(res), Stats.std(res)),
        qqline=:identity,
        axis=(;
            title="Normal Q-Q\n" * form,
            xlabel="Theoretical Quantiles",
            ylabel="Standarized residuals")
    )
    return fig
end
</pre>
<p>We begin with extracting residuals (<code>res</code>) and predicted (<code>pred</code>) values from our model (<code>reg</code>). Additionally, we extract the formula (<code>form</code>) as a string. Then, we prepare a scatter plot (<code>Cmk.scatter</code>) with <code>pred</code> and <code>res</code> placed on X- and Y-axis, respectively. Next, we add a horizontal line (<code>Cmk.hlines!</code>) at 0 on Y-axis (the points should be randomly scattered around it). All that’s left to do is to build the required Q-Q plot (<code>qqplot</code>) with X-axis that contains the theoretical <a href="https://en.wikipedia.org/wiki/Normal_distribution#Standard_normal_distribution">standard normal distribution</a> (<code>Dsts.Normal(0, 1)</code>) and Y-axis with the standardized (<code>getZScore</code>) residuals (<code>res</code>). We also add <code>qqline=:identity</code> (here, identity means x = y) to facilitate the interpretation [if two distributions (on X- and Y-axis)] are alike then the points should lie roughly on the line. Since the visual impression we get may depend on the spacial arrangement (stretching or tightening of the points on a graph) our function enables us to choose (<code>byCol</code>) between column (<code>true</code>) and row (<code>false</code>) alignment of the subplots.</p>
<p>For a change let’s test our function on the <code>iceMod2</code> from Section <a href="./pred_multiple_lin_reg.html#sec:pred_multiple_lin_reg">8.3</a>. Behold the result of <code>drawDiagPlot(iceMod2, false)</code>.</p>
<figure>
<img src="./images/ch08ex1v2.png" id="fig:ch08ex1v2" alt="Figure 38: Diagnostic plot for regression model (iceMod2)." /><figcaption aria-hidden="true">Figure 38: Diagnostic plot for regression model (iceMod2).</figcaption>
</figure>
<p>Hmm, I don’t know about you but to me the bottom panel looks rather normal. However, the top panel seems to display a wave (‘w’) pattern. This may be a sign of auto-correlation (explanation in a moment) and translate into instability of the error in estimation produced by the model across the values of the explanatory variable(s). The error will display a wave pattern (once bigger once smaller). Now we got a choice, either we leave this model as it is (and we bear the consequences) or we try to find a better one.</p>
<p>To understand what the auto-correlation means in our case let’s do a thought experiment. Right now in the room that I am sitting the temperature is equal to 20 degrees of Celsius (68 deg. Fahrenheit). Which one is the more probable value of the temperature in 1 minute from now: 0 deg. Cels. (32 deg. Fahr.) or 21 deg. Cels. (70 deg. Fahr.)? I guess the latter is the more reasonable option. That is because the temperature one minute from now is a derivative of the temperature at present (i.e. both values are correlated).</p>
<p>The same might be true for <a href="https://vincentarelbundock.github.io/Rdatasets/doc/Ecdat/Icecream.html">Icecream</a> data frame, since it contains <code>Temp</code> column that we used in our model (<code>iceMod2</code>). We could try to remedy this by removing (kind of) the auto-correlation, e.g. with <code>ice2 = ice[2:end, :]</code> and <code>ice2.TempDiff = ice.Temp[1:(end-1)] .- ice.Temp[2:end]</code> and building our model a new. This is what we will do in the next exercise (although we will try to automate the process a bit).</p>
<h3 data-number="8.5.2" id="sec:prediction_ex2_solution"><span class="header-section-number">8.5.2</span> Solution to Exercise 2</h3>
<p>Let’s start with a few helper functions.</p>
<pre class="language-julia"><code>function getLmMod(
    df::Dfs.DataFrame,
    y::String, xs::Vector{&lt;:String}
    )::Glm.StatsModels.TableRegressionModel
    return Glm.lm(Glm.term(y) ~ sum(Glm.term.(xs)), df)
end

function getPredictorsPvals(
    m::Glm.StatsModels.TableRegressionModel)::Vector{&lt;:Float64}
    allPvals::Vector{&lt;:Float64} = Glm.coeftable(m).cols[4]
    # 1st pvalue is for intercept
    return allPvals[2:end]
end

function getIndsEltsNotEqlM(v::Vector{&lt;:Real}, m::Real)::Vector{&lt;:Int}
    return findall(x -&gt; !isapprox(x, m), v)
end</code></pre>
<p>We begin with <code>getLmMod</code> that accepts a data frame (<code>df</code>), name of the dependent variable (<code>y</code>) and names of the independent/predictor variables (<code>xs</code>). Based on the inputs it creates the model programmatically using <code>Glm.term</code>.</p>
<p>Next, we go with <code>getPredictorsPvals</code> that returns the p-values corresponding to a model’s coefficients.</p>
<p>Then, we define <code>getIndsEltsNotEqlM</code> that we will use to filter out the highest p-value from our model.</p>
<p>OK, time for the main actor of the show.</p>
<pre class="language-julia"><code># returns minimal adequate model
function getMinAdeqMod(
    df::Dfs.DataFrame, y::String, xs::Vector{&lt;:String}
    )::Glm.StatsModels.TableRegressionModel

    preds::Vector{&lt;:String} = copy(xs)
    mod::Glm.StatsModels.TableRegressionModel = getLmMod(df, y, preds)
    pvals::Vector{&lt;:Float64} = getPredictorsPvals(mod)
    maxPval::Float64 = maximum(pvals)
    inds::Vector{&lt;:Int} = getIndsEltsNotEqlM(pvals, maxPval)

    for _ in xs
        if (maxPval &lt;= 0.05)
            break
        end
        if (length(preds) == 1 &amp;&amp; maxPval &gt; 0.05)
            mod = Glm.lm(Glm.term(y) ~ Glm.term(1), df)
            break
        end
        preds = preds[inds]
        mod = getLmMod(df, y, preds)
        pvals = getPredictorsPvals(mod)
        maxPval = maximum(pvals)
        inds = getIndsEltsNotEqlM(pvals, maxPval)
    end

    return mod
end</code></pre>
<p>We begin with defining the necessary variables that we will update in a for loop. The variables are: predictors (<code>preds</code>), linear model (<code>mod</code>), p-values for the model’s coefficients (<code>pvals</code>), maximum p-value (<code>maxPval</code>) and indices of predictors that we will leave in our model (<code>inds</code>). We start each iteration (<code>for _ in xs</code>) by checking if we already reached our minimal adequate model. To that end we make sure that all the remaining coefficients are statistically significant (<code>if (maxPval &lt;= 0.05)</code>) or if we run out of the explanatory variables (<code>length(preds) == 1 &amp;&amp; maxPval &gt; 0.05</code>) we return our default (<code>y ~ 1</code>) model (the intercept of this model is equal to <code>Stats.mean(y)</code>). If not then we remove one predictor variable from the model (<code>preds = preds[inds]</code>) and update the remaining helper variables (<code>mod</code>, <code>pvals</code>, <code>maxPval</code>, <code>inds</code>). And that’s it, let’s see how it works.</p>
<pre class="language-julia"><code>ice2mod = getMinAdeqMod(ice2, names(ice2)[1], names(ice2)[2:end])</code></pre>
<pre class="output"><code>Cons ~ 1 + Income + Temp + TempDiff

Coefficients:
──────────────────────────────────────────────────────────────────────────────────
                   Coef.   Std. Error      t  Pr(&gt;|t|)     Lower 95%     Upper 95%
──────────────────────────────────────────────────────────────────────────────────
(Intercept)  -0.0672394   0.0988057    -0.68    0.5024  -0.270733      0.136255
Income        0.00311856  0.0010429     2.99    0.0062   0.000970674   0.00526645
Temp          0.00321959  0.000402895   7.99    &lt;1e-07   0.00238982    0.00404937
TempDiff     -0.00216081  0.000736842  -2.93    0.0071  -0.00367836   -0.000643256
──────────────────────────────────────────────────────────────────────────────────</code></pre>
<p>It appears to work as expected. Let’s compare it with a full model.</p>
<pre class="language-julia"><code>ice2FullMod = getLmMod(ice2, names(ice2)[1], names(ice2)[2:end])

Glm.ftest(ice2FullMod.model, ice2mod.model)</code></pre>
<pre class="output"><code>F-test: 2 models fitted on 29 observations
───────────────────────────────────────────────────────────────
     DOF  ΔDOF     SSR    ΔSSR      R²      ΔR²      F*   p(&gt;F)
───────────────────────────────────────────────────────────────
[1]   10        0.0193          0.8450                         
[2]    5    -5  0.0227  0.0034  0.8179  -0.0272  0.7019  0.6285
───────────────────────────────────────────────────────────────</code></pre>
<p>It looks good as well. We reduced the number of explanatory variables while maintaining comparable (p &gt; 0.05) explanatory power of our our model.</p>
<p>Time to check the assumptions with our diagnostic plot (<code>drawDiagPlot</code> from Section <a href="./prediction_exercises_solutions.html#sec:prediction_ex1_solution">8.5.1</a>).</p>
<figure>
<img src="./images/ch08ex2.png" id="fig:ch08ex2" alt="Figure 39: Diagnostic plot for regression model (ice2mod)." /><figcaption aria-hidden="true">Figure 39: Diagnostic plot for regression model (ice2mod).</figcaption>
</figure>
<p>To me, the plot has slightly improved.</p>
<p>Now, let’s compare our <code>ice2mod</code>, that aimed to counteract the auto-correlation, with its predecessor (<code>iceMod2</code>). We will focus on the explanatory powers (adjusted <span class="math inline">\(r^2\)</span>, the higher the better)</p>
<pre class="language-julia"><code>(
    Glm.adjr2(iceMod2),
    Glm.adjr2(ice2mod)
)</code></pre>
<pre class="output"><code>(0.6799892012945553, 0.796000295561351)</code></pre>
<p>and the average prediction errors (the lower the better).</p>
<pre class="language-julia"><code>(
    abs.(Glm.residuals(iceMod2)) |&gt; Stats.mean,
    abs.(Glm.residuals(ice2mod)) |&gt; Stats.mean
)</code></pre>
<pre class="output"><code>(0.026114993652645798, 0.022116071809225545)</code></pre>
<p>Again, it appears that we managed to improve our model.</p>
<p>At a very long last we may check how our <code>getMinAdeqMod</code> will behave when there are no meaningful explanatory variables.</p>
<pre class="language-julia"><code>getMinAdeqMod(ice2, &quot;Cons&quot;, [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;])</code></pre>
<pre class="output"><code>Cons ~ 1

Coefficients:
────────────────────────────────────────────────────────────────────────
                Coef.  Std. Error      t  Pr(&gt;|t|)  Lower 95%  Upper 95%
────────────────────────────────────────────────────────────────────────
(Intercept)  0.358517    0.012397  28.92    &lt;1e-21   0.333123   0.383911
────────────────────────────────────────────────────────────────────────</code></pre>
<p>In that case (no meaningful explanatory variables) our best estimate of <code>y</code> (here <code>Cons</code>) is the variable’s average (<code>Stats.mean(ice2.cons)</code>) which is returned as the <code>Coef.</code> for <code>(Intercept)</code>. In that case <code>Std. Error</code> is just the standard error of the mean that we met in Section <a href="./compare_contin_data_one_samp_ttest.html#sec:compare_contin_data_one_samp_ttest">5.2</a> (compare with <code>getSem(ice2.cons)</code>).</p>
<p>To be continued …</p>


<div class="bottom-nav">
    <p id="nav-prev" style="text-align: left;">
        <a class="menu-level-2" href="./prediction_exercises.html"><b>8.4</b> Exercises - Prediction</a> <kbd>←</kbd>
        <span id="nav-next" style="float: right;">
            <kbd>→</kbd> <a class="menu-level-1" href="./appendix.html"><b></b> Appendix</a>
        </span>
    </p>
</div>


<div class="license">
    <br/>
  <br/>
  <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>
    Bartlomiej Lukaszuk
</div>
</div>
</div>
</body>
</html>