<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Bartlomiej Lukaszuk" />
  <title>Hypothesis testing - Romeo and Julia</title>
  <link rel="stylesheet" href="./style.css"/>
    <script src="./mousetrap.min.js"></script>
    <style>
  @font-face {
    font-family: JuliaMono-Regular;
    src: url("./JuliaMono-Regular.woff2");
  }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <link rel="stylesheet" href="./github.min.css">
<script src="./highlight.min.js"></script>
<script src="./julia.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', (event) => {
    document.querySelectorAll('pre').forEach((el) => {
        if (!el.classList.contains('output')) {
            hljs.highlightElement(el);
        }
    });
});
</script>
 
</head>
<body>
<script>
function click_next() {
  var next = document.getElementById('nav-next');
  next.firstElementChild.nextElementSibling.click();
}
function click_prev() {
  var prev = document.getElementById('nav-prev');
  prev.firstElementChild.click();
}
Mousetrap.bind('right', click_next);
Mousetrap.bind('h', click_prev);
Mousetrap.bind('left', click_prev);
Mousetrap.bind('l', click_next);
</script>

<div class="books-container">
<aside class="books-menu">
<input type="checkbox" id="menu">
<label for="menu">☰</label>
<div class="books-title">
<a href="./">Romeo and Julia</a>
</div><br />
<span class="books-subtitle">
where Romeo is Basic Statistics
</span>
<div class="books-menu-content">
<li><a class="menu-level-1" href="./about.html"><b>1</b> About</a></li>
<li><a class="menu-level-1" href="./why_julia.html"><b>2</b> Why Julia</a></li>
<li><a class="menu-level-2" href="./julia_is_fast.html"><b>2.1</b> Julia is fast</a></li>
<li><a class="menu-level-2" href="./julia_is_simple.html"><b>2.2</b> Julia is simple</a></li>
<li><a class="menu-level-2" href="./jl_pleasure_to_write.html"><b>2.3</b> Pleasure to write</a></li>
<li><a class="menu-level-2" href="./jl_not_mainstream.html"><b>2.4</b> Not mainstream</a></li>
<li><a class="menu-level-2" href="./jl_open_source.html"><b>2.5</b> Julia is free</a></li>
<li><a class="menu-level-1" href="./julia_first_encounter.html"><b>3</b> Julia - first encounter</a></li>
<li><a class="menu-level-2" href="./julia_installation.html"><b>3.1</b> Installation</a></li>
<li><a class="menu-level-2" href="./julia_language_constructs.html"><b>3.2</b> Language Constructs</a></li>
<li><a class="menu-level-2" href="./julia_language_variables.html"><b>3.3</b> Variables</a></li>
<li><a class="menu-level-2" href="./julia_language_functions.html"><b>3.4</b> Functions</a></li>
<li><a class="menu-level-2" href="./julia_language_decision_making.html"><b>3.5</b> Decision Making</a></li>
<li><a class="menu-level-2" href="./julia_language_repetition.html"><b>3.6</b> Repetition</a></li>
<li><a class="menu-level-2" href="./julia_language_libraries.html"><b>3.7</b> Additional libraries</a></li>
<li><a class="menu-level-2" href="./julia_language_exercises.html"><b>3.8</b> Julia - Exercises</a></li>
<li><a class="menu-level-2" href="./julia_language_exercises_solutions.html"><b>3.9</b> Julia - Solutions</a></li>
<li><a class="menu-level-1" href="./statistics_intro.html"><b>4</b> Statistics - introduction</a></li>
<li><a class="menu-level-2" href="./statistics_intro_imports.html"><b>4.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./statistics_intro_probability_definition.html"><b>4.2</b> Probability - definition</a></li>
<li><a class="menu-level-2" href="./statistics_intro_probability_properties.html"><b>4.3</b> Probability - properties</a></li>
<li><a class="menu-level-2" href="./statistics_prob_theor_practice.html"><b>4.4</b> Probability - theory and..</a></li>
<li><a class="menu-level-2" href="./statistics_prob_distribution.html"><b>4.5</b> Probability distribution</a></li>
<li><a class="menu-level-2" href="./statistics_normal_distribution.html"><b>4.6</b> Normal distribution</a></li>
<li><a class="menu-level-2" href="./statistics_intro_hypothesis_testing.html"><b>4.7</b> Hypothesis testing</a></li>
<li><a class="menu-level-2" href="./statistics_intro_exercises.html"><b>4.8</b> Statistics intro - Exerc..</a></li>
<li><a class="menu-level-2" href="./statistics_intro_exercises_solutions.html"><b>4.9</b> Statistics intro - Solut..</a></li>
<li><a class="menu-level-1" href="./appendix.html"><b></b> Appendix</a></li>
<li><a class="menu-level-1" href="./references.html"><b>5</b> References</a></li>
</div>
</aside>

<div class="books-content">
<h2 data-number="4.7" id="sec:statistics_intro_hypothesis_testing"><span class="header-section-number">4.7</span> Hypothesis testing</h2>
<p>OK, now we are going to discuss a concept of hypothesis testing. But first let’s go through an example from everyday life that we know or at least can imagine. Ready?</p>
<h3 data-number="4.7.1" id="sec:statistics_intro_tennis"><span class="header-section-number">4.7.1</span> A game of tennis</h3>
<p>So imagine there is a group of people and among them two amateur tennis players: John and Peter. Everyone wants to know which one of them is a better tennis player. Well, there is only one way to find out. Let’s play some games!</p>
<p>As far as I’m aware a tennis match can end with a win of one player, the other loses (there are no draws). Before the games the people set the rules. Everyone agrees that the players will play six games. To prove their supremacy a player must win all six games (six wins in a row are unlikely to happen by accident, I hope we can all agree on that). The series of games ends with the result 0-6 for Peter. According to the previously set rules he is declared the local champion.</p>
<p>Believe it or not but this is what statisticians do. Of course they use more formal methodology and some mathematics, but still, this is what they do:</p>
<ul>
<li><p>before the experiment they start with two assumptions</p>
<ul>
<li>initial assumption: be fair and assume that both players are equally good (this is called <a href="https://en.wikipedia.org/wiki/Null_hypothesis">null hypothesis</a>, <span class="math inline">\(H_{0}\)</span>)</li>
<li>alternative assumption: one player is better than the other (this is called <a href="https://en.wikipedia.org/wiki/Alternative_hypothesis">alternative hypothesis</a>, <span class="math inline">\(H_{A}\)</span>)</li>
</ul></li>
<li><p>before the experiment they decide on how big a sample should be (in our case six games).</p></li>
<li><p>before the experiment they decide on the cutoff level, once it is reached they will abandon their initial assumption and chose the alternative (in our case when a player wins six games in a row)</p></li>
<li><p>they conduct the experiment (players play six games) and record the results</p></li>
<li><p>after the experiment when the result provides enough evidence (in our case six games won by the same player) they decide to reject <span class="math inline">\(H_{0}\)</span>, and choose <span class="math inline">\(H_{A}\)</span>. Otherwise they stick to their initial assumption (do not reject <span class="math inline">\(H_{0}\)</span>)</p></li>
</ul>
<p>And that’s how it is, only that statisticians prefer to rely on probabilities instead of absolute numbers. So in our case a statistician says:</p>
<p>“I assume that <span class="math inline">\(H_{0}\)</span> is true. Then I will conduct the experiment and record then result. I will calculate the probability of such a result (or more extreme result) happening by chance. If it is small enough, let’s say 5% or less, then the result is unlikely to have occurred by accident. Therefore I will reject my initial assumption (<span class="math inline">\(H_{0}\)</span>) and choose the alternative (<span class="math inline">\(H_{A}\)</span>). Otherwise I will stay with my initial assumption.”</p>
<p>Let’s see such a process in practice and connect it with what we already know.</p>
<h3 data-number="4.7.2" id="sec:statistics_intro_tennis_comp_simul"><span class="header-section-number">4.7.2</span> Tennis - computer simulation</h3>
<p>First a computer simulation.</p>
<pre class="language-julia"><code>function getResultOf6TennisGames()
    return sum(rnd.rand(0:1, 6)) # 0 means John won, 1 means Peter won
end

rnd.seed!(321)
tennisGames = [getResultOf6TennisGames() for _ in 1:100_000]
tennisCounts = getCounts(tennisGames)
tennisProbs = getProbs(tennisCounts)</code></pre>
<p>Here <code>getResultOf6TennisGames</code> returns a result of 6 games (in every game each player got the same chance to win). When John wins a game then we get 0, when Peter we get 1. So if after running <code>getResultOf6TennisGames</code> we get, e.g. 4 we know that Peter won 4 games and John won 2 games. We repeat the experiment 100’000 times to get a reliable estimate of the results.</p>
<p>OK, at the beginning of this chapter we intuitively said that a player needs to win 6 games to become the local champion. We know that the result was 0-6 for Peter. Let’s see what is the probability that Peter won by chance six games in a row.</p>
<pre class="language-julia"><code>tennisProbs[6]</code></pre>
<p>0.01538</p>
<p>In this case the probability of Peter winning by chance six games in a row is very small. So it seems that intuitively we set the cutoff level well. Let’s see if the statistician from the quotation above would be satisfied (“If it is small enough, let’s say 5% or less, then the result is unlikely to have occurred by accident. Therefore I will reject my initial assumption (<span class="math inline">\(H_{0}\)</span>) and choose the alternative (<span class="math inline">\(H_{A}\)</span>). Otherwise I will stay with my initial assumption.”)</p>
<pre class="language-julia"><code># in statistics the cutoff level for probability is often called alpha (α)
# 5% = 5/100 = 0.05
function shouldRejectH0(prob::Float64, alpha::Float64 = 0.05)::Bool
    @assert (0 &lt;= prob &lt;= 1) &quot;Probabiliy takes values between 0 and 1&quot;
    @assert (0 &lt;= alpha &lt;= 1) &quot;Probabiliy takes values between 0 and 1&quot;
    return prob &lt;= alpha
end

shouldRejectH0(tennisProbs[6])</code></pre>
<p>true</p>
<p>Indeed he would. He would have to reject <span class="math inline">\(H_{0}\)</span> and assume that Peter is a better player (<span class="math inline">\(H_{A}\)</span>).</p>
<h3 data-number="4.7.3" id="sec:statistics_intro_tennis_theor_calc"><span class="header-section-number">4.7.3</span> Tennis - theoretical calculations</h3>
<p>OK, to be sure of our conclusions let’s try the same with <a href="https://juliastats.org/Distributions.jl/stable/">Distributions</a> package we met before (imported as <code>dsts</code>).</p>
<p>Remember one of two tennis players must win a game (John or Peter). So this is a binomial distributions we met before. We assume (<span class="math inline">\(H_{0}\)</span>) both of them play equally well so the probability of any of them winning is 0.5. Now we can proceed like this using Dictionary comprehensions we have seen before (e.g. see <code>getProbs</code> definition from Section <a href="./statistics_prob_theor_practice.html#sec:statistics_prob_theor_practice">4.4</a>)</p>
<pre class="language-julia"><code>tennisTheorProbs = Dict(i =&gt; dsts.pdf(dsts.Binomial(6, 0.5), i) for i in 0:6)
tennisTheorProbs[6]</code></pre>
<p>0.015624999999999977</p>
<p>Yep, the number is pretty close to <code>tennisProbs[6]</code> we got before which is 0.01538. So we decide to go with <span class="math inline">\(H_{A}\)</span> and say that Peter is a better player. Just in case I will place both distributions (experimental and theoretical) one below the other to make the comparison easier. Behold</p>
<figure>
<img src="./images/tennisExperimTheorDists.png" id="fig:tennisExperimTheorDists" alt="Figure 8: Probability distribution for 6 tennis games if H_{0} is true." /><figcaption aria-hidden="true">Figure 8: Probability distribution for 6 tennis games if <span class="math inline">\(H_{0}\)</span> is true.</figcaption>
</figure>
<p>Once we warmed up we can even calculate the probability using our knowledge from Section <a href="./statistics_intro_probability_properties.html#sec:statistics_intro_probability_summary">4.3.1</a>. We can do this since basically given our null hypothesis (<span class="math inline">\(H_{0}\)</span>) we compared the result of a game between John and Peter to a coin toss (0 or 1, John or Peter, heads or tails).</p>
<p>The probability of Peter winning a single game is <span class="math inline">\(P(Peter) = \frac{1}{2} = 0.5\)</span>. Peter won all six games. In order to get two wins, first he had to won one game. In order to get three wins first he had to won two games, and so on. So he had to win game 1 AND game 2 AND game 3 AND … . Given the above, and what we stated in Section <a href="./statistics_intro_probability_properties.html#sec:statistics_intro_probability_summary">4.3.1</a>, here we deal with probabilities conjunction. Therefore we use probability multiplication like so</p>
<pre class="language-julia"><code>tennisTheorProbWin6games = 0.5 * 0.5 * 0.5 * 0.5 * 0.5 * 0.5
# or
tennisTheorProbWin6games = 0.5 ^ 6

tennisTheorProbWin6games</code></pre>
<p>0.015625</p>
<p>Compare it with <code>tennisThorProbs[6]</code> calculated by <code>Distributions</code> package</p>
<pre class="language-julia"><code>(tennisTheorProbs[6], tennisTheorProbWin6games)</code></pre>
<pre class="output"><code>(0.015624999999999977, 0.015625)</code></pre>
<p>They are the same. The difference is caused by computer representation of floats and rounding (as a reminder see Section <a href="./julia_language_variables.html#sec:julia_float_comparisons">3.3.3</a>, and Section <a href="./julia_language_exercises_solutions.html#sec:julia_language_exercise2_solution">3.9.2</a>).</p>
<p>Anyway I just wanted to present all three methods for two reasons. First, that’s the way we checked our reasoning at math in primary school (solving with different methods). Second, chances are that one of the explanations may be too vague for you, if so help yourself to the other methods :)</p>
<h3 data-number="4.7.4" id="sec:statistics_intro_one_or_two_tails"><span class="header-section-number">4.7.4</span> One or two tails</h3>
<p>Hopefully, the above explanations were clear enough. There is a small nuance to what we did. In the beginning of Section <a href="./statistics_intro_hypothesis_testing.html#sec:statistics_intro_tennis">4.7.1</a> we said ‘To prove their supremacy a player must win all six games.’ A player, so either John or Peter. Still, we calculated only the probability of Peter winning the six games (<code>tennisTheorProbs[6]</code>), Peter and not John. What we did there was calculating <a href="https://en.wikipedia.org/wiki/One-_and_two-tailed_tests">one tail probability</a> (see the figures in the link). Now, take a look at Figure <a href="#fig:tennisExperimTheorDists">8</a> (e.g. bottom) the middle of it is ‘body’ and the edges to the left and right are tails.</p>
<p>This approach (one-tailed test) is rather OK in our case. However, in statistics it is frequently recommended to calculate two-tails probability (usually this is the default option in many statistical functions/packages). That is why at the beginning of Section <a href="./statistics_intro_hypothesis_testing.html#sec:statistics_intro_tennis">4.7.1</a> I wrote ‘alternative assumption: one player is better than the other (this is called alternative hypothesis, <span class="math inline">\(H_{A}\)</span>).’</p>
<p>Calculating the two-tail probability is very simple, we can either add <code>tennisTheorProbs[6] + tennisTheorProbs[0]</code> (remember 0 means that John won all six games) or multiply <code>tennisTheorProbs[6]</code> by 2 (since the graph in Figure <a href="#fig:tennisExperimTheorDists">8</a> is symmetrical).</p>
<pre class="language-julia"><code>(tennisTheorProbs[6] + tennisTheorProbs[0], tennisTheorProbs[6] * 2)</code></pre>
<pre class="output"><code>(0.031249999999999955, 0.031249999999999955)</code></pre>
<p>Once we got it we can perform our reasoning one more time.</p>
<pre class="language-julia"><code>shouldRejectH0(tennisTheorProbs[6] + tennisTheorProbs[0])</code></pre>
<p>true</p>
<p>In this case the decision is the same (but that is not always the case). As I said before in general it is recommended to choose two-tailed test over the one-tailed. Why? Let me try to explain this with another example.</p>
<p>Imagine I tell you that I’m a psychic that talks with the spirits and I know a lot of stuff that is covered to mere mortals (like the rank and suit of a covered <a href="https://en.wikipedia.org/wiki/Playing_card">playing card</a>). You say you don’t believe me and propose a simple test.</p>
<p>You take 10 random cards from a deck. My task is to tell you the color (red or black). And I did, the only problem is that I was wrong every single time! If you think that proves that your were right in the first place then try to guess 10 cards in a row wrongly yourself (if you don’t have cards on you go with 10 consecutive fair coin tosses).</p>
<p>It turns out that guessing 10 cards wrong is just as unlikely as guessing 10 of them right (<code>0.5^10</code> = 0.0009765625 or 1 per 1024 tries in each case). This could potentially mean a few things, e.g.</p>
<ul>
<li>I really talk with the spirits, but in their language “red” means “black,” and “black” means “red” (cultural fun fact: they say Bulgarians nod their heads when they say “no,” and shake them for “yes”),</li>
<li>I live in one of 1024 alternative dimensions/realities and in this reality I managed to guess all of them wrong, when other versions of me had mixed results, and that one version of me guessed all of them right,</li>
<li>I am a superhero and have an x-ray vision in my eyes so I saw the cards, but I decided to tell them wrong to protect my secret identity,</li>
<li>I cheated, and were able to see the cards beforehand, but decided to mock you,</li>
<li>or some other explanation is in order, but I didn’t think of it right now.</li>
</ul>
<p>The small probability only tells us that the result is unlikely to has happened by chance alone. Still, you should always choose your null (<span class="math inline">\(H_{0}\)</span>) and alternative (<span class="math inline">\(H_{A}\)</span>) hypothesis carefully. Moreover, it is a good idea to look at both ends of a probability distribution.</p>
<h3 data-number="4.7.5" id="sec:statistics_intro_errors"><span class="header-section-number">4.7.5</span> All the errors that we make</h3>
<p>Long time ago when I was a student I visited a local chess club. I was late that day, and only one person was without a pair, Paul. I introduced myself and we played a few games. In chess you can either win, lose, or draw a game. Unfortunately, I lost all six games. I was upset, I assumed I just encountered a better player. I thought: “Too bad, but next week I will be on time and find someone else to play with” (nobody likes loosing all the time). Next week I came to the club, and again the only person without a pair was Paul (just my luck). Still, despite the bad feelings I won all six games that we played that day (what are the odds). Later on it turned out that me and Paul are pretty well matched chess players (we played chess at a similar level).</p>
<p>The story demonstrates that even when there is a lot of evidence (six lost games during the first meeting) we can still make an error by rejecting our null hypothesis (<span class="math inline">\(H_{0}\)</span>).</p>
<p>In fact, whenever we do statistics we turn into judges, since we can make a mistake in two ways (see figure below).</p>
<figure>
<img src="./images/judgeVerdict.png" id="fig:judgeVerdict" alt="Figure 9: A judge making a verdict. FP - false positive, FN - false negative." /><figcaption aria-hidden="true">Figure 9: A judge making a verdict. FP - false positive, FN - false negative.</figcaption>
</figure>
<p>An accused is either guilty or innocent. A judge (or a jury in some countries) sets a verdict based on the evidence.</p>
<p>If the accused is innocent but is sentenced anyway then it is an error, it is usually called <a href="https://en.wikipedia.org/wiki/Type_I_and_type_II_errors"><strong>type I error</strong></a> (FP - false positive in Figure <a href="#fig:judgeVerdict">9</a>). Its probability is denoted by the first letter of Greek alphabet, so alpha (α).</p>
<p>In the case of John and Peter playing tennis the type I probability was <span class="math inline">\(\le\)</span> 0.05. More precisely it was <code>tennisTheorProbs[6]</code> = 0.015625 (for a one tailed test).</p>
<p>If the accused is guilty but is declared innocent then it is another type of error, it is usually called <strong>type II error</strong> (FN - false negative in Figure <a href="#fig:judgeVerdict">9</a>). Its probability is denoted by the second letter of Greek alphabet, so beta (β). Beta helps us determine <a href="https://en.wikipedia.org/wiki/Power_of_a_test">the power of a test</a> (power = 1 - β), i.e. if <span class="math inline">\(H_{A}\)</span> is really true then how likely it is that we will choose <span class="math inline">\(H_{A}\)</span> over <span class="math inline">\(H_{0}\)</span>.</p>
<p>So to sum up, in the judge analogy innocent is <span class="math inline">\(H_{0}\)</span> being true and guilty is <span class="math inline">\(H_{A}\)</span> being true.</p>
<p>Unfortunately, most of the statistical textbooks that I’ve read revolve around type I errors and alphas, whereas type II error is covered much less extensively (hence my own knowledge of the topic is more limited).</p>
<p>In the tennis example above we rejected <span class="math inline">\(H_{0}\)</span>, hence here we risk committing type I error. Therefore, we didn’t speak about type II error, but don’t worry we will discuss it in more detail in the upcoming exercises at the end of this chapter.</p>
<h3 data-number="4.7.6" id="sec:statistics_intro_cutoff_levels"><span class="header-section-number">4.7.6</span> Cutoff levels</h3>
<p>OK, once we know what are the type I and type II errors it is time to discuss their cutoff values.</p>
<p>Obviously, the ideal situation would be if the probabilities of both type I and type II errors were exactly 0 (no mistakes is always the best). The only problem is that this is not possible. In our tennis example one player won all six games, and still some small risk of a mistake existed (<code>tennisTheorProbs[6] =</code> 0.015625). If you ever see a statistical package reporting p-value equal, e.g. 0.0000, then this is just rounding to 4 decimal places and not an actual zero. So what are the acceptable cutoff levels for <span class="math inline">\(\alpha\)</span> (probability of type I error) and <span class="math inline">\(\beta\)</span> (probability of type II error).</p>
<p>The most popular choices for <span class="math inline">\(\alpha\)</span> are usually:</p>
<ul>
<li>0.05, or</li>
<li>0.01</li>
</ul>
<p>Actually, as far as I’m aware, the first of them (<span class="math inline">\(\alpha = 0.05\)</span>) was initially proposed by <a href="https://en.wikipedia.org/wiki/Ronald_Fisher">Ronald Fisher</a>, a person sometimes named the father of XX-century statistics. This value was chosen arbitrarily and is currently frowned upon by some modern statisticians as being to lenient. Therefore, 0.01 is proposed as a more reasonable alternative.</p>
<p>As regards <span class="math inline">\(\beta\)</span> its two most commonly accepted values are:</p>
<ul>
<li>0.2, or</li>
<li>0.1</li>
</ul>
<p>Actually, as far as I remember the textbooks usually do not report values for <span class="math inline">\(\beta\)</span>, but for power of the test (if <span class="math inline">\(H_{A}\)</span> is really true then how likely it is that we will choose <span class="math inline">\(H_{A}\)</span> over <span class="math inline">\(H_{0}\)</span>) to be 0.8 or 0.9. However, since as we mentioned earlier power = 1 - <span class="math inline">\(\beta\)</span>, then we can easily calculate the value for this parameter.</p>
<p>OK, enough of theory, time for some practice. Whenever you’re ready click the right arrow to proceed to the exercises I prepared for you.</p>


<div class="bottom-nav">
    <p id="nav-prev" style="text-align: left;">
        <a class="menu-level-2" href="./statistics_normal_distribution.html"><b>4.6</b> Normal distribution</a> <kbd>←</kbd>
        <span id="nav-next" style="float: right;">
            <kbd>→</kbd> <a class="menu-level-2" href="./statistics_intro_exercises.html"><b>4.8</b> Statistics intro - Exerc..</a>
        </span>
    </p>
</div>


<div class="license">
    <br/>
  <br/>
  <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>
    Bartlomiej Lukaszuk
</div>
</div>
</div>
</body>
</html>