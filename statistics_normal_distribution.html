<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Bartlomiej Lukaszuk" />
  <title>Normal distribution - Romeo and Julia</title>
  <link rel="stylesheet" href="./style.css"/>
    <script src="./mousetrap.min.js"></script>
    <style>
  @font-face {
    font-family: JuliaMono-Regular;
    src: url("./JuliaMono-Regular.woff2");
  }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <link rel="stylesheet" href="./github.min.css">
<script src="./highlight.min.js"></script>
<script src="./julia.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', (event) => {
    document.querySelectorAll('pre').forEach((el) => {
        if (!el.classList.contains('output')) {
            hljs.highlightElement(el);
        }
    });
});
</script>
 
</head>
<body>
<script>
function click_next() {
  var next = document.getElementById('nav-next');
  next.firstElementChild.nextElementSibling.click();
}
function click_prev() {
  var prev = document.getElementById('nav-prev');
  prev.firstElementChild.click();
}
Mousetrap.bind('right', click_next);
Mousetrap.bind('h', click_prev);
Mousetrap.bind('left', click_prev);
Mousetrap.bind('l', click_next);
</script>

<div class="books-container">
<aside class="books-menu">
<input type="checkbox" id="menu">
<label for="menu">☰</label>
<div class="books-title">
<a href="./">Romeo and Julia</a>
</div><br />
<span class="books-subtitle">
where Romeo is Basic Statistics
</span>
<div class="books-menu-content">
<li><a class="menu-level-1" href="./about.html"><b>1</b> About</a></li>
<li><a class="menu-level-1" href="./why_julia.html"><b>2</b> Why Julia</a></li>
<li><a class="menu-level-2" href="./julia_is_fast.html"><b>2.1</b> Julia is fast</a></li>
<li><a class="menu-level-2" href="./julia_is_simple.html"><b>2.2</b> Julia is simple</a></li>
<li><a class="menu-level-2" href="./jl_pleasure_to_write.html"><b>2.3</b> Pleasure to write</a></li>
<li><a class="menu-level-2" href="./jl_not_mainstream.html"><b>2.4</b> Not mainstream</a></li>
<li><a class="menu-level-2" href="./jl_open_source.html"><b>2.5</b> Julia is free</a></li>
<li><a class="menu-level-1" href="./julia_first_encounter.html"><b>3</b> Julia - first encounter</a></li>
<li><a class="menu-level-2" href="./julia_installation.html"><b>3.1</b> Installation</a></li>
<li><a class="menu-level-2" href="./julia_language_constructs.html"><b>3.2</b> Language Constructs</a></li>
<li><a class="menu-level-2" href="./julia_language_variables.html"><b>3.3</b> Variables</a></li>
<li><a class="menu-level-2" href="./julia_language_functions.html"><b>3.4</b> Functions</a></li>
<li><a class="menu-level-2" href="./julia_language_decision_making.html"><b>3.5</b> Decision Making</a></li>
<li><a class="menu-level-2" href="./julia_language_repetition.html"><b>3.6</b> Repetition</a></li>
<li><a class="menu-level-2" href="./julia_language_libraries.html"><b>3.7</b> Additional libraries</a></li>
<li><a class="menu-level-2" href="./julia_language_exercises.html"><b>3.8</b> Julia - Exercises</a></li>
<li><a class="menu-level-2" href="./julia_language_exercises_solutions.html"><b>3.9</b> Julia - Solutions</a></li>
<li><a class="menu-level-1" href="./statistics_intro.html"><b>4</b> Statistics - introduction</a></li>
<li><a class="menu-level-2" href="./statistics_intro_imports.html"><b>4.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./statistics_intro_probability_definition.html"><b>4.2</b> Probability - definition</a></li>
<li><a class="menu-level-2" href="./statistics_intro_probability_properties.html"><b>4.3</b> Probability - properties</a></li>
<li><a class="menu-level-2" href="./statistics_prob_theor_practice.html"><b>4.4</b> Probability - theory and..</a></li>
<li><a class="menu-level-2" href="./statistics_prob_distribution.html"><b>4.5</b> Probability distribution</a></li>
<li><a class="menu-level-2" href="./statistics_normal_distribution.html"><b>4.6</b> Normal distribution</a></li>
<li><a class="menu-level-2" href="./statistics_intro_hypothesis_testing.html"><b>4.7</b> Hypothesis testing</a></li>
<li><a class="menu-level-1" href="./appendix.html"><b></b> Appendix</a></li>
<li><a class="menu-level-1" href="./references.html"><b>5</b> References</a></li>
</div>
</aside>

<div class="books-content">
<h2 data-number="4.6" id="sec:statistics_normal_distribution"><span class="header-section-number">4.6</span> Normal distribution</h2>
<p>Let’s start where we left. We know that a probability distribution is a (possibly graphical) depiction of the values that probability takes for any possible outcome. Probabilities come in different forms and shapes. Additionally one probability distribution can transform into another (or at least into a distribution that resembles another distribution).</p>
<p>Let’s look at a few examples.</p>
<figure>
<img src="./images/binomAndMultinomDistr.png" id="fig:unifAndBinomDistr" alt="Figure 4: Experimental binomial and multinomial probability distributions." /><figcaption aria-hidden="true">Figure 4: Experimental binomial and multinomial probability distributions.</figcaption>
</figure>
<p>Here we got experimental distributions for tossing a standard fair coin and rolling a six-sided dice. The code for Figure <a href="#fig:unifAndBinomDistr">4</a> can be found in <a href="https://github.com/b-lukaszuk/RJ_BS_eng/tree/main/code_snippets">the code snippets for this chapter</a> and it uses the same functions that we developed in the previous chapter(s).</p>
<p>Those are examples of the binomial (<code>bi</code> - two, <code>nomen</code> - name, those two names could be: heads/tails, A/B, or most general success/failure) and multinomial (<code>multi</code> - many, <code>nomen</code> - name, here the names are <code>1:6</code>) distributions. Moreover, both of them are examples of discrete (probability is calculated for a few distinctive values) and uniform (values are equally likely to be observed) distribution.</p>
<p>Notice that in the Figure <a href="#fig:unifAndBinomDistr">4</a> (above) rolling one six-sided dice gives us an uniform distribution. However in the previous chapter when tossing two six-sided dice we got the distribution that looks like this.</p>
<figure>
<img src="./images/rolling2diceProbs.png" id="fig:rolling2diceProbs" alt="Figure 5: Experimental probability distribution for rolling two 6-sided dice." /><figcaption aria-hidden="true">Figure 5: Experimental probability distribution for rolling two 6-sided dice.</figcaption>
</figure>
<p>What we got here is a <a href="https://en.wikipedia.org/wiki/Bell">bell</a> shaped distribution (c’mon use your imagination). It turns out that quite a few distributions may transform into the distribution that is bell shaped (as an exercise you may want to draw a distribution for the number of heads when tossing 10 fair coins simultaneously). Moreover, many biological phenomena got a bell shaped distribution, e.g. men’s height or the famous <a href="https://en.wikipedia.org/wiki/Intelligence_quotient">intelligence quotient</a> (aka IQ). The theoretical name for it is <a href="https://en.wikipedia.org/wiki/Normal_distribution">normal distribution</a>. Placed on a graph it looks like this.</p>
<figure>
<img src="./images/normDistribution.png" id="fig:normDistribution" alt="Figure 6: Examples of normal distribution." /><figcaption aria-hidden="true">Figure 6: Examples of normal distribution.</figcaption>
</figure>
<p>In Figure <a href="#fig:normDistribution">6</a> the upper panel depicts standard normal distributions (<span class="math inline">\(\mu = 0, \sigma = 1\)</span>, explanation in a moment), a theoretical distribution that all statisticians and probably some mathematicians love. The bottom panel shows a distribution that is likely closer to the adult males’ height distribution in my country. Long time ago I read that the average height for an adult man in Poland is 172 [cm] (5.64 [feet]) and standard deviation is equal to 7 [cm] (2.75 [inch]) hence this plot.</p>
<blockquote>
<p><strong><em>Note:</em></strong> In order to get real height distribution you should probably visit a country’s statistics office instead relying on information like mine.</p>
</blockquote>
<p>As you can see normal distribution is often depicted as a line plot. That is because it is a continuous distribution (the values on x axes can take any number from a given range). Take a look at the height. In my old <a href="https://en.wikipedia.org/wiki/Polish_identity_card">identity card</a> next to the field “Height in cm” stands “181,” but is this really my precise height? What if during a measurement the height was 180.7 or 181.3 and in the ID there could be only height in integers. I would have to round it, right? So based on the identity card information my real height is probably somewhere between 180.5 and 181.49999… . Moreover, it can be any value in between (like 180.6354551…, although in reality a measuring device does not have such a precision). So, in the bottom panel of Figure <a href="#fig:normDistribution">6</a> I rounded theoretical values for height (<code>round(height, digits=0)</code>) obtained from <code>rnd.rand(dsts.Normal(172, 7), 10_000_000)</code>, drew bars (using <code>cmk.barplot</code> that you know), and added a line that goes through the middle of each bar.</p>
<p>As you perhaps noticed, the distribution is characterized by two parameters:</p>
<ul>
<li>the average (also called the mean) (in population denoted as: <span class="math inline">\(\mu\)</span>, in sample as: <span class="math inline">\(\overline{x}\)</span>)</li>
<li>the standard deviation (in population denoted as: <span class="math inline">\(\sigma\)</span>, in sample as: <span class="math inline">\(s\)</span>, <span class="math inline">\(sd\)</span> or <span class="math inline">\(std\)</span>)</li>
</ul>
<p>We already know the first one from school and previous chapters (e.g. <code>getAvg</code> from Section <a href="./julia_language_repetition.html#sec:julia_language_for_loops">3.6.1</a>). The last one however requires some explanation.</p>
<p>Let’s say that we have two students. Here are their grades.</p>
<pre class="language-julia"><code>gradesStudA = [3.0, 3.5, 5.0, 4.5, 4.0]
gradesStudB = [6.0, 5.5, 1.5, 1.0, 6.0]</code></pre>
<p>Imagine that we want to send one student to represent the school in a national level competition. Therefore we want to know who is a better student. So, let’s check their averages.</p>
<pre class="language-julia"><code>avgStudA = getAvg(gradesStudA)
avgStudB = getAvg(gradesStudB)
(avgStudA, avgStudB)</code></pre>
<pre class="output"><code>(4.0, 4.0)</code></pre>
<p>Hmm, they are identical. OK, in that situation let’s see who is more consistent with their scores.</p>
<p>To test the spread of the scores around the mean we will subtract every single score from the mean and take their average (average of the differences).</p>
<pre class="language-julia"><code>diffsStudA = gradesStudA .- avgStudA
diffsStudB = gradesStudB .- avgStudB
(getAvg(diffsStudA), getAvg(diffsStudB))</code></pre>
<pre class="output"><code>(0.0, 0.0)</code></pre>
<blockquote>
<p><strong><em>Note:</em></strong> Here we used the dot functions described in Section <a href="./julia_language_repetition.html#sec:julia_language_dot_functions">3.6.6</a></p>
</blockquote>
<p>The method is of no use since <code>sum(diffs)</code> is always equal to 0 (and hence the average is 0). See for yourself</p>
<pre class="language-julia"><code>(sum(diffsStudA), sum(diffsStudB))</code></pre>
<pre class="output"><code>(0.0, 0.0)</code></pre>
<p>Personally in this situation I would take the average of diffs without looking at the sign (<code>abs</code> function does that) like so.</p>
<pre class="language-julia"><code>absDiffsStudA = abs.(diffsStudA)
absDiffsStudB = abs.(diffsStudB)
(getAvg(absDiffsStudA), getAvg(absDiffsStudB))</code></pre>
<pre class="output"><code>(0.6, 2.2)</code></pre>
<p>Based on this we would say that student A is more consistent in his grades so he is probably a better student of the two. I would send student A to represent the school during the national level competition. Student B is also good, but choosing him is a gamble. He could shine or embarrass himself (and spot the school’s name) during the competition.</p>
<p>For any reason statisticians decided to get rid of the sign in a different way, i.e. by squaring (<span class="math inline">\(x^{2}\)</span>) the diffs. Afterwards they calculated the average and took square root (<span class="math inline">\(\sqrt{x}\)</span>) of it to get rid of the squaring. So, they did more or less this</p>
<pre class="language-julia"><code>function getSd(nums::Vector{&lt;:Real})::Real
    avg::Real = getAvg(nums)
    diffs::Vector{&lt;:Real} = nums .- avg
    squaredDiffs::Vector{&lt;:Real} = diffs .^ 2
    return sqrt(getAvg(squaredDiffs))
end

(getSd(gradesStudA), getSd(gradesStudB))</code></pre>
<pre class="output"><code>(0.7071067811865476, 2.258317958127243)</code></pre>
<blockquote>
<p><strong><em>Note:</em></strong> In reality standard deviation for a sample is calculated with a slightly different formula but the one above is easier to understand.</p>
</blockquote>
<p>In the end we got similar numbers, reasoning, and conclusions to the one based on <code>abs</code> function.</p>
<p>Although I like my method better the <code>sd</code> and squaring/square rooting is so deeply fixed into statistics that everyone should know it.</p>
<p>And now a big question.</p>
<p><strong>Why should we care about the mean (<span class="math inline">\(\mu\)</span>, <span class="math inline">\(\overline{x}\)</span>) or sd (<span class="math inline">\(\sigma\)</span>, <span class="math inline">\(s\)</span>, <span class="math inline">\(sd\)</span>, <span class="math inline">\(std\)</span>) anyway?</strong></p>
<p>The answer. For practical reasons that got something to do with the so called <a href="https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule">three sigma rule</a>.</p>
<h3 data-number="4.6.1" id="sec:statistics_intro_three_sigma_rule"><span class="header-section-number">4.6.1</span> The three sigma rule</h3>
<p><a href="https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule">The rule</a> says that:</p>
<ul>
<li>roughly 68% of the results in the population lie within <span class="math inline">\(\pm\)</span> 1 sd from the mean</li>
<li>roughly 95% of the results in the population lie within <span class="math inline">\(\pm\)</span> 2 sd from the mean</li>
<li>roughly 99% of the results in the population lie within <span class="math inline">\(\pm\)</span> 3 sd from the mean</li>
</ul>
<p><strong>Example 1</strong></p>
<p>Have you ever tested your <a href="https://en.wikipedia.org/wiki/Blood">blood</a> and received the lab results that said something like</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Complete_blood_count#Reference_ranges">RBC</a>: 4.45 [<span class="math inline">\(10^{12}/\mu L\)</span>] (4.2 - 6.00)</li>
</ul>
<p>The RBC stands for <strong>r</strong>ed <strong>b</strong>lood <strong>c</strong>ell count and the parenthesis contain the reference values (if you are within this normal range then it is a good sign). But where did those reference values come from? This <a href="https://en.wikipedia.org/wiki/Blood">wikipedia’s page</a> gives us a clue. It reports a value for <a href="https://en.wikipedia.org/wiki/Hematocrit">hematocrit</a> (which is in %) to be:</p>
<ul>
<li>45 <span class="math inline">\(\pm\)</span> 7 (38–52%) for males</li>
<li>42 <span class="math inline">\(\pm\)</span> 5 (37–47%) for females</li>
</ul>
<p>Look at this <span class="math inline">\(\pm\)</span> symbol. Have you seen it before? No? Then look at the three sigma rule above.</p>
<p>The reference values were most likely composed in the following way. A large number (let’s say 30’000) females gave their blood for testing. Hematocrit value was calculated for all of them. The distribution was established in a similar way that we did it before. The average hematocrit was 42 units, the standard deviation was 5 units. The majority of the results (roughly 68%) lie within <span class="math inline">\(\pm\)</span> 1 sd from the mean. If so, then we got 42 - 5 = 37, and 42 + 5 = 47. And that is how those two values were considered to be the reference values for the population. Most likely the same is true for any other reference values you see in your lab result when you <a href="https://en.wikipedia.org/wiki/Complete_blood_count">test your blood</a> or when you perform other medical examination.</p>
<p><strong>Example 2</strong></p>
<p>Let’s say a person named Peter lives in Poland. Peter approaches the famous IQ test in one of our universities. He read on the internet that there are different <a href="https://en.wikipedia.org/wiki/Intelligence_quotient#Current_tests">intelligence scales</a> used throughout the world. His score is 125. The standard deviation is 24. Is his score high, does it indicate he is gifted (a genius level intellect)? Well, in order to be a genius one has to be in the top 2% of the population with respect to their IQ value. What is the location of Peter’s IQ value in the population.</p>
<p>The score of 125 is just a bit greater than 1 standard deviation above the mean (which in an IQ test is always 100). From Section <a href="./statistics_prob_distribution.html#sec:statistics_prob_distribution">4.5</a> we know that when we add all the probabilities we get 1 (so the area under the curve in Figure <a href="#fig:normDistribution">6</a> is equal to 1). Half of the area lies on the left, half of it on the right (1 / 2 = 0.5). So, a person with IQ = 100 is as intelligent or more intelligent than half the people (<span class="math inline">\(\frac{1}{2}\)</span> = 0.5 = 50%) in the population. Roughly 68% of the results lies within 1 sd from the mean (half of it below, half of it above). So, from IQ = 100 to IQ = 124 we got (68% / 2 = 34%). By adding 50% (IQ <span class="math inline">\(\le\)</span> 100) to 34% (100 <span class="math inline">\(\le\)</span> IQ <span class="math inline">\(\le\)</span> 124) we get 50% + 34% = 84%. Therefore in our case Peter (with his IQ = 125) is more intelligent than 84% of people in the population (so top 16% of the population). His intelligence is above the average, but it is not enough to label him a genius.</p>
<h3 data-number="4.6.2" id="sec:statistics_intro_distributions_package"><span class="header-section-number">4.6.2</span> Distributions package</h3>
<p>This is all nice and good to know, but in practice it is slow and not precise enough. What if in the previous example the IQ was let’s say 139. What is the percentage of people more intelligent than Peter. In the past that kind of questions were to be answered with satisfactory precision using statistical tables at the end of a textbook. Nowadays it can be quickly answered with a greater exactitude and speed, e.g. with <a href="https://juliastats.org/Distributions.jl/stable/">Distributions</a> package. For instance in the case of Peter described above we get</p>
<pre class="language-julia"><code>import Distributions as dsts

dsts.cdf(dsts.Normal(100, 24), 139)</code></pre>
<p>0.9479187205847805</p>
<p>Here we first create a normal distribution with <span class="math inline">\(\mu\)</span> = 100 and <span class="math inline">\(\sigma\)</span> = 24 (<code>dsts.Normal(100, 24)</code>). Then we sum all the probabilities <span class="math inline">\(\le\)</span> 139 with <code>dsts.cdf</code> and see that in this case only ~5% of people are as intelligent or more intelligent than Peter. BTW, <code>cdf</code> stands for <a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function">cumulative distribution function</a>. For more information on <code>dsts.cdf</code> see <a href="https://juliastats.org/Distributions.jl/stable/univariate/#Distributions.cdf-Tuple%7BUnivariateDistribution,%20Real%7D">these docs</a> or for <code>dsts.Normal</code> <a href="https://juliastats.org/Distributions.jl/stable/univariate/#Distributions.Normal">those docs</a>.</p>
<p>To further consolidate our knowledge. Let’s go with another example. Remember that I’m 181 cm tall. Hmm, I wonder what percentage of men in Poland is taller than me if <span class="math inline">\(\mu = 172\)</span> [cm] and <span class="math inline">\(\sigma = 7\)</span> [cm].</p>
<pre class="language-julia"><code>1 - dsts.cdf(dsts.Normal(172, 7), 181)</code></pre>
<p>0.09927139684333097</p>
<p>The <code>dsts.cdf</code> gives me left side of the curve (height <span class="math inline">\(\le\)</span> 181). So in order to get those that are higher than me I subtract it from 1. It seems that under those assumptions roughly 10% of men in Poland are taller than me (approx. 1 out of 10 men that I encounter is taller than me).</p>
<p>OK, and how many men in Poland are exactly as tall as I am? In general that is the job for <code>dsts.pdf</code> (<code>pdf</code> stands for <a href="https://en.wikipedia.org/wiki/Probability_density_function">probablity density function</a>, see <a href="https://juliastats.org/Distributions.jl/stable/univariate/#Distributions.pdf-Tuple%7BUnivariateDistribution,%20Real%7D">the docs for Distributions.pdf</a>). It works pretty well for discrete distributions (we talked about them at the beginning of this sub-chapter). For instance theoretical probability of getting 12 while rolling two six-sided dice is</p>
<pre class="language-julia"><code>dsts.pdf(dsts.Binomial(2, 1/6), 2)</code></pre>
<p>0.02777777777777778</p>
<p>Compare it with the empirical probability from Section <a href="./statistics_prob_distribution.html#sec:statistics_prob_distribution">4.5</a> which was equal to 0.0278. Here we treated it as a binomial distribution (success: two sixes (6 + 6 = 12), failure: other result) hence <code>dsts.Binomial</code> with <code>2</code> (number of dice to roll) and <code>1/6</code> (probability of getting 6 in a single roll). Then we used <code>dsts.pdf</code> to get the probability of getting exactly two sixes. More info on <code>dsts.Binomial</code> can be found <a href="https://juliastats.org/Distributions.jl/stable/univariate/#Distributions.Binomial">here</a> and on <code>dsts.pdf</code> can be found <a href="https://juliastats.org/Distributions.jl/stable/univariate/#Distributions.pdf-Tuple%7BUnivariateDistribution,%20Real%7D">there</a>.</p>
<p>However there is a problem with using <code>dsts.pdf</code> for continues distributions because it can take any of the infinite values within the range. Remember, in theory there is an infinite number of values between 180 and 181 (like 180.1111, 180.12222, etc.). So usually for practical reasons it is recommended not to calculate a probability density function (hence <code>pdf</code>) for a continuous distribution (1 / infinity <span class="math inline">\(\approx\)</span> 0). Still, remember that the height of 181 [cm] means that the value lies somewhere between 180.5 and 181.49999… . Moreover, we can reliably calculate the probabilities (with <code>dsts.cdf</code>) for <span class="math inline">\(\le\)</span> 180.5 and <span class="math inline">\(\le\)</span> 181.49999… so a good approximation would be</p>
<pre class="language-julia"><code>heightDist = dsts.Normal(172, 7)
# 2 digits after dot because of the assumed precision of a measuring device
dsts.cdf(heightDist, 181.49) - dsts.cdf(heightDist, 180.50)</code></pre>
<p>0.024724273314878698</p>
<p>OK. So it seems that roughly 2.5% of adult men in Poland got 181 [cm] in the field “Height” in their identity cards. If there are let’s say 10 million adult men in Poland then rougly 250000.0 (so 250 k) people are approximately my height.</p>
<p>If you are still confused about this method take a look at the figure below.</p>
<figure>
<img src="./images/normDistCdfUsage.png" id="fig:normDistCdfUsage" alt="Figure 7: Using cdf to calculate proportion of men that are between 170 and 180 [cm] tall." /><figcaption aria-hidden="true">Figure 7: Using cdf to calculate proportion of men that are between 170 and 180 [cm] tall.</figcaption>
</figure>
<p>Here for better separation I placed the height of men between 170 and 180 [cm]. The method that I used subtracts the area in blue from the area in red (red - blue). That is exactly what I did (but for 181.49 and 180.50 [cm]) when I typed <code>dsts.cdf(heightDist, 181.49) - dsts.cdf(heightDist, 180.50)</code> above.</p>
<p>OK, time for the last theoretical sub-chapter in this section. Whenever you’re ready click on the right arrow.</p>


<div class="bottom-nav">
    <p id="nav-prev" style="text-align: left;">
        <a class="menu-level-2" href="./statistics_prob_distribution.html"><b>4.5</b> Probability distribution</a> <kbd>←</kbd>
        <span id="nav-next" style="float: right;">
            <kbd>→</kbd> <a class="menu-level-2" href="./statistics_intro_hypothesis_testing.html"><b>4.7</b> Hypothesis testing</a>
        </span>
    </p>
</div>


<div class="license">
    <br/>
  <br/>
  <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>
    Bartlomiej Lukaszuk
</div>
</div>
</div>
</body>
</html>