<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Bartlomiej Lukaszuk" />
  <title>Exercises - Association and Prediction - Romeo and Julia, where Romeo is Basic Statistics</title>
  <link rel="stylesheet" href="./style.css"/>
    <script src="./mousetrap.min.js"></script>
    <style>
  @font-face {
    font-family: JuliaMono-Regular;
    src: url("./JuliaMono-Regular.woff2");
  }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <link rel="stylesheet" href="./github.min.css">
<script src="./highlight.min.js"></script>
<script src="./julia.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', (event) => {
    document.querySelectorAll('pre').forEach((el) => {
        if (!el.classList.contains('output')) {
            hljs.highlightElement(el);
        }
    });
});
</script>
 
</head>
<body>
<script>
function click_next() {
  var next = document.getElementById('nav-next');
  next.firstElementChild.nextElementSibling.click();
}
function click_prev() {
  var prev = document.getElementById('nav-prev');
  prev.firstElementChild.click();
}
Mousetrap.bind('right', click_next);
Mousetrap.bind('h', click_prev);
Mousetrap.bind('left', click_prev);
Mousetrap.bind('l', click_next);
</script>

<div class="books-container">
<aside class="books-menu">
<input type="checkbox" id="menu">
<label for="menu">☰</label>
<div class="books-title">
<a href="./">Romeo and Julia, where Romeo is Basic Statistics</a>
</div><br />
<span class="books-subtitle">

</span>
<div class="books-menu-content">
<li><a class="menu-level-1" href="./about.html"><b>1</b> About</a></li>
<li><a class="menu-level-1" href="./why_julia.html"><b>2</b> Why Julia</a></li>
<li><a class="menu-level-2" href="./julia_is_fast.html"><b>2.1</b> Julia is fast</a></li>
<li><a class="menu-level-2" href="./julia_is_simple.html"><b>2.2</b> Julia is simple</a></li>
<li><a class="menu-level-2" href="./jl_pleasure_to_write.html"><b>2.3</b> Pleasure to write</a></li>
<li><a class="menu-level-2" href="./jl_not_mainstream.html"><b>2.4</b> Not mainstream</a></li>
<li><a class="menu-level-2" href="./jl_open_source.html"><b>2.5</b> Julia is free</a></li>
<li><a class="menu-level-1" href="./julia_first_encounter.html"><b>3</b> Julia - first encounter</a></li>
<li><a class="menu-level-2" href="./julia_installation.html"><b>3.1</b> Installation</a></li>
<li><a class="menu-level-2" href="./julia_language_constructs.html"><b>3.2</b> Language Constructs</a></li>
<li><a class="menu-level-2" href="./julia_language_variables.html"><b>3.3</b> Variables</a></li>
<li><a class="menu-level-2" href="./julia_language_functions.html"><b>3.4</b> Functions</a></li>
<li><a class="menu-level-2" href="./julia_language_decision_making.html"><b>3.5</b> Decision Making</a></li>
<li><a class="menu-level-2" href="./julia_language_repetition.html"><b>3.6</b> Repetition</a></li>
<li><a class="menu-level-2" href="./julia_language_libraries.html"><b>3.7</b> Additional libraries</a></li>
<li><a class="menu-level-2" href="./julia_language_exercises.html"><b>3.8</b> Julia - Exercises</a></li>
<li><a class="menu-level-2" href="./julia_language_exercises_solutions.html"><b>3.9</b> Julia - Solutions</a></li>
<li><a class="menu-level-1" href="./statistics_intro.html"><b>4</b> Statistics - introduction</a></li>
<li><a class="menu-level-2" href="./statistics_intro_imports.html"><b>4.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./statistics_intro_probability_definition.html"><b>4.2</b> Probability - definition</a></li>
<li><a class="menu-level-2" href="./statistics_intro_probability_properties.html"><b>4.3</b> Probability - properties</a></li>
<li><a class="menu-level-2" href="./statistics_prob_theor_practice.html"><b>4.4</b> Probability - theory and..</a></li>
<li><a class="menu-level-2" href="./statistics_prob_distribution.html"><b>4.5</b> Probability distribution</a></li>
<li><a class="menu-level-2" href="./statistics_normal_distribution.html"><b>4.6</b> Normal distribution</a></li>
<li><a class="menu-level-2" href="./statistics_intro_hypothesis_testing.html"><b>4.7</b> Hypothesis testing</a></li>
<li><a class="menu-level-2" href="./statistics_intro_exercises.html"><b>4.8</b> Statistics intro - Exerc..</a></li>
<li><a class="menu-level-2" href="./statistics_intro_exercises_solutions.html"><b>4.9</b> Statistics intro - Solut..</a></li>
<li><a class="menu-level-1" href="./compare_contin_data.html"><b>5</b> Comparisons - continuous d..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_imports.html"><b>5.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_one_samp_ttest.html"><b>5.2</b> One sample Student’s t..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_two_samp_ttest.html"><b>5.3</b> Two samples Student’s ..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_one_way_anova.html"><b>5.4</b> One-way ANOVA</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_post_hoc_tests.html"><b>5.5</b> Post-hoc tests</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_multip_correction.html"><b>5.6</b> Multiplicity correction</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_exercises.html"><b>5.7</b> Exercises - Comparisons ..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_exercises_solutions.html"><b>5.8</b> Solutions - Comparisons ..</a></li>
<li><a class="menu-level-1" href="./compare_categ_data.html"><b>6</b> Comparisons - categorical ..</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_imports.html"><b>6.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_flashback.html"><b>6.2</b> Flashback</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_chisq_test.html"><b>6.3</b> Chi squared test</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_fisher_exact_text.html"><b>6.4</b> Fisher’s exact test</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_bigger_table.html"><b>6.5</b> Bigger table</a></li>
<li><a class="menu-level-2" href="./compare_categ_test_for_independence.html"><b>6.6</b> Test for independence</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_exercises.html"><b>6.7</b> Exercises - Comparisons ..</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_exercises_solutions.html"><b>6.8</b> Solutions - Comparisons ..</a></li>
<li><a class="menu-level-1" href="./assoc_pred.html"><b>7</b> Association and Prediction</a></li>
<li><a class="menu-level-2" href="./assoc_pred_imports.html"><b>7.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./assoc_pred_lin_relation.html"><b>7.2</b> Linear relation</a></li>
<li><a class="menu-level-2" href="./assoc_pred_covariance.html"><b>7.3</b> Covariance</a></li>
<li><a class="menu-level-2" href="./assoc_pred_correlation.html"><b>7.4</b> Correlation</a></li>
<li><a class="menu-level-2" href="./assoc_pred_corr_pitfalls.html"><b>7.5</b> Correlation Pitfalls</a></li>
<li><a class="menu-level-2" href="./assoc_pred_simple_lin_reg.html"><b>7.6</b> Simple Linear Regression</a></li>
<li><a class="menu-level-2" href="./assoc_pred_multiple_lin_reg.html"><b>7.7</b> Multiple Linear Regressi..</a></li>
<li><a class="menu-level-2" href="./assoc_pred_exercises.html"><b>7.8</b> Exercises - Association ..</a></li>
<li><a class="menu-level-2" href="./assoc_pred_exercises_solutions.html"><b>7.9</b> Solutions - Association</a></li>
<li><a class="menu-level-1" href="./time_to_say_goodbye.html"><b>8</b> Time to say goodbye</a></li>
</div>
</aside>

<div class="books-content">
<h2 data-number="7.8" id="sec:assoc_pred_exercises"><span class="header-section-number">7.8</span> Exercises - Association and Prediction</h2>
<p>Just like in the previous chapters here you will find some exercises that you may want to solve to get from this chapter as much as you can (best option). Alternatively, you may read the task descriptions and the solutions (and try to understand them).</p>
<h3 data-number="7.8.1" id="sec:assoc_pred_ex1"><span class="header-section-number">7.8.1</span> Exercise 1</h3>
<p>The <code>RDatasets</code> package mentioned in Section <a href="./assoc_pred_corr_pitfalls.html#sec:assoc_pred_corr_pitfalls">7.5</a> contains a lot of interesting data. For instance the <a href="https://vincentarelbundock.github.io/Rdatasets/doc/MASS/Animals.html">Animals</a> data frame.</p>
<pre class="language-julia"><code>animals = RD.dataset(&quot;MASS&quot;, &quot;Animals&quot;)
first(animals, 5)
</code></pre>
<div id="tbl:animalsDf">
<table>
<caption>Table 15: DataFrame for brain and body weights of 28 animal species.</caption>
<thead>
<tr class="header">
<th style="text-align: right;">Species</th>
<th style="text-align: right;">Body</th>
<th style="text-align: right;">Brain</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">Mountain beaver</td>
<td style="text-align: right;">1.35</td>
<td style="text-align: right;">8.1</td>
</tr>
<tr class="even">
<td style="text-align: right;">Cow</td>
<td style="text-align: right;">465.0</td>
<td style="text-align: right;">423.0</td>
</tr>
<tr class="odd">
<td style="text-align: right;">Grey wolf</td>
<td style="text-align: right;">36.33</td>
<td style="text-align: right;">119.5</td>
</tr>
<tr class="even">
<td style="text-align: right;">Goat</td>
<td style="text-align: right;">27.66</td>
<td style="text-align: right;">115.0</td>
</tr>
<tr class="odd">
<td style="text-align: right;">Guinea pig</td>
<td style="text-align: right;">1.04</td>
<td style="text-align: right;">5.5</td>
</tr>
</tbody>
</table>
</div>
<p>Since this chapter is about association then we are interested to know if animal body and brain weights [kg] are correlated. Let’s take a sneak peak at the data points.</p>
<figure>
<img src="./images/ch07ex1v1.png" id="fig:ch07ex1v1" alt="Figure 33: Body and brain weight of 28 animal species." /><figcaption aria-hidden="true">Figure 33: Body and brain weight of 28 animal species.</figcaption>
</figure>
<p>Hmm, at first sight the data looks like a little mess. Most likely because of the large range of data on X- and Y-axis. Moreover, the fact that some animals got large body mass with relatively small brain weight doesn’t help either. Still, my impression is that in general (except for the first three points from the right) greater body weight is associated with a greater brain weight. However, it is quite hard to tell for sure as the points on the left are so close to each other on the scale of X-axis. So, let’s put that to the test.</p>
<pre class="language-julia"><code>getCorAndPval(animals.Body, animals.Brain)</code></pre>
<pre class="output"><code>(-0.0053411625612511315, 0.9784802067532017)</code></pre>
<p>The Pearson’s correlation coefficient is not able to discern the points and confirm that either. Nevertheless, let’s narrow our ranges by taking logarithms (with <code>log10</code> function) of the data and look at the scatter plot again.</p>
<figure>
<img src="./images/ch07ex1v2.png" id="fig:ch07ex1v2" alt="Figure 34: Body (log10) and brain (log10) weight of 28 animal species." /><figcaption aria-hidden="true">Figure 34: Body (log10) and brain (log10) weight of 28 animal species.</figcaption>
</figure>
<p>The impression we get is quite different than before. The points are much better separated. The three outliers remain, but they are are much closer to the imaginary trend line. Now we would like to express that relationship. One way to do it is with <a href="https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient">Spearman’s rank correlation coefficient</a>. As the name implies instead of correlating the numbers themselves it correlates their ranks.</p>
<blockquote>
<p><strong><em>Note:</em></strong> It might be a good idea to examine the three outliers and see do they have anything in common. If so, we might want to determine the relationship between X- and Y- variable separately for the outliers and the remaining animals. Here, the three outliers are dinosaurs, whereas rest of the animals are mammals. This could explain why the association is different in these two group of animals.</p>
</blockquote>
<p>So here is a warm up task for you.</p>
<p>Write a <code>getSpearmCorAndPval</code> function and run it on <code>animals</code> data frame. To do that first you will need a function <code>getRanks(v::Vector{&lt;:Real})::Vector{&lt;:Float64}</code> that returns the ranks for you like this.</p>
<pre><code>getRanks([500, 100, 1000]) # returns [2.0, 1.0, 3.0]
getRanks([500, 100, 500, 1000]) # returns [2.5, 1.0, 2.5, 4.0]
getRanks([500, 100, 500, 1000, 500]) # returns [3.0, 1.0, 3.0, 5.0, 3.0]
# etc.</code></pre>
<p>Personally, I found <code>Base.findall</code> and <code>Base.sort</code> to be useful while writing <code>getRanks</code>, but feel free to employ whatever constructs you want. Anyway, once you got it, you can apply it to get Spearman’s correlation coefficient (<code>getCorAndPval(getRanks(v1), getRanks(v2))</code>).</p>
<blockquote>
<p><strong><em>Note:</em></strong> In real life to calculate the coefficient you would probably use <a href="https://juliastats.org/StatsBase.jl/stable/ranking/#StatsBase.corspearman">StatsBase.corspearman</a>.</p>
</blockquote>
<h3 data-number="7.8.2" id="sec:assoc_pred_ex2"><span class="header-section-number">7.8.2</span> Exercise 2</h3>
<p>P-value multiplicity correction, a classic theme in this book. Let’s revisit it again. Take a look at the following data frame.</p>
<pre class="language-julia"><code>Rand.seed!(321)

letters = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;, &quot;g&quot;, &quot;h&quot;, &quot;i&quot;, &quot;j&quot;]
bogusCors = Dfs.DataFrame(
    Dict(l =&gt; Rand.rand(Dsts.Normal(100, 15), 10) for l in letters)
)
bogusCors[1:3, 1:3]
</code></pre>
<div id="tbl:boguscorsDf">
<table>
<caption>Table 16: DataFrame with random variables for bogus correlations.</caption>
<thead>
<tr class="header">
<th style="text-align: right;">a</th>
<th style="text-align: right;">b</th>
<th style="text-align: right;">c</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">102.04452249090404</td>
<td style="text-align: right;">126.62114430860125</td>
<td style="text-align: right;">72.58784224875757</td>
</tr>
<tr class="even">
<td style="text-align: right;">81.10997573989799</td>
<td style="text-align: right;">101.02869856127887</td>
<td style="text-align: right;">123.65904493232378</td>
</tr>
<tr class="odd">
<td style="text-align: right;">85.54321961150684</td>
<td style="text-align: right;">109.98477666117208</td>
<td style="text-align: right;">132.32635179854458</td>
</tr>
</tbody>
</table>
</div>
<p>It contains a random made up data. In total we can calculate <code>binomial(10, 2)</code> = 45 different unique correlations for the 10 columns we got here. Out of them roughly 2-3 (<code>binomial(10, 2) * 0.05</code> = 2.25) would appear to be valid correlations (<span class="math inline">\(p \le 0.05\)</span>), but in reality were the false positives (since we know that each column is a random variable obtained from the same distribution). So here is a task for you. Write a function that will return all the possible correlations (coefficients and p-values). Check how many of them are false positives. Apply a multiplicity correction (e.g. <code>Mt.BenjaminiHochberg()</code> we met in Section <a href="./compare_contin_data_multip_correction.html#sec:compare_contin_data_multip_correction">5.6</a>) to the p-values and check if the number of false positives drops to zero.</p>
<h3 data-number="7.8.3" id="sec:assoc_pred_ex3"><span class="header-section-number">7.8.3</span> Exercise 3</h3>
<p>Sometimes we would like to have a quick visual way to depict all the correlations in one plot to get a general impression of the correlations in the data (and possible patterns present). One way to do this is to use a so called heatmap.</p>
<p>So, here is a task for you. Read the documentation and examples for <a href="https://docs.makie.org/stable/reference/plots/heatmap/">CairoMakie’s heatmap</a> (or a heatmap from other plotting library) and for the data in <code>bogusCors</code> from the previous section create a graph similar to the one you see below.</p>
<figure>
<img src="./images/ch07ex3v1.png" id="fig:ch07ex3v1" alt="Figure 35: Correlation heatmap for data in bogusCors." /><figcaption aria-hidden="true">Figure 35: Correlation heatmap for data in <code>bogusCors</code>.</figcaption>
</figure>
<p>The graph depicts the Pearson’s correlation coefficients for all the possible correlations in <code>bogusCors</code>. Positive correlations are depicted as the shades of blue, negative correlations as the shades of red.</p>
<p>Your figure doesn’t have to be the exact replica of mine, for instance you may choose a different <a href="https://docs.makie.org/stable/explanations/colors/">color map</a>.</p>
<p>If you like challenges you may add (write it in the center of a given square) the value of the correlation coefficient (rounded to let’s say 2 decimal digits). Furthermore, you may add a significance marker (e.g. if a ‘raw’ p-value is <span class="math inline">\(\le 0.05\)</span> put ‘#’ character in a square) for the correlations.</p>
<h3 data-number="7.8.4" id="sec:assoc_pred_ex4"><span class="header-section-number">7.8.4</span> Exercise 4</h3>
<p>Linear regression just like other methods mentioned in this book got its <a href="https://en.wikipedia.org/wiki/Regression_analysis#Underlying_assumptions">assumptions</a> that if possible should be verified. The R programming language got <a href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/plot.lm">plot.lm</a> function to verify them graphically. The two most important plots (or at least the ones that I understand the best) are scatter-plot of residuals vs. fitted values and <a href="https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot">Q-Q plot</a> of standardized residuals (see Figure <a href="#fig:ch07ex4v1">36</a> below).</p>
<figure>
<img src="./images/ch07ex4v1.png" id="fig:ch07ex4v1" alt="Figure 36: Diagnostic plot for regression model (ageFatM1)." /><figcaption aria-hidden="true">Figure 36: Diagnostic plot for regression model (ageFatM1).</figcaption>
</figure>
<p>If the assumptions hold, then the points in residuals vs. fitted plot should be randomly scattered around 0 (on Y-axis) with equal spread of points from left to right and no apparent pattern visible. On the other hand, the points in Q-Q plot should lie along the Q-Q line which indicates their normal distribution. To me (I’m not an expert though) the above seem to hold in Figure <a href="#fig:ch07ex4v1">36</a> above. If that was not the case then we should try to correct our model. We might transform one or more variables (for instance by using <code>log10</code> function we met in Section <a href="./assoc_pred_exercises.html#sec:assoc_pred_ex1">7.8.1</a>) or fit a different model. Otherwise, the model we got may give poor predictions. For instance, if our residuals vs. fitted plot displayed a greater spread of points on the right side of X-axis, then most likely our predictions would be more off for large values of explanatory variable(s).</p>
<p>Anyway, your task here is to write a function <code>drawDiagPlot</code> that accepts a linear regression model and returns a graph similar to Figure <a href="#fig:ch07ex4v1">36</a> above (when called with <code>ageFatM1</code> as an input).</p>
<p>Below you will find some (but not all) of the functions that I found useful while solving this task (feel free to use whatever functions you want):</p>
<ul>
<li><code>Glm.predict</code></li>
<li><code>Glm.residuals</code></li>
<li><code>string(Glm.formula(mod))</code></li>
<li><code>Cmk.qqplot</code></li>
</ul>
<p>The rest is up to you.</p>
<h3 data-number="7.8.5" id="sec:assoc_pred_ex5"><span class="header-section-number">7.8.5</span> Exercise 5</h3>
<p>While developing the solution to exercise 4 (Section <a href="./assoc_pred_exercises_solutions.html#sec:assoc_pred_ex4_solution">7.9.4</a>) we pointed out on the flaws of <code>iceMod2</code>. We decided to develop a better model. So, here is a task for you.</p>
<p>Read about <a href="https://juliastats.org/StatsModels.jl/stable/formula/#Constructing-a-formula-programmatically-1">constructing formula programmatically</a> using <code>StatsModels</code> package (<code>GLM</code> uses it internally).</p>
<p>Next, given the <code>ice2</code> data frame below.</p>
<pre class="language-julia"><code>Rand.seed!(321)

ice = RD.dataset(&quot;Ecdat&quot;, &quot;Icecream&quot;) # reading fresh data frame
ice2 = ice[2:end, :] # copy of ice data frame
# an attempt to remove autocorrelation from Temp variable
ice2.TempDiff = ice.Temp[1:(end-1)] .- ice.Temp[2:end]

# dummy variables aimed to confuse our new function
ice2.a = Rand.rand(-100:1:100, 29)
ice2.b = Rand.rand(-100:1:100, 29)
ice2.c = Rand.rand(-100:1:100, 29)
ice2.d = Rand.rand(-100:1:100, 29)
ice2</code></pre>
<p>Write a function that return the minimal adequate model.</p>
<pre><code># return a minimal adequate (linear) model
function getMinAdeqMod(
    df::Dfs.DataFrame, y::String, xs::Vector{&lt;:String}
    )::Glm.StatsModels.TableRegressionModel</code></pre>
<p>The function accepts a data frame (<code>df</code>), name of the outcome variable (<code>y</code>), and names of the explanatory variables (<code>xs</code>). In its insides the functions builds a full additive linear model (<code>y ~ x1 + x2 + ... + etc.</code>). Then, it eliminates an <code>x</code> (predictor variable) with the greatest p-value (only if it is greater than 0.05). The removal process is continued for all <code>xs</code> until only <code>xs</code> with p-values <span class="math inline">\(\le 0.05\)</span> remain. If none of the <code>xs</code> is impactful it should return the model in the form <code>y ~ 1</code> (the intercept of this model is equal to <code>Stats.mean(y)</code>). Test it out, e.g. for <code>getMinAdeqMod(ice2, names(ice2)[1], names(ice2)[2:end])</code> it should return a model in the form <code>Cons ~ Income + Temp + TempDiff</code>.</p>
<p><em>Hint: You can extract p-values for the coefficients of the model with <code>Glm.coeftable(m).cols[4]</code>. <code>GLM</code> got its own function for constructing model terms (<code>Glm.term</code>). You can add the terms either using <code>+</code> operator or <code>sum</code> function (if you got a vector of terms).</em></p>


<div class="bottom-nav">
    <p id="nav-prev" style="text-align: left;">
        <a class="menu-level-2" href="./assoc_pred_multiple_lin_reg.html"><b>7.7</b> Multiple Linear Regressi..</a> <kbd>←</kbd>
        <span id="nav-next" style="float: right;">
            <kbd>→</kbd> <a class="menu-level-2" href="./assoc_pred_exercises_solutions.html"><b>7.9</b> Solutions - Association</a>
        </span>
    </p>
</div>


<div class="license">
    <br/>
  <br/>
  <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>
    Bartlomiej Lukaszuk
</div>
</div>
</div>
</body>
</html>