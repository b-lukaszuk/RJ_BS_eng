<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Bartlomiej Lukaszuk" />
  <title>One-way ANOVA - Romeo and Julia, where Romeo is Basic Statistics</title>
  <link rel="stylesheet" href="./style.css"/>
    <script src="./mousetrap.min.js"></script>
    <style>
  @font-face {
    font-family: JuliaMono-Regular;
    src: url("./JuliaMono-Regular.woff2");
  }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <link rel="stylesheet" href="./github.min.css">
<script src="./highlight.min.js"></script>
<script src="./julia.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', (event) => {
    document.querySelectorAll('pre').forEach((el) => {
        if (!el.classList.contains('output')) {
            hljs.highlightElement(el);
        }
    });
});
</script>
 
</head>
<body>
<script>
function click_next() {
  var next = document.getElementById('nav-next');
  next.firstElementChild.nextElementSibling.click();
}
function click_prev() {
  var prev = document.getElementById('nav-prev');
  prev.firstElementChild.click();
}
Mousetrap.bind('right', click_next);
Mousetrap.bind('h', click_prev);
Mousetrap.bind('left', click_prev);
Mousetrap.bind('l', click_next);
</script>

<div class="books-container">
<aside class="books-menu">
<input type="checkbox" id="menu">
<label for="menu">☰</label>
<div class="books-title">
<a href="./">Romeo and Julia, where Romeo is Basic Statistics</a>
</div><br />
<span class="books-subtitle">

</span>
<div class="books-menu-content">
<li><a class="menu-level-1" href="./about.html"><b>1</b> About</a></li>
<li><a class="menu-level-1" href="./why_julia.html"><b>2</b> Why Julia</a></li>
<li><a class="menu-level-2" href="./julia_is_fast.html"><b>2.1</b> Julia is fast</a></li>
<li><a class="menu-level-2" href="./julia_is_simple.html"><b>2.2</b> Julia is simple</a></li>
<li><a class="menu-level-2" href="./jl_pleasure_to_write.html"><b>2.3</b> Pleasure to write</a></li>
<li><a class="menu-level-2" href="./jl_not_mainstream.html"><b>2.4</b> Not mainstream</a></li>
<li><a class="menu-level-2" href="./jl_open_source.html"><b>2.5</b> Julia is free</a></li>
<li><a class="menu-level-1" href="./julia_first_encounter.html"><b>3</b> Julia - first encounter</a></li>
<li><a class="menu-level-2" href="./julia_installation.html"><b>3.1</b> Installation</a></li>
<li><a class="menu-level-2" href="./julia_language_constructs.html"><b>3.2</b> Language Constructs</a></li>
<li><a class="menu-level-2" href="./julia_language_variables.html"><b>3.3</b> Variables</a></li>
<li><a class="menu-level-2" href="./julia_language_functions.html"><b>3.4</b> Functions</a></li>
<li><a class="menu-level-2" href="./julia_language_decision_making.html"><b>3.5</b> Decision Making</a></li>
<li><a class="menu-level-2" href="./julia_language_repetition.html"><b>3.6</b> Repetition</a></li>
<li><a class="menu-level-2" href="./julia_language_libraries.html"><b>3.7</b> Additional libraries</a></li>
<li><a class="menu-level-2" href="./julia_language_exercises.html"><b>3.8</b> Julia - Exercises</a></li>
<li><a class="menu-level-2" href="./julia_language_exercises_solutions.html"><b>3.9</b> Julia - Solutions</a></li>
<li><a class="menu-level-1" href="./statistics_intro.html"><b>4</b> Statistics - introduction</a></li>
<li><a class="menu-level-2" href="./statistics_intro_imports.html"><b>4.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./statistics_intro_probability_definition.html"><b>4.2</b> Probability - definition</a></li>
<li><a class="menu-level-2" href="./statistics_intro_probability_properties.html"><b>4.3</b> Probability - properties</a></li>
<li><a class="menu-level-2" href="./statistics_prob_theor_practice.html"><b>4.4</b> Probability - theory and..</a></li>
<li><a class="menu-level-2" href="./statistics_prob_distribution.html"><b>4.5</b> Probability distribution</a></li>
<li><a class="menu-level-2" href="./statistics_normal_distribution.html"><b>4.6</b> Normal distribution</a></li>
<li><a class="menu-level-2" href="./statistics_intro_hypothesis_testing.html"><b>4.7</b> Hypothesis testing</a></li>
<li><a class="menu-level-2" href="./statistics_intro_exercises.html"><b>4.8</b> Statistics intro - Exerc..</a></li>
<li><a class="menu-level-2" href="./statistics_intro_exercises_solutions.html"><b>4.9</b> Statistics intro - Solut..</a></li>
<li><a class="menu-level-1" href="./compare_contin_data.html"><b>5</b> Comparisons - continuous d..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_imports.html"><b>5.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_one_samp_ttest.html"><b>5.2</b> One sample Student’s t..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_two_samp_ttest.html"><b>5.3</b> Two samples Student’s ..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_one_way_anova.html"><b>5.4</b> One-way ANOVA</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_post_hoc_tests.html"><b>5.5</b> Post-hoc tests</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_multip_correction.html"><b>5.6</b> Multiplicity correction</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_exercises.html"><b>5.7</b> Exercises - Comparisons ..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_exercises_solutions.html"><b>5.8</b> Solutions - Comparisons ..</a></li>
<li><a class="menu-level-1" href="./compare_categ_data.html"><b>6</b> Comparisons - categorical ..</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_imports.html"><b>6.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_flashback.html"><b>6.2</b> Flashback</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_chisq_test.html"><b>6.3</b> Chi squared test</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_fisher_exact_text.html"><b>6.4</b> Fisher’s exact test</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_bigger_table.html"><b>6.5</b> Bigger table</a></li>
<li><a class="menu-level-2" href="./compare_categ_test_for_independence.html"><b>6.6</b> Test for independence</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_exercises.html"><b>6.7</b> Exercises - Comparisons ..</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_exercises_solutions.html"><b>6.8</b> Solutions - Comparisons ..</a></li>
<li><a class="menu-level-1" href="./association.html"><b>7</b> Association</a></li>
<li><a class="menu-level-2" href="./assoc_and_pred_data_imports.html"><b>7.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./association_lin_relation.html"><b>7.2</b> Linear relation</a></li>
<li><a class="menu-level-2" href="./assocociation_covariance.html"><b>7.3</b> Covariance</a></li>
<li><a class="menu-level-2" href="./assoc_and_pred_correlation.html"><b>7.4</b> Correlation</a></li>
<li><a class="menu-level-2" href="./association_corr_pitfalls.html"><b>7.5</b> Correlation Pitfalls</a></li>
<li><a class="menu-level-2" href="./association_exercises.html"><b>7.6</b> Exercises - Association</a></li>
<li><a class="menu-level-2" href="./association_exercises_solutions.html"><b>7.7</b> Solutions - Association</a></li>
<li><a class="menu-level-1" href="./appendix.html"><b></b> Appendix</a></li>
<li><a class="menu-level-1" href="./references.html"><b>8</b> References</a></li>
</div>
</aside>

<div class="books-content">
<h2 data-number="5.4" id="sec:compare_contin_data_one_way_anova"><span class="header-section-number">5.4</span> One-way ANOVA</h2>
<p>One-way ANOVA is a technique to compare two or more groups of continuous data. It allows us to tell if all the groups are alike or not based on the spread of the data around the mean(s).</p>
<p>Let’s start with something familiar. Do you still remember our tennis players Peter and John from Section <a href="./statistics_intro_hypothesis_testing.html#sec:statistics_intro_tennis">4.7.1</a>. Well, guess what they work at two different biological institutes. The institutes independently test a new weight reducing drug, called drug Y, that is believed to reduce body weight of an animal by 23%. The drug administration is fairly simple. You just dilute it in water and leave it in a cage for mice to drink it.</p>
<p>So both our friends independently run the following experiment: a researcher takes eight mice, writes at random numbers at their tails (1:8), and decides that the mice 1:4 will drink pure water, and the mice 5:8 will drink the water with the drug. After a week body weights of all mice are recorded.</p>
<p>As said, Peter and John run the experiments independently not knowing one about the other. After a week Peter noticed that he messed things up and did not give the drug to mice (when diluted the drug is colorless and by accident he took the wrong bottle). It happened, still let’s compare the results that were obtained by both our friends.</p>
<pre class="language-julia"><code>import Random as Rand

# Peter&#39;s mice, experiment 1 (ex1)
Rand.seed!(321)
ex1BwtsWater = Rand.rand(Dsts.Normal(25, 3), 4)
ex1BwtsPlacebo = Rand.rand(Dsts.Normal(25, 3), 4)

# John&#39;s mice, experiment 2 (ex2)
ex2BwtsWater = Rand.rand(Dsts.Normal(25, 3), 4)
ex2BwtsDrugY = Rand.rand(Dsts.Normal(25 * 0.77, 3), 4)</code></pre>
<p>In Peter’s case both mice groups came from the same population <code>Dsts.Normal(25, 3)</code> (<span class="math inline">\(\mu = 25\)</span>, <span class="math inline">\(\sigma = 3\)</span>) since they both ate and drunk the same stuff. For need of different name the other group is named <a href="https://en.wikipedia.org/wiki/Placebo">placebo</a>.</p>
<p>In John’s case the other group comes from a different distribution (e.g. the one where body weight is reduced on average by 23%, hence <span class="math inline">\(\mu = 25 * 0.77\)</span>).</p>
<p>Let’s see the results side by side on a graph.</p>
<figure>
<img src="./images/oneWayAnovaDrugY.png" id="fig:oneWayAnovaDrugY" alt="Figure 15: The results of drug Y application on body weight of laboratory mice." /><figcaption aria-hidden="true">Figure 15: The results of drug Y application on body weight of laboratory mice.</figcaption>
</figure>
<p>I don’t know about you, but my first impression is that the data points are more scattered around in John’s experiment. Let’s add some means to the graph to make it more obvious.</p>
<figure>
<img src="./images/oneWayAnovaDrugY2.png" id="fig:oneWayAnovaDrugY2" alt="Figure 16: The results of drug Y application on body weight of laboratory mice with group and overall means." /><figcaption aria-hidden="true">Figure 16: The results of drug Y application on body weight of laboratory mice with group and overall means.</figcaption>
</figure>
<p>Indeed, with the lines (especially the overall means) the difference in spread of the data points seems to be even more evident. Notice an interesting fact, in the case of water and placebo the group means are closer to each other, and to the overall mean. It makes sense, after all the animals ate and drunk exactly the same stuff, so they belong to the same population. On the other hand in the case of the two populations (water and drugY) the group means differ from the overall mean (again, think of it for a moment and convince yourself that it makes sense). Since we got Julia on our side we could even try to express this spread of data with numbers. First, the spread of data points around the group means</p>
<pre class="language-julia"><code>function getAbsDiffs(v::Vector{&lt;:Real})::Vector{&lt;:Real}
    return abs.(Stats.mean(v) .- v)
end

function getAbsPointDiffsFromGroupMeans(
    v1::Vector{&lt;:Real}, v2::Vector{&lt;:Real})::Vector{&lt;:Real}
    return vcat(getAbsDiffs(v1), getAbsDiffs(v2))
end

ex1withinGroupsSpread = getAbsPointDiffsFromGroupMeans(
    ex1BwtsWater, ex1BwtsPlacebo)
ex2withinGroupsSpread = getAbsPointDiffsFromGroupMeans(
    ex2BwtsWater, ex2BwtsDrugY)

ex1AvgWithinGroupsSpread = Stats.mean(ex1withinGroupsSpread)
ex2AvgWithingGroupsSpread = Stats.mean(ex2withinGroupsSpread)

(ex1AvgWithinGroupsSpread, ex2AvgWithingGroupsSpread)</code></pre>
<pre class="output"><code>(1.941755009754579, 2.87288915817597)</code></pre>
<p>The code is pretty simple. Here we calculate the distance of data points around the group means. Since we are not interested in a sign of a difference [<code>+</code> (above), <code>-</code> (below) the mean] we use <code>abs</code> function. We used a similar methodology when we calculated <code>absDiffsStudA</code> and <code>absDiffsStudB</code> in Section <a href="./statistics_normal_distribution.html#sec:statistics_normal_distribution">4.6</a>. This is as if we measured the distances from the group means in Figure <a href="#fig:oneWayAnovaDrugY2">16</a> with a ruler and took the average of them. The only new part is the <a href="https://docs.julialang.org/en/v1/base/arrays/#Base.vcat">vcat</a> function. All it does is it glues two vectors together, like: <code>vcat([1, 2], [3, 4])</code> gives us <code>[1, 2, 3, 4]</code>. Anyway, na average distance of a point from a group mean is 1.9 [g] for experiment 1 (left panel in Figure <a href="#fig:oneWayAnovaDrugY2">16</a>). For experiment 2 (right panel in Figure <a href="#fig:oneWayAnovaDrugY2">16</a>) it is equal to 2.9 [g]. That is nice, as it follows our expectations. However, <code>AvgWithinGroupsSpread</code> by itself is not enough since sooner or later in <code>experiment 1</code> (hence prefix <code>ex1-</code>) we may encounter (a) population(s) with a wide natural spread of the data. Therefore, we need a more robust metric.</p>
<p>This is were the average spread of group means around the overall mean could be useful. Let’s get to it, we will start with these functions</p>
<pre class="language-julia"><code>function repVectElts(v::Vector{T}, times::Vector{Int})::Vector{T} where {T}
    @assert (length(v) == length(times)) &quot;length(v) not equal length(times)&quot;
    @assert all(map(x -&gt; x &gt; 0, times)) &quot;times elts must be positive&quot;
    result::Vector{T} = Vector{eltype(v)}(undef, sum(times))
    currInd::Int = 1
    for i in eachindex(v)
        for _ in 1:times[i]
            result[currInd] = v[i]
            currInd += 1
        end
    end
    return result
end

function getAbsGroupDiffsFromOverallMean(
    v1::Vector{&lt;:Real}, v2::Vector{&lt;:Real})::Vector{&lt;:Real}
    overallMean::Float64 = Stats.mean(vcat(v1, v2))
    groupMeans::Vector{Float64} = [Stats.mean(v1), Stats.mean(v2)]
    absGroupDiffs::Vector{&lt;:Real} = abs.(overallMean .- groupMeans)
    absGroupDiffs = repVectElts(absGroupDiffs, map(length, [v1, v2]))
    return absGroupDiffs
end</code></pre>
<p>The function <code>repVectElts</code> is a helper function. It is slightly complicated and I will not explain it in detail. Just treat it as any other function from a library. A function you know only by name, input, and output. A function that you are not aware of its insides (of course if you really want you can figure them out by yourself). All it does is it takes two vectors <code>v</code> and <code>times</code>, then it replicates each element of <code>v</code> a number of times specified in <code>times</code> like so: <code>repVectElts([10, 20], [1, 2])</code> <code>output [10, 20, 20]</code>. And this is actually all you care about right now.</p>
<p>As for the <code>getAbsGroupDiffsFromOverallMean</code> it does exactly what it says. It subtracts group means from the overall mean <code>(overallMean .- groupMeans)</code> and takes absolute values of that [<code>abs.(</code>]. Then it repeats each difference as many times as there are observations in the group <code>repVectElts(absGroupDiffs, map(length, [v1, v2]))</code> (as if every single point in a group was that far away from the overall mean). This is what it returns to us.</p>
<p>OK, time to use the last function, behold</p>
<pre class="language-julia"><code>ex1groupSpreadFromOverallMean = getAbsGroupDiffsFromOverallMean(
    ex1BwtsWater, ex1BwtsPlacebo)
ex2groupSpreadFromOverallMean = getAbsGroupDiffsFromOverallMean(
    ex2BwtsWater, ex2BwtsDrugY)

ex1AvgGroupSpreadFromOverallMean = Stats.mean(ex1groupSpreadFromOverallMean)
ex2AvgGroupSpreadFromOverallMean = Stats.mean(ex2groupSpreadFromOverallMean)

(ex1AvgGroupSpreadFromOverallMean, ex2AvgGroupSpreadFromOverallMean)</code></pre>
<pre class="output"><code>(0.597596847858199, 3.6750594521844278)</code></pre>
<p>OK, we got it. The average group mean spread around the overall mean is 0.6 [g] for experiment 1 (left panel in Figure <a href="#fig:oneWayAnovaDrugY2">16</a>) and 3.7 [g] for experiment 2 (right panel in Figure <a href="#fig:oneWayAnovaDrugY2">16</a>). Again, the values are as we expected them to be based on our intuition.</p>
<p>Now, we can use the obtained before <code>AvgWithinGroupSpread</code> as a reference point for <code>AvgGroupSpreadFromOverallMean</code> like so</p>
<pre class="language-julia"><code>LStatisticEx1 = ex1AvgGroupSpreadFromOverallMean / ex1AvgWithinGroupsSpread
LStatisticEx2 = ex2AvgGroupSpreadFromOverallMean / ex2AvgWithingGroupsSpread

(LStatisticEx1, LStatisticEx2)</code></pre>
<pre class="output"><code>(0.3077611979143188, 1.2792207599536367)</code></pre>
<p>Here, we calculated a so called <code>LStatistic</code>. I made the name up, because that is the first name that came to my mind. Perhaps it is because my family name is Lukaszuk or maybe because I’m selfish. Anyway, the higher the L-statistic (so the ratio of group spread around the overall mean to within group spread) the smaller the probability that such a big difference was caused by a chance alone (hmm, I think I said something along those lines in one of the previous chapters). If only we could reliably determine the cutoff point for my <code>LStatistic</code>.</p>
<p>Luckily, there is no point for us to do that since one-way ANOVA relies on a similar metric called F-statistic (BTW. Did I mention that the ANOVA was developed by <a href="https://en.wikipedia.org/wiki/Ronald_Fisher">Ronald Fisher</a>). Observe. First, experiment 1:</p>
<pre class="language-julia"><code>Htests.OneWayANOVATest(ex1BwtsWater, ex1BwtsPlacebo)</code></pre>
<pre class="output"><code>One-way analysis of variance (ANOVA) test
-----------------------------------------
Population details:
    parameter of interest:   Means
    value under h_0:         &quot;all equal&quot;
    point estimate:          NaN

Test summary:
    outcome with 95% confidence: fail to reject h_0
    p-value:                     0.5738

Details:
    number of observations: [4, 4]
    F statistic:            0.353601
    degrees of freedom:     (1, 6)
</code></pre>
<p>Here, my made up <code>LStatistic</code> was 0.31 whereas the F-Statistic is 0.35, so kind of close. Chances are they measure the same thing but using slightly different methodology. Here, the p-value (p &gt; 0.05) demonstrates that the groups come from the same population.</p>
<p>OK, now time for experiment 2:</p>
<pre class="language-julia"><code>Htests.OneWayANOVATest(ex2BwtsWater, ex2BwtsDrugY)</code></pre>
<pre class="output"><code>One-way analysis of variance (ANOVA) test
-----------------------------------------
Population details:
    parameter of interest:   Means
    value under h_0:         &quot;all equal&quot;
    point estimate:          NaN

Test summary:
    outcome with 95% confidence: reject h_0
    p-value:                     0.0428

Details:
    number of observations: [4, 4]
    F statistic:            6.56001
    degrees of freedom:     (1, 6)
</code></pre>
<p>Here, the p-value (p &lt; 0.05) demonstrates that the groups come from different populations (the means of those populations differ). As a reminder, in this case my made up <code>LStatistic</code> was 1.28 whereas the F-Statistic is 6.56, so this time it is more distant.</p>
<p>The differences stem from different methodology. For instance, just like in Section <a href="./statistics_normal_distribution.html#sec:statistics_normal_distribution">4.6</a> here we used <code>abs</code> function as our power horse. But do you remember, that statisticians love to get rid of the sign from a number by squaring it. That is why you cannot always expect similar numbers, after all <code>abs(1)</code> = 1, <code>abs(-2)</code> = 2, but <span class="math inline">\(1^2 = 1, (-2)^2 = 4\)</span>. Anyway, let’s rewrite our functions in a more statistical manner</p>
<pre class="language-julia"><code># compare with our getAbsDiffs
function getSquaredDiffs(v::Vector{&lt;:Real})::Vector{&lt;:Real}
    return (Stats.mean(v) .- v) .^ 2
end

# compare with our getAbsPointDiffsFromOverallMean
function getResidualSquaredDiffs(
    v1::Vector{&lt;:Real}, v2::Vector{&lt;:Real})::Vector{&lt;:Real}
    return vcat(getSquaredDiffs(v1), getSquaredDiffs(v2))
end

# compare with our getAbsGroupDiffsAroundOverallMean
function getGroupSquaredDiffs(
    v1::Vector{&lt;:Real}, v2::Vector{&lt;:Real})::Vector{&lt;:Real}
    overallMean::Float64 = Stats.mean(vcat(v1, v2))
    groupMeans::Vector{Float64} = [Stats.mean(v1), Stats.mean(v2)]
    groupSqDiffs::Vector{&lt;:Real} = (overallMean .- groupMeans) .^ 2
    groupSqDiffs = repVectElts(groupSqDiffs, map(length, [v1, v2]))
    return groupSqDiffs
end</code></pre>
<p>The functions are very similar to the ones we developed earlier. Of course, instead of <code>abs.(</code> we used <code>.^2</code> to get rid of the sign. Here, I tried to adopt the names (<code>group sum of squares</code> and <code>residual sum of squares</code>) that you may find in a statistical textbook/software.</p>
<p>Now we can finally calculate averages of those squares and the F-statistics itself with the following functions</p>
<pre class="language-julia"><code>function getResidualMeanSquare(
    v1::Vector{&lt;:Real}, v2::Vector{&lt;:Real})::Float64
    residualSquaredDiffs::Vector{&lt;:Real} = getResidualSquaredDiffs(v1, v2)
    return sum(residualSquaredDiffs) / getDf(v1, v2)
end

function getGroupMeanSquare(
    v1::Vector{&lt;:Real}, v2::Vector{&lt;:Real})::Float64
    groupSquaredDiffs::Vector{&lt;:Real} = getGroupSquaredDiffs(v1, v2)
    groupMeans::Vector{Float64} = [Stats.mean(v1), Stats.mean(v2)]
    return sum(groupSquaredDiffs) / getDf(groupMeans)
end

function getFStatistic(v1::Vector{&lt;:Real}, v2::Vector{&lt;:Real})::Float64
    return getGroupMeanSquare(v1, v2) / getResidualMeanSquare(v1, v2)
end</code></pre>
<p>Again, here I tried to adopt the names (<code>group mean square</code> and <code>residual mean square</code>) that you may find in a statistical textbook/software. Anyway, notice that in order to calculate <code>MeanSquare</code>s we divided our sum of squares by the degrees of freedom (we met this concept and developed the functions for its calculation in Section <a href="./compare_contin_data_one_samp_ttest.html#sec:compare_contin_data_one_samp_ttest">5.2</a> and in Section <a href="./compare_contin_data_two_samp_ttest.html#sec:compare_contin_data_unpaired_ttest">5.3.2</a>). Using degrees of freedom (instead of <code>length(vector)</code> like in the arithmetic mean) is usually said to provide better estimates of the wanted values when the sample size(s) is/are small.</p>
<p>OK, time to verify our functions for the F-statistic calculation.</p>
<pre class="language-julia"><code>(
getFStatistic(ex1BwtsWater, ex1BwtsPlacebo),
getFStatistic(ex2BwtsWater, ex2BwtsDrugY),
)</code></pre>
<pre class="output"><code>(0.3536010850042917, 6.560010563323356)</code></pre>
<p>To me, they look similar to the ones produced by <code>Htests.OneWayANOVATest</code> before, but go ahead scroll up and check it yourself. Anyway, under <span class="math inline">\(H_{0}\)</span> (all groups come from the same population) the F-statistic (so <span class="math inline">\(\frac{groupMeanSq}{residMeanSq}\)</span>) got the <a href="https://en.wikipedia.org/wiki/F-distribution">F-Distribution</a> (a probability distribution), hence we can calculate the probability of obtaining such a value (or greater) by chance and get our p-value (similarily as we did in Section <a href="./statistics_normal_distribution.html#sec:statistics_intro_distributions_package">4.6.2</a> or in Section <a href="./compare_contin_data_one_samp_ttest.html#sec:compare_contin_data_one_samp_ttest">5.2</a>). Based on that we can deduce whether samples come from the same population (p &gt; 0.05) or from different populations (<span class="math inline">\(p \le 0.05\)</span>). Ergo, we get to know if any group (means) differ(s) from the other(s).</p>


<div class="bottom-nav">
    <p id="nav-prev" style="text-align: left;">
        <a class="menu-level-2" href="./compare_contin_data_two_samp_ttest.html"><b>5.3</b> Two samples Student’s ..</a> <kbd>←</kbd>
        <span id="nav-next" style="float: right;">
            <kbd>→</kbd> <a class="menu-level-2" href="./compare_contin_data_post_hoc_tests.html"><b>5.5</b> Post-hoc tests</a>
        </span>
    </p>
</div>


<div class="license">
    <br/>
  <br/>
  <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>
    Bartlomiej Lukaszuk
</div>
</div>
</div>
</body>
</html>