<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Bartlomiej Lukaszuk" />
  <title>Association - Romeo and Julia, where Romeo is Basic Statistics</title>
  <link rel="stylesheet" href="./style.css"/>
    <script src="./mousetrap.min.js"></script>
    <style>
  @font-face {
    font-family: JuliaMono-Regular;
    src: url("./JuliaMono-Regular.woff2");
  }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <link rel="stylesheet" href="./github.min.css">
<script src="./highlight.min.js"></script>
<script src="./julia.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', (event) => {
    document.querySelectorAll('pre').forEach((el) => {
        if (!el.classList.contains('output')) {
            hljs.highlightElement(el);
        }
    });
});
</script>
 
</head>
<body>
<script>
function click_next() {
  var next = document.getElementById('nav-next');
  next.firstElementChild.nextElementSibling.click();
}
function click_prev() {
  var prev = document.getElementById('nav-prev');
  prev.firstElementChild.click();
}
Mousetrap.bind('right', click_next);
Mousetrap.bind('h', click_prev);
Mousetrap.bind('left', click_prev);
Mousetrap.bind('l', click_next);
</script>

<div class="books-container">
<aside class="books-menu">
<input type="checkbox" id="menu">
<label for="menu">☰</label>
<div class="books-title">
<a href="./">Romeo and Julia, where Romeo is Basic Statistics</a>
</div><br />
<span class="books-subtitle">

</span>
<div class="books-menu-content">
<li><a class="menu-level-1" href="./about.html"><b>1</b> About</a></li>
<li><a class="menu-level-1" href="./why_julia.html"><b>2</b> Why Julia</a></li>
<li><a class="menu-level-2" href="./julia_is_fast.html"><b>2.1</b> Julia is fast</a></li>
<li><a class="menu-level-2" href="./julia_is_simple.html"><b>2.2</b> Julia is simple</a></li>
<li><a class="menu-level-2" href="./jl_pleasure_to_write.html"><b>2.3</b> Pleasure to write</a></li>
<li><a class="menu-level-2" href="./jl_not_mainstream.html"><b>2.4</b> Not mainstream</a></li>
<li><a class="menu-level-2" href="./jl_open_source.html"><b>2.5</b> Julia is free</a></li>
<li><a class="menu-level-1" href="./julia_first_encounter.html"><b>3</b> Julia - first encounter</a></li>
<li><a class="menu-level-2" href="./julia_installation.html"><b>3.1</b> Installation</a></li>
<li><a class="menu-level-2" href="./julia_language_constructs.html"><b>3.2</b> Language Constructs</a></li>
<li><a class="menu-level-2" href="./julia_language_variables.html"><b>3.3</b> Variables</a></li>
<li><a class="menu-level-2" href="./julia_language_functions.html"><b>3.4</b> Functions</a></li>
<li><a class="menu-level-2" href="./julia_language_decision_making.html"><b>3.5</b> Decision Making</a></li>
<li><a class="menu-level-2" href="./julia_language_repetition.html"><b>3.6</b> Repetition</a></li>
<li><a class="menu-level-2" href="./julia_language_libraries.html"><b>3.7</b> Additional libraries</a></li>
<li><a class="menu-level-2" href="./julia_language_exercises.html"><b>3.8</b> Julia - Exercises</a></li>
<li><a class="menu-level-2" href="./julia_language_exercises_solutions.html"><b>3.9</b> Julia - Solutions</a></li>
<li><a class="menu-level-1" href="./statistics_intro.html"><b>4</b> Statistics - introduction</a></li>
<li><a class="menu-level-2" href="./statistics_intro_imports.html"><b>4.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./statistics_intro_probability_definition.html"><b>4.2</b> Probability - definition</a></li>
<li><a class="menu-level-2" href="./statistics_intro_probability_properties.html"><b>4.3</b> Probability - properties</a></li>
<li><a class="menu-level-2" href="./statistics_prob_theor_practice.html"><b>4.4</b> Probability - theory and..</a></li>
<li><a class="menu-level-2" href="./statistics_prob_distribution.html"><b>4.5</b> Probability distribution</a></li>
<li><a class="menu-level-2" href="./statistics_normal_distribution.html"><b>4.6</b> Normal distribution</a></li>
<li><a class="menu-level-2" href="./statistics_intro_hypothesis_testing.html"><b>4.7</b> Hypothesis testing</a></li>
<li><a class="menu-level-2" href="./statistics_intro_exercises.html"><b>4.8</b> Statistics intro - Exerc..</a></li>
<li><a class="menu-level-2" href="./statistics_intro_exercises_solutions.html"><b>4.9</b> Statistics intro - Solut..</a></li>
<li><a class="menu-level-1" href="./compare_contin_data.html"><b>5</b> Comparisons - continuous d..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_imports.html"><b>5.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_one_samp_ttest.html"><b>5.2</b> One sample Student’s t..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_two_samp_ttest.html"><b>5.3</b> Two samples Student’s ..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_one_way_anova.html"><b>5.4</b> One-way ANOVA</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_post_hoc_tests.html"><b>5.5</b> Post-hoc tests</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_multip_correction.html"><b>5.6</b> Multiplicity correction</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_exercises.html"><b>5.7</b> Exercises - Comparisons ..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_exercises_solutions.html"><b>5.8</b> Solutions - Comparisons ..</a></li>
<li><a class="menu-level-1" href="./compare_categ_data.html"><b>6</b> Comparisons - categorical ..</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_imports.html"><b>6.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_flashback.html"><b>6.2</b> Flashback</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_chisq_test.html"><b>6.3</b> Chi squared test</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_fisher_exact_text.html"><b>6.4</b> Fisher’s exact test</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_bigger_table.html"><b>6.5</b> Bigger table</a></li>
<li><a class="menu-level-2" href="./compare_categ_test_for_independence.html"><b>6.6</b> Test for independence</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_exercises.html"><b>6.7</b> Exercises - Comparisons ..</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_exercises_solutions.html"><b>6.8</b> Solutions - Comparisons ..</a></li>
<li><a class="menu-level-1" href="./assoc_and_pred.html"><b>7</b> Association and Prediction</a></li>
<li><a class="menu-level-2" href="./assoc_and_pred_data_imports.html"><b>7.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./assoc_and_pred_association.html"><b>7.2</b> Association</a></li>
<li><a class="menu-level-1" href="./appendix.html"><b></b> Appendix</a></li>
<li><a class="menu-level-1" href="./references.html"><b>8</b> References</a></li>
</div>
</aside>

<div class="books-content">
<h2 data-number="7.2" id="sec:assoc_and_pred_association"><span class="header-section-number">7.2</span> Association</h2>
<p>Imagine you are a biologist that conducts their research in <a href="https://en.wikipedia.org/wiki/Amazon_rainforest">the Amazon rainforest</a> known for biodiversity and heavy rainfalls (see the name). You divided the area into 20 equal size fields on which you measured the volume of rain and biomass of two plants (named creatively <code>plantA</code> and <code>plantB</code>). The results are contained in <code>biomass.csv</code> file, let’s take a sneak peak at them.</p>
<pre class="language-julia"><code>import CSV as Csv
import DataFrames as Dfs

# if you are in &#39;code_snippets&#39; folder, then use: &quot;./ch07/biomass.csv&quot;
# if you are in &#39;ch07&#39; folder, then use: &quot;./biomass.csv&quot;
biomass = Csv.read(&quot;./code_snippets/ch07/biomass.csv&quot;, Dfs.DataFrame)
first(biomass, 5)
</code></pre>
<div id="tbl:biomassDf">
<table>
<caption>Table 9: Effect of rainfall on plants biomass (fictitious data).</caption>
<thead>
<tr class="header">
<th style="text-align: right;">plantAkg</th>
<th style="text-align: right;">rainL</th>
<th style="text-align: right;">plantBkg</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">20.26</td>
<td style="text-align: right;">15.09</td>
<td style="text-align: right;">21.76</td>
</tr>
<tr class="even">
<td style="text-align: right;">9.18</td>
<td style="text-align: right;">5.32</td>
<td style="text-align: right;">6.08</td>
</tr>
<tr class="odd">
<td style="text-align: right;">11.36</td>
<td style="text-align: right;">12.5</td>
<td style="text-align: right;">10.96</td>
</tr>
<tr class="even">
<td style="text-align: right;">11.26</td>
<td style="text-align: right;">10.7</td>
<td style="text-align: right;">4.96</td>
</tr>
<tr class="odd">
<td style="text-align: right;">9.05</td>
<td style="text-align: right;">5.7</td>
<td style="text-align: right;">9.55</td>
</tr>
</tbody>
</table>
</div>
<p>I think some plot would be helpful to get a better picture of the data (pun intended).</p>
<pre class="language-julia"><code>import CairoMakie as Cmk

fig = Cmk.Figure()
ax1, sc1 = Cmk.scatter(fig[1, 1], biomass.rainL, biomass.plantAkg,
    markersize=25, color=&quot;skyblue&quot;, strokewidth=1, strokecolor=&quot;gray&quot;,
    axis=(; title=&quot;Effect of rainfall on biomass of plant A&quot;,
        xlabel=&quot;water [L]&quot;, ylabel=&quot;biomass [kg]&quot;)
)
ax2, sc2 = Cmk.scatter(fig[1, 2], biomass.rainL, biomass.plantBkg,
    markersize=25, color=&quot;linen&quot;, strokewidth=1, strokecolor=&quot;black&quot;,
    axis=(; title=&quot;Effect of rainfall on bomass of plant B&quot;,
        xlabel=&quot;water [L]&quot;, ylabel=&quot;biomass [kg]&quot;)
)
Cmk.linkxaxes!(ax1, ax2)
Cmk.linkyaxes!(ax1, ax2)
fig</code></pre>
<figure>
<img src="./images/ch07biomassCor.png" id="fig:ch07biomassCor" alt="Figure 27: Effect of rainfall on plants’ biomass." /><figcaption aria-hidden="true">Figure 27: Effect of rainfall on plants’ biomass.</figcaption>
</figure>
<p>Overall, it looks like the biomass of both plants is directly related (one increases and the other increases) with the volume of rain. That seems reasonable. Moreover, we can see that the points are spread along an imaginary line (go ahead imagine it) that goes through all the points on a graph. We can also see that <code>plantB</code> has a somewhat greater spread of points. It would be nice to be able to express such a relation between two variables (here biomass and volume of rain) with a single number. It turns out that we can. That’s the job for <a href="https://en.wikipedia.org/wiki/Covariance">covariance</a>.</p>
<h3 data-number="7.2.1" id="sec:assoc_and_pred_covariance"><span class="header-section-number">7.2.1</span> Covariance</h3>
<p>The formula for covariance resembles the one for <code>variance</code> that we met in Section <a href="./statistics_normal_distribution.html#sec:statistics_normal_distribution">4.6</a> (<code>getVar</code> function) only that it is calculated for pairs of values (here a plant biomass and rainfall for a field), so two vectors instead of one. Observe</p>
<pre class="language-julia"><code>function getCov(v1::Vector{&lt;:Real}, v2::Vector{&lt;:Real})::Float64
    @assert length(v1) == length(v2) &quot;v1 and v2 must be of equal lengths&quot;
    avg1::Float64 = Stats.mean(v1)
    avg2::Float64 = Stats.mean(v2)
    diffs1::Vector{&lt;:Real} = v1 .- avg1
    diffs2::Vector{&lt;:Real} = v2 .- avg2
    return sum(diffs1 .* diffs2) / (length(v1) - 1)
end</code></pre>
<blockquote>
<p><strong><em>Note:</em></strong> To calculate the covariance you may also use <a href="https://docs.julialang.org/en/v1/stdlib/Statistics/#Statistics.cov">Statistics.cov</a>.</p>
</blockquote>
<p>A few points of notice. In Section <a href="./statistics_normal_distribution.html#sec:statistics_normal_distribution">4.6</a> in <code>getVar</code> we squared the differences (<code>diffs</code>), i.e. we multiplied the diffs by themselves (<span class="math inline">\(x * x = x^2\)</span>). Here, we do something similar by multiplying parallel values from both vectors of <code>diffs</code> (<code>diffs1</code> and <code>diffs2</code>) by each other (<span class="math inline">\(x * y\)</span>, for a given field). Moreover, instead of taking the average (so <code>sum(diffs1 .* diffs2)/length(v1)</code>) here we use the more fine tuned statistical formula that relies on degrees of freedom we met in Section <a href="./compare_contin_data_one_samp_ttest.html#sec:compare_contin_data_one_samp_ttest">5.2</a> (there we used <code>getDf</code> function, here we kind of use <code>getDf</code> on the number of fields that are represented by the points in Figure <a href="#fig:ch07biomassCor">27</a>).</p>
<p>Enough explanations, let’s see how it works. First, a few possible associations that roughly take the following shapes on a graph: <code>/</code>, <code>\</code>, <code>|</code>, and <code>-</code>.</p>
<pre class="language-julia"><code>rowLenBiomass, _ = size(biomass)

(
    # assuming getCov(xs, ys)
    getCov(biomass.rainL, biomass.plantAkg), # /
    getCov(collect(1:1:rowLenBiomass), collect(rowLenBiomass:-1:1)), # \
    getCov(repeat([5], rowLenBiomass), biomass.plantAkg), # |
    getCov(biomass.rainL, repeat([5], rowLenBiomass)) # -
)</code></pre>
<pre class="output"><code>(8.721824210526316, -35.0, 0.0, 0.0)</code></pre>
<p>We can see that whenever both variables (on X- and on Y-axis) increase simultaneously (points lie alongside <code>/</code> imaginary line like in Figure <a href="#fig:ch07biomassCor">27</a>) then the covariance is positive. If one variable increases whereas the other decreases (points lie alongside <code>\</code> imaginary line) then the covariance is negative. Whereas in the case when one variable changes and the other is stable (points lie alongside <code>|</code> or <code>-</code> line) the covariance is equal zero.</p>
<p>OK, time to compare the both plants.</p>
<pre class="language-julia"><code>covPlantA = getCov(biomass.plantAkg, biomass.rainL)
covPlantB = getCov(biomass.plantBkg, biomass.rainL)

(
    covPlantA,
    covPlantB,
)</code></pre>
<pre class="output"><code>(8.721824210526316, 9.527113684210526)</code></pre>
<p>In Section <a href="./statistics_normal_distribution.html#sec:statistics_normal_distribution">4.6</a> greater <code>variance</code> (and <code>standard deviation</code>) meant greater spread of points around the mean, here the greater covariance expresses the greater spread of the points around the imaginary trend line (in Figure <a href="#fig:ch07biomassCor">27</a>). But beware, you shouldn’t judge the spread of data based on the covariance alone. To understand why let’s look at the graph below.</p>
<figure>
<img src="./images/ch07biomassCorDiffUnits.png" id="fig:ch07biomassCorDiffUnits" alt="Figure 28: Effect of rainfall on plants’ biomass." /><figcaption aria-hidden="true">Figure 28: Effect of rainfall on plants’ biomass.</figcaption>
</figure>
<p>Here, we got the biomass of <code>plantA</code> in different units (kilograms and pounds). Logic and visual inspection of the points spread on the graph suggest that the covariances should be the same. Or maybe not?</p>
<pre class="language-julia"><code>(
    getCov(biomass.plantAkg, biomass.rainL),
    getCov(biomass.plantAkg .* 2.205, biomass.rainL),
)</code></pre>
<pre class="output"><code>(8.721824210526316, 19.231622384210525)</code></pre>
<p>The covariances suggest that the spread of the data points is like 2 times greater between the two sub-graphs of Figure <a href="#fig:ch07biomassCorDiffUnits">28</a>, but that is clearly not the case. The problem is that the covariance is easily inflated by the units of measurements. That is why we got an improved metrics for association named <a href="https://en.wikipedia.org/wiki/Correlation">correlation</a>.</p>
<h3 data-number="7.2.2" id="sec:assoc_and_pred_correlation"><span class="header-section-number">7.2.2</span> Correlation</h3>
<p>Correlation is most frequently expressed in the term of <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Pearson correlation coefficient</a> that by itself relies on covariance we met in the previous section. Its formula is pretty straightforward</p>
<pre class="language-julia"><code># calculates the Pearson correlation coefficient
function getCor(v1::Vector{&lt;:Real}, v2::Vector{&lt;:Real})::Float64
    return getCov(v1, v2) / (Stats.std(v1) * Stats.std(v2))
end</code></pre>
<pre class="output"><code>getCor (generic function with 1 method)</code></pre>
<blockquote>
<p><strong><em>Note:</em></strong> To calculate the Pearson correlation coefficient you may also use <a href="https://docs.julialang.org/en/v1/stdlib/Statistics/#Statistics.cor">Statistics.cor</a>.</p>
</blockquote>
<p>The correlation coefficient is just the covariance (numerator) divided by the product of two standard deviations (denominator). The lowest absolute value (<code>abs(getCov(v1, v2))</code>) possible for covariance is 0. The maximum absolute value possible for covariance is <code>Stats.std(v1) * Stats.std(v2)</code>. Therefore, the correlation coefficient (often abbreviated as <code>r</code>) takes values from 0 to 1 for positive covariance and from 0 to -1 for negative covariance.</p>
<p>Let’s see how it works.</p>
<pre class="language-julia"><code>biomassCors = (
    getCor(biomass.plantAkg, biomass.rainL),
    getCor(biomass.plantAkg .* 2.205, biomass.rainL), # pounds
    getCor(biomass.plantBkg, biomass.rainL),
    getCor(biomass.plantBkg .* 2.205, biomass.rainL), # pounds
)
round.(biomassCors, digits = 2)</code></pre>
<pre class="output"><code>(0.78, 0.78, 0.53, 0.53)</code></pre>
<p>Clearly, the new and improved coefficient is more useful than the old one (covariance) and now we can be fairly sure of the greater strength of association between <code>plantA</code> and rainfall than <code>plantB</code> and the condition.</p>
<p>The interpretation of the correlation coefficient differs depending on a textbook and field of science, but for biology it is approximated by those cutoffs:</p>
<ul>
<li><code>abs(r)</code> = [0 - 0.2) - very weak correlation</li>
<li><code>abs(r)</code> = [0.2 - 0.4) - weak correlation</li>
<li><code>abs(r)</code> = [0.4 - 0.6) - moderate correlation</li>
<li><code>abs(r)</code> = [0.6 - 0.8) - strong correlation</li>
<li><code>abs(r)</code> = [0.8 - 1] - very strong correlation</li>
</ul>
<blockquote>
<p><strong><em>Note:</em></strong> <code>]</code> and <code>)</code> signify closed and open interval, respectively. So, x in range <code>[0, 1]</code> means 0 &lt;= x &lt;= 1, whereas x in range <code>[0, 1)</code> means 0 &lt;= x &lt; 1.</p>
</blockquote>
<p>In general, if <code>x</code> and <code>y</code> are correlated then this may mean one of a few things, the most obvious of which are:</p>
<ul>
<li><code>x</code> is a cause, <code>y</code> is an effect</li>
<li><code>y</code> is a cause, <code>x</code> is an effect</li>
<li>changes in <code>x</code> and <code>y</code> are caused by an unknown third factor(s)</li>
<li><code>x</code> and <code>y</code> are not related but it just happened that in the sample they appear to be related by chance alone (in a small sample drawn from a population they appear to be associated, but in the population they are not).</li>
</ul>
<p>We can protect ourselves against the last contingency (to a certain extent) with our good old Student’s T-test (see Section <a href="./compare_contin_data_one_samp_ttest.html#sec:compare_contin_data_one_samp_ttest">5.2</a>). As stated in <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#Testing_using_Student&#39;s_t-distribution">the wikipedia’s page</a>:</p>
<blockquote>
<p>[…] Pearson’s correlation coefficient follows Student’s t-distribution with degrees of freedom n − 2. Specifically, if the underlying variables have a bivariate normal distribution the variable</p>
<p><span class="math inline">\(t = \frac{r}{\sigma_r} = r * \sqrt{\frac{n-2}{1-r^2}}\)</span></p>
<p>has a student’s t-distribution in the null case (zero correlation)</p>
</blockquote>
<p>Let’s put that knowledge to good use:</p>
<pre class="language-julia"><code># calculates the Pearson correlation coefficient and pvalue
# assumption (not tested in the function): v1 &amp; v2 got normal distribution
function getCorAndPval(
    v1::Vector{&lt;:Real}, v2::Vector{&lt;:Real})::Tuple{Float64, Float64}
    r::Float64 = getCov(v1, v2) / (Stats.std(v1) * Stats.std(v2))
    n::Int = length(v1) # num of points
    df::Int = n - 2
    t::Float64 = r * sqrt(df / (1 - r^2)) # t-statistics
    leftTail::Float64 = Dsts.cdf(Dsts.TDist(df), t)
    pval::Float64 = (t &gt; 0) ? (1 - leftTail) : leftTail
    return (r, pval * 2) # (* 2) two-tailed probability
end</code></pre>
<pre class="output"><code>getCorAndPval (generic function with 1 method)</code></pre>
<p>The function is just a translation of the formula given above + some calculations similar to those we did in Section <a href="./compare_contin_data_one_samp_ttest.html#sec:compare_contin_data_one_samp_ttest">5.2</a> to get the p-value. And now for our correlations.</p>
<pre class="language-julia"><code>biomassCorsPvals = (
    getCorAndPval(biomass.plantAkg, biomass.rainL),
    getCorAndPval(biomass.plantAkg .* 2.205, biomass.rainL), # pounds
    getCorAndPval(biomass.plantBkg, biomass.rainL),
    getCorAndPval(biomass.plantBkg .* 2.205, biomass.rainL), # pounds
)
biomassCorsPvals</code></pre>
<pre class="output"><code>((0.7820227869193522, 4.635013786202791e-5),
 (0.7820227869193522, 4.635013786202791e-5),
 (0.5265458470350619, 0.017073389709765907),
 (0.5265458470350618, 0.017073389709765907))</code></pre>
<p>We can see that both correlation coefficients are unlikely to have occurred by chance alone (<span class="math inline">\(p \le 0.05\)</span>). Therefore, we can conclude that in each case the biomass is associated with the amount of water a plant receives. I don’t know a formal test to compare two correlation coefficients, but based on the <code>r</code>s alone it appears that the biomass of <code>plantA</code> is more tightly related to (or maybe even it relies more on) the amount of water than the other plant (<code>plantB</code>).</p>
<h3 data-number="7.2.3" id="sec:assoc_and_pred_association_cor_pitfalls"><span class="header-section-number">7.2.3</span> Correlation Pitfalls</h3>
<p>The Pearson correlation coefficient is pretty useful (especially in connection with the Student’s t-test), but it shouldn’t be applied thoughtlessly.</p>
<p>Let’s take a look at the <a href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet">Anscombe’s quartet</a>.</p>
<pre class="language-julia"><code>import RDatasets as RD

anscombe = RD.dataset(&quot;datasets&quot;, &quot;anscombe&quot;)
first(anscombe, 5)
</code></pre>
<div id="tbl:anscombeDf">
<table>
<caption>Table 10: DataFrame for Anscombe’s quartet</caption>
<thead>
<tr class="header">
<th style="text-align: right;">X1</th>
<th style="text-align: right;">X2</th>
<th style="text-align: right;">X3</th>
<th style="text-align: right;">X4</th>
<th style="text-align: right;">Y1</th>
<th style="text-align: right;">Y2</th>
<th style="text-align: right;">Y3</th>
<th style="text-align: right;">Y4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">10.0</td>
<td style="text-align: right;">10.0</td>
<td style="text-align: right;">10.0</td>
<td style="text-align: right;">8.0</td>
<td style="text-align: right;">8.04</td>
<td style="text-align: right;">9.14</td>
<td style="text-align: right;">7.46</td>
<td style="text-align: right;">6.58</td>
</tr>
<tr class="even">
<td style="text-align: right;">8.0</td>
<td style="text-align: right;">8.0</td>
<td style="text-align: right;">8.0</td>
<td style="text-align: right;">8.0</td>
<td style="text-align: right;">6.95</td>
<td style="text-align: right;">8.14</td>
<td style="text-align: right;">6.77</td>
<td style="text-align: right;">5.76</td>
</tr>
<tr class="odd">
<td style="text-align: right;">13.0</td>
<td style="text-align: right;">13.0</td>
<td style="text-align: right;">13.0</td>
<td style="text-align: right;">8.0</td>
<td style="text-align: right;">7.58</td>
<td style="text-align: right;">8.74</td>
<td style="text-align: right;">12.74</td>
<td style="text-align: right;">7.71</td>
</tr>
<tr class="even">
<td style="text-align: right;">9.0</td>
<td style="text-align: right;">9.0</td>
<td style="text-align: right;">9.0</td>
<td style="text-align: right;">8.0</td>
<td style="text-align: right;">8.81</td>
<td style="text-align: right;">8.77</td>
<td style="text-align: right;">7.11</td>
<td style="text-align: right;">8.84</td>
</tr>
<tr class="odd">
<td style="text-align: right;">11.0</td>
<td style="text-align: right;">11.0</td>
<td style="text-align: right;">11.0</td>
<td style="text-align: right;">8.0</td>
<td style="text-align: right;">8.33</td>
<td style="text-align: right;">9.26</td>
<td style="text-align: right;">7.81</td>
<td style="text-align: right;">8.47</td>
</tr>
</tbody>
</table>
</div>
<p>The data frame is part of <a href="https://github.com/JuliaStats/RDatasets.jl">RDatasets</a> that contains a collection of standard datasets used in the <a href="https://en.wikipedia.org/wiki/R_(programming_language)">R programming language</a>. The data frame was carefully designed to demonstrate the perils of relying blindly on correlation coefficients.</p>
<pre>
fig = Cmk.Figure()
i = 0
for r in 1:2
    for c in 1:2
        i += 1
        xname = string("X", i)
        yname = string("Y", i)
        xs = anscombe[:, xname]
        ys = anscombe[:, yname]
        cor, pval = getCorAndPval(xs, ys)
        Cmk.scatter(fig[r, c], xs, ys,
            axis=(;
                title=string("Figure ", "ABCD"[i]),
                xlabel=xname, ylabel=yname,
                limits=(0, 20, 0, 15)
            ))
        Cmk.text!(fig[r, c], 9, 3,
            text="cor(x, y) = $(round(cor, digits=2))")
        Cmk.text!(fig[r, c], 9, 1,
            text="p-val = $(round(pval, digits=4))")
    end
end

fig
</pre>
<p>There’s not much to explain here. The only new part is <code>string</code> function that converts its elements to strings (if they aren’t already) and glues them together into a one long string. The rest is just plain drawing with <code>CairoMakie</code>. Still, take a look at the picture below</p>
<figure>
<img src="./images/ch07AnscombesQuartet.png" id="fig:ch07AnscombesQuartet" alt="Figure 29: Anscombe’s Quartet." /><figcaption aria-hidden="true">Figure 29: Anscombe’s Quartet.</figcaption>
</figure>
<p>All the sub-figures from Figure <a href="#fig:ch07AnscombesQuartet">29</a> depict different relation types between the X and Y variables, yet the correlations and p-values are the same. Two points of notice here. In <strong>Figure B</strong> the points lie in a perfect order on a curve. So, in a perfect word the correlation coefficient should be equal to 1. Yet it is not, as it only measures the spread of the points around an imaginary straight line. Moreover, correlation is sensitive to <a href="https://en.wikipedia.org/wiki/Outlier">outliers</a>. In <strong>Figure D</strong> the X and Y variables appear not to be associated at all. Again, in the perfect world the correlation coefficient should be equal to 0. Still, the outlier on far right (that in real life may have occurred by a typographical error) pumps it up to 0.82 (or what we could call a very strong correlation). Lesson to be learned here, don’t trust the numbers, and whenever you can draw a scatter plot to double check them. And remember, <a href="https://en.wikipedia.org/wiki/All_models_are_wrong">“All models are wrong, but some are useful”</a>.</p>
<p>Other pitfalls are also possible. For instance, imagine you measured body and tail length of a certain species of mouse, here are your results.</p>
<pre class="language-julia"><code># if you are in &#39;code_snippets&#39; folder, then use: &quot;./ch07/miceLengths.csv&quot;
# if you are in &#39;ch07&#39; folder, then use: &quot;./miceLengths.csv&quot;
miceLengths = Csv.read(
    &quot;./code_snippets/ch07/miceLengths.csv&quot;,
    Dfs.DataFrame)
first(miceLengths, 5)
</code></pre>
<div id="tbl:miceLengthsDf">
<table>
<caption>Table 11: Body lengths of a certain mouse species (fictitious data).</caption>
<thead>
<tr class="header">
<th style="text-align: right;">bodyCm</th>
<th style="text-align: right;">tailCm</th>
<th style="text-align: right;">sex</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">11.3</td>
<td style="text-align: right;">2.55</td>
<td style="text-align: right;">f</td>
</tr>
<tr class="even">
<td style="text-align: right;">11.18</td>
<td style="text-align: right;">2.22</td>
<td style="text-align: right;">f</td>
</tr>
<tr class="odd">
<td style="text-align: right;">9.42</td>
<td style="text-align: right;">2.54</td>
<td style="text-align: right;">f</td>
</tr>
<tr class="even">
<td style="text-align: right;">9.21</td>
<td style="text-align: right;">2.2</td>
<td style="text-align: right;">f</td>
</tr>
<tr class="odd">
<td style="text-align: right;">9.97</td>
<td style="text-align: right;">2.63</td>
<td style="text-align: right;">f</td>
</tr>
</tbody>
</table>
</div>
<p>You are interested to know if the tail length is associated with the body length of the animals.</p>
<pre class="language-julia"><code>getCorAndPval(miceLengths.bodyCm, miceLengths.tailCm)</code></pre>
<pre class="output"><code>(0.8899347709623199, 1.5005298337200657e-7)</code></pre>
<p>Clearly it is and even very strongly. Or is it? Well let’s take a look</p>
<figure>
<img src="./images/ch07miceLengths.png" id="fig:ch07miceBodyLengths" alt="Figure 30: Mice body length vs. tail length." /><figcaption aria-hidden="true">Figure 30: Mice body length vs. tail length.</figcaption>
</figure>
<p>It turns out that we have two clusters of points. In both of them the points seem to be randomly scattered. This could be confirmed by testing correlation coefficient for the clusters.</p>
<pre class="language-julia"><code># fml - female mice lengths
# mml - male mice lengths
fml = miceLengths[miceLengths.sex .== &quot;f&quot;, :] # choose only females
mml = miceLengths[miceLengths.sex .== &quot;m&quot;, :] # choose only males

(
    getCorAndPval(fml.bodyCm, fml.tailCm),
    getCorAndPval(mml.bodyCm, mml.tailCm)
)</code></pre>
<pre class="output"><code>((-0.1593819718041706, 0.6821046994037891),
 (-0.02632446813765732, 0.9387606491398499))</code></pre>
<p>The Pearson correlation coefficients are small and not statistically significant (p &gt; 0.05). But since the two clusters of points lie on the opposite sides of the graph, then the overall correlation measures their spread alongside the imaginary dashed line in Figure <a href="#fig:ch07miceBodyLengths">30</a>. This inflates the value of the coefficient. Therefore, it is always good to inspect a graph (scatter plot) to see if there are any clusters of points. The clusters are usually a result of some grouping present in the data (either different experimental groups/treatments or due to some natural grouping). Sometimes we may be unaware of the groups in our data set. Still, if we do know about them, then it is a good idea to inspect the overall correlation and the correlation coefficients for each of the groups.</p>
<p>As the last example let’s take a look at this data frame.</p>
<pre class="language-julia"><code># if you are in &#39;code_snippets&#39; folder, then use: &quot;./ch07/candyBars.csv&quot;
# if you are in &#39;ch07&#39; folder, then use: &quot;./candyBars.csv&quot;
candyBars = Csv.read(
    &quot;./code_snippets/ch07/candyBars.csv&quot;,
    Dfs.DataFrame)
first(candyBars, 5)
</code></pre>
<div id="tbl:candyBarsDf">
<table>
<caption>Table 12: Candy bar composition [g] (fictitious data).</caption>
<thead>
<tr class="header">
<th style="text-align: right;">total</th>
<th style="text-align: right;">carb</th>
<th style="text-align: right;">fat</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">44.49</td>
<td style="text-align: right;">30.23</td>
<td style="text-align: right;">9.67</td>
</tr>
<tr class="even">
<td style="text-align: right;">48.39</td>
<td style="text-align: right;">29.31</td>
<td style="text-align: right;">12.48</td>
</tr>
<tr class="odd">
<td style="text-align: right;">49.83</td>
<td style="text-align: right;">30.95</td>
<td style="text-align: right;">10.58</td>
</tr>
<tr class="even">
<td style="text-align: right;">40.51</td>
<td style="text-align: right;">25.22</td>
<td style="text-align: right;">9.89</td>
</tr>
<tr class="odd">
<td style="text-align: right;">44.51</td>
<td style="text-align: right;">29.45</td>
<td style="text-align: right;">10.15</td>
</tr>
</tbody>
</table>
</div>
<p>Here, we got a data set on composition of different chocolate bars. You are interested to see if the carbohydrate (<code>carb</code>) content in bars is associated with their fat mass.</p>
<pre class="language-julia"><code>getCorAndPval(candyBars.carb, candyBars.fat)</code></pre>
<pre class="output"><code>(0.12176486958519656, 0.7375535843598793)</code></pre>
<p>And it appears it is not. OK, no big deal, and what about <code>carb</code> and <code>total</code> mass of a candy bar?</p>
<pre class="language-julia"><code>getCorAndPval(candyBars.carb, candyBars.total)</code></pre>
<pre class="output"><code>(0.822779226943004, 0.0034638410860259317)</code></pre>
<p>Now we got it. It’s big (r &gt; 0.8) and it’s real (<span class="math inline">\(p \le 0.05\)</span>). But did it really make sense to test that?</p>
<p>If we got a random variable <code>aa</code> then it is going to be perfectly correlated with itself.</p>
<pre class="language-julia"><code>import Random as Rand

Rand.seed!(321)
aa = Rand.rand(Dsts.Normal(100, 15), 10)

getCorAndPval(aa, aa)</code></pre>
<pre class="output"><code>(1.0, 0.0)</code></pre>
<p>On the other hand it shouldn’t be correlated with another random variable <code>bb</code>.</p>
<pre class="language-julia"><code>bb = Rand.rand(Dsts.Normal(100, 15), 10)

getCorAndPval(aa, bb)</code></pre>
<pre class="output"><code>(0.1939999719555875, 0.5912393958185727)</code></pre>
<p>Now, if we add the two variables together we will get the total (<code>cc</code>), that will be correlated with both <code>aa</code> and <code>bb</code>.</p>
<pre class="language-julia"><code>cc = aa .+ bb

(
    getCorAndPval(aa, cc),
    getCorAndPval(bb, cc)
)</code></pre>
<pre class="output"><code>((0.7813386818990972, 0.007608814877251735),
 (0.7638298560460359, 0.01012008535535891))</code></pre>
<p>This is because while correlating <code>aa</code> with <code>cc</code> we are partially correlating <code>aa</code> with itself (<code>aa .+ bb</code>). In general, the greater portion of <code>cc</code> our <code>aa</code> makes the greater the correlation coefficient. So, although possible, it makes little logical sense to compare a part of something with its total. Therefore, in reality running <code>getCorAndPval(candyBars.carb, candyBars.total)</code> does not make much sense despite the interesting result it seems to produce.</p>
<p>To be continued…</p>


<div class="bottom-nav">
    <p id="nav-prev" style="text-align: left;">
        <a class="menu-level-2" href="./assoc_and_pred_data_imports.html"><b>7.1</b> Chapter imports</a> <kbd>←</kbd>
        <span id="nav-next" style="float: right;">
            <kbd>→</kbd> <a class="menu-level-1" href="./appendix.html"><b></b> Appendix</a>
        </span>
    </p>
</div>


<div class="license">
    <br/>
  <br/>
  <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>
    Bartlomiej Lukaszuk
</div>
</div>
</div>
</body>
</html>