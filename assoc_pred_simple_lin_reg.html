<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Bartlomiej Lukaszuk" />
  <title>Simple Linear Regression - Romeo and Julia, where Romeo is Basic Statistics</title>
  <link rel="stylesheet" href="./style.css"/>
    <script src="./mousetrap.min.js"></script>
    <style>
  @font-face {
    font-family: JuliaMono-Regular;
    src: url("./JuliaMono-Regular.woff2");
  }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <link rel="stylesheet" href="./github.min.css">
<script src="./highlight.min.js"></script>
<script src="./julia.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', (event) => {
    document.querySelectorAll('pre').forEach((el) => {
        if (!el.classList.contains('output')) {
            hljs.highlightElement(el);
        }
    });
});
</script>
 
</head>
<body>
<script>
function click_next() {
  var next = document.getElementById('nav-next');
  next.firstElementChild.nextElementSibling.click();
}
function click_prev() {
  var prev = document.getElementById('nav-prev');
  prev.firstElementChild.click();
}
Mousetrap.bind('right', click_next);
Mousetrap.bind('h', click_prev);
Mousetrap.bind('left', click_prev);
Mousetrap.bind('l', click_next);
</script>

<div class="books-container">
<aside class="books-menu">
<input type="checkbox" id="menu">
<label for="menu">☰</label>
<div class="books-title">
<a href="./">Romeo and Julia, where Romeo is Basic Statistics</a>
</div><br />
<span class="books-subtitle">

</span>
<div class="books-menu-content">
<li><a class="menu-level-1" href="./about.html"><b>1</b> About</a></li>
<li><a class="menu-level-1" href="./why_julia.html"><b>2</b> Why Julia</a></li>
<li><a class="menu-level-2" href="./julia_is_fast.html"><b>2.1</b> Julia is fast</a></li>
<li><a class="menu-level-2" href="./julia_is_simple.html"><b>2.2</b> Julia is simple</a></li>
<li><a class="menu-level-2" href="./jl_pleasure_to_write.html"><b>2.3</b> Pleasure to write</a></li>
<li><a class="menu-level-2" href="./jl_not_mainstream.html"><b>2.4</b> Not mainstream</a></li>
<li><a class="menu-level-2" href="./jl_open_source.html"><b>2.5</b> Julia is free</a></li>
<li><a class="menu-level-1" href="./julia_first_encounter.html"><b>3</b> Julia - first encounter</a></li>
<li><a class="menu-level-2" href="./julia_installation.html"><b>3.1</b> Installation</a></li>
<li><a class="menu-level-2" href="./julia_language_constructs.html"><b>3.2</b> Language Constructs</a></li>
<li><a class="menu-level-2" href="./julia_language_variables.html"><b>3.3</b> Variables</a></li>
<li><a class="menu-level-2" href="./julia_language_functions.html"><b>3.4</b> Functions</a></li>
<li><a class="menu-level-2" href="./julia_language_decision_making.html"><b>3.5</b> Decision Making</a></li>
<li><a class="menu-level-2" href="./julia_language_repetition.html"><b>3.6</b> Repetition</a></li>
<li><a class="menu-level-2" href="./julia_language_libraries.html"><b>3.7</b> Additional libraries</a></li>
<li><a class="menu-level-2" href="./julia_language_exercises.html"><b>3.8</b> Julia - Exercises</a></li>
<li><a class="menu-level-2" href="./julia_language_exercises_solutions.html"><b>3.9</b> Julia - Solutions</a></li>
<li><a class="menu-level-1" href="./statistics_intro.html"><b>4</b> Statistics - introduction</a></li>
<li><a class="menu-level-2" href="./statistics_intro_imports.html"><b>4.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./statistics_intro_probability_definition.html"><b>4.2</b> Probability - definition</a></li>
<li><a class="menu-level-2" href="./statistics_intro_probability_properties.html"><b>4.3</b> Probability - properties</a></li>
<li><a class="menu-level-2" href="./statistics_prob_theor_practice.html"><b>4.4</b> Probability - theory and..</a></li>
<li><a class="menu-level-2" href="./statistics_prob_distribution.html"><b>4.5</b> Probability distribution</a></li>
<li><a class="menu-level-2" href="./statistics_normal_distribution.html"><b>4.6</b> Normal distribution</a></li>
<li><a class="menu-level-2" href="./statistics_intro_hypothesis_testing.html"><b>4.7</b> Hypothesis testing</a></li>
<li><a class="menu-level-2" href="./statistics_intro_exercises.html"><b>4.8</b> Statistics intro - Exerc..</a></li>
<li><a class="menu-level-2" href="./statistics_intro_exercises_solutions.html"><b>4.9</b> Statistics intro - Solut..</a></li>
<li><a class="menu-level-1" href="./compare_contin_data.html"><b>5</b> Comparisons - continuous d..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_imports.html"><b>5.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_one_samp_ttest.html"><b>5.2</b> One sample Student’s t..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_two_samp_ttest.html"><b>5.3</b> Two samples Student’s ..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_one_way_anova.html"><b>5.4</b> One-way ANOVA</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_post_hoc_tests.html"><b>5.5</b> Post-hoc tests</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_multip_correction.html"><b>5.6</b> Multiplicity correction</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_exercises.html"><b>5.7</b> Exercises - Comparisons ..</a></li>
<li><a class="menu-level-2" href="./compare_contin_data_exercises_solutions.html"><b>5.8</b> Solutions - Comparisons ..</a></li>
<li><a class="menu-level-1" href="./compare_categ_data.html"><b>6</b> Comparisons - categorical ..</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_imports.html"><b>6.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_flashback.html"><b>6.2</b> Flashback</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_chisq_test.html"><b>6.3</b> Chi squared test</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_fisher_exact_text.html"><b>6.4</b> Fisher’s exact test</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_bigger_table.html"><b>6.5</b> Bigger table</a></li>
<li><a class="menu-level-2" href="./compare_categ_test_for_independence.html"><b>6.6</b> Test for independence</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_exercises.html"><b>6.7</b> Exercises - Comparisons ..</a></li>
<li><a class="menu-level-2" href="./compare_categ_data_exercises_solutions.html"><b>6.8</b> Solutions - Comparisons ..</a></li>
<li><a class="menu-level-1" href="./assoc_pred.html"><b>7</b> Association and Prediction</a></li>
<li><a class="menu-level-2" href="./assoc_pred_imports.html"><b>7.1</b> Chapter imports</a></li>
<li><a class="menu-level-2" href="./assoc_pred_lin_relation.html"><b>7.2</b> Linear relation</a></li>
<li><a class="menu-level-2" href="./assoc_pred_covariance.html"><b>7.3</b> Covariance</a></li>
<li><a class="menu-level-2" href="./assoc_pred_correlation.html"><b>7.4</b> Correlation</a></li>
<li><a class="menu-level-2" href="./assoc_pred_corr_pitfalls.html"><b>7.5</b> Correlation Pitfalls</a></li>
<li><a class="menu-level-2" href="./assoc_pred_simple_lin_reg.html"><b>7.6</b> Simple Linear Regression</a></li>
<li><a class="menu-level-2" href="./assoc_pred_multiple_lin_reg.html"><b>7.7</b> Multiple Linear Regressi..</a></li>
<li><a class="menu-level-2" href="./assoc_pred_exercises.html"><b>7.8</b> Exercises - Association ..</a></li>
<li><a class="menu-level-2" href="./assoc_pred_exercises_solutions.html"><b>7.9</b> Solutions - Association</a></li>
<li><a class="menu-level-1" href="./time_to_say_goodbye.html"><b>8</b> Time to say goodbye</a></li>
</div>
</aside>

<div class="books-content">
<h2 data-number="7.6" id="sec:assoc_pred_simple_lin_reg"><span class="header-section-number">7.6</span> Simple Linear Regression</h2>
<p>We began Section <a href="./assoc_pred_lin_relation.html#sec:assoc_pred_lin_relation">7.2</a> with describing the relation between the volume of water and biomass of two plants of amazon rain forest. Let’s revisit the problem.</p>
<pre class="language-julia"><code>biomass
first(biomass, 5)
</code></pre>
<table>
<caption>Effect of rainfall on plants biomass (fictitious data).</caption>
<thead>
<tr class="header">
<th style="text-align: right;">plantAkg</th>
<th style="text-align: right;">rainL</th>
<th style="text-align: right;">plantBkg</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">20.26</td>
<td style="text-align: right;">15.09</td>
<td style="text-align: right;">21.76</td>
</tr>
<tr class="even">
<td style="text-align: right;">9.18</td>
<td style="text-align: right;">5.32</td>
<td style="text-align: right;">6.08</td>
</tr>
<tr class="odd">
<td style="text-align: right;">11.36</td>
<td style="text-align: right;">12.5</td>
<td style="text-align: right;">10.96</td>
</tr>
<tr class="even">
<td style="text-align: right;">11.26</td>
<td style="text-align: right;">10.7</td>
<td style="text-align: right;">4.96</td>
</tr>
<tr class="odd">
<td style="text-align: right;">9.05</td>
<td style="text-align: right;">5.7</td>
<td style="text-align: right;">9.55</td>
</tr>
</tbody>
</table>
<figure>
<img src="./images/ch07biomassCor.png" alt="Effect of rainfall on plants’ biomass. Revisited." /><figcaption aria-hidden="true">Effect of rainfall on plants’ biomass. Revisited.</figcaption>
</figure>
<p>Previously, we said that the points are scattered around an imaginary line that goes through their center. Now, we could draw that line at a rough guess using a pen and a piece of paper (or a graphics editor). Based on the line we could make a prediction of the values on Y-axis based on the values on the X-axis. The variable placed on the X-axis is called independent (the rain does not depend on a plant, it falls or not), predictor or explanatory variable. The variable placed on the Y-axis is called dependent (the plant depends on rain) or outcome variable. The problem with drawing the line by hand is that it wouldn’t be reproducible, a line drawn by the same person would differ slightly from draw to draw. The same is true if a few different people have undertaken this task. Luckily, we got the <a href="https://en.wikipedia.org/wiki/Simple_linear_regression">simple linear regression</a>, a method that allows us to draw the same line every single time based on a simple mathematical formula that takes the form:</p>
<p><span class="math inline">\(y = a + b*x\)</span>, where:</p>
<ul>
<li>y - predicted value of y</li>
<li>a - intercept (a point on Y-axis where the imaginary line crosses it at x = 0)</li>
<li>b - slope (a value by which y increases/decreases when x changes by one unit)</li>
<li>x - the value of x for which we want to estimate/predict the value of y</li>
</ul>
<p>The slope (<code>b</code>) is fairly easy to calculate with Julia</p>
<pre class="language-julia"><code>function getSlope(xs::Vector{&lt;:Real}, ys::Vector{&lt;:Real})::Float64
    avgXs::Float64 = Stats.mean(xs)
    avgYs::Float64 = Stats.mean(ys)
    diffsXs::Vector{&lt;:Real} = xs .- avgXs
    diffsYs::Vector{&lt;:Real} = ys .- avgYs
    return sum(diffsXs .* diffsYs) / sum(diffsXs .^ 2)
end</code></pre>
<pre class="output"><code>getSlope (generic function with 1 method)</code></pre>
<p>The function resembles the formula for the covariance that we met in Section <a href="./assoc_pred_covariance.html#sec:assoc_pred_covariance">7.3</a>. The difference is that there we divided <code>sum(diffs1 .* diffs2)</code> (here we called it <code>sum(diffsXs .* diffsYs)</code>) by the the degrees of freedom (<code>length(v1) - 1</code>) and here we divide it by <code>sum(diffsXs .^ 2)</code>. We might not have come up with the formula ourselves, still, it makes sense given that we are looking for the value by which y changes when x changes by one unit.</p>
<p>Once we got it, we may proceed to calculate the intercept (<code>a</code>) like so</p>
<pre class="language-julia"><code>function getIntercept(xs::Vector{&lt;:Real}, ys::Vector{&lt;:Real})::Float64
    return Stats.mean(ys) - getSlope(xs, ys) * Stats.mean(xs)
end</code></pre>
<pre class="output"><code>getIntercept (generic function with 1 method)</code></pre>
<p>And now the results.</p>
<pre class="language-julia"><code># be careful, unlike in getCor or getCov, here the order of variables
# in parameters influences the result
plantAIntercept = getIntercept(biomass.rainL, biomass.plantAkg)
plantASlope = getSlope(biomass.rainL, biomass.plantAkg)
plantBIntercept = getIntercept(biomass.rainL, biomass.plantBkg)
plantBSlope = getSlope(biomass.rainL, biomass.plantBkg)

round.([plantASlope, plantBSlope], digits = 2)</code></pre>
<pre class="output"><code>[1.04, 1.14]</code></pre>
<p>The intercepts are not our primary interest (we will explain why in a moment or two). We are more concerned with the slopes. Based on the slopes we can say that on average each additional liter or water (<code>rainL</code>) translates into 1.04 [kg] more biomass for <code>plantA</code> and 1.14 [kg] more biomass for <code>plantB</code>. Although, based on the correlation coefficients from Section <a href="./assoc_pred_correlation.html#sec:assoc_pred_correlation">7.4</a> we know that the estimate for <code>plantB</code> is less precise. This is because the smaller correlation coefficient means a greater spread of the points along the line as can be seen in the figure below.</p>
<pre><code>fig = Cmk.Figure()
ax1, sc1 = Cmk.scatter(fig[1, 1], biomass.rainL, biomass.plantAkg,
    markersize=25, color=&quot;skyblue&quot;, strokewidth=1, strokecolor=&quot;gray&quot;,
    axis=(; title=&quot;Effect of rainfall on biomass of plant A&quot;,
        xlabel=&quot;water [L]&quot;, ylabel=&quot;biomass [kg]&quot;)
)
ax2, sc2 = Cmk.scatter(fig[1, 2], biomass.rainL, biomass.plantBkg,
    markersize=25, color=&quot;linen&quot;, strokewidth=1, strokecolor=&quot;black&quot;,
    axis=(; title=&quot;Effect of rainfall on bomass of plant B&quot;,
        xlabel=&quot;water [L]&quot;, ylabel=&quot;biomass [kg]&quot;)
)
Cmk.ablines!(fig[1, 1],
    plantAIntercept,
    plantASlope,
    linestyle=:dash, color=&quot;gray&quot;)
Cmk.ablines!(fig[1, 2],
    plantBIntercept,
    plantBSlope,
    linestyle=:dash, color=&quot;gray&quot;)
Cmk.linkxaxes!(ax1, ax2)
Cmk.linkyaxes!(ax1, ax2)
fig</code></pre>
<figure>
<img src="./images/ch07biomassCor2.png" id="fig:ch07biomassCor2" alt="Figure 31: Effect of rainfall on plants’ biomass with trend lines superimposed." /><figcaption aria-hidden="true">Figure 31: Effect of rainfall on plants’ biomass with trend lines superimposed.</figcaption>
</figure>
<p>The trend line is placed more or less where we would have placed it at a rough guess, so it seems we got our functions right.</p>
<p>Now we can either use the graph (Figure <a href="#fig:ch07biomassCor2">31</a>) and read the expected value of the variable on the Y-axis based on a value on the X-axis (using a dashed line). Alternatively, we can write a formula based on <span class="math inline">\(y = a + b*x\)</span> we mentioned before to get that estimate.</p>
<pre class="language-julia"><code>function getPrecictedY(
    x::Float64, intercept::Float64, slope::Float64)::Float64
    return intercept + slope * x
end

round.(
    getPrecictedY.([6.0, 10, 12], plantAIntercept, plantASlope),
    digits = 2)</code></pre>
<pre class="output"><code>[8.4, 12.57, 14.65]</code></pre>
<p>It appears to work as expected (to confirm it read from Figure <a href="#fig:ch07biomassCor2">31</a> values on Y-axis for the following values on X-axis: [6.0, 10, 12] using the dashed line for <code>plantA</code>).</p>
<p>OK, and now imagine you intend to introduce <code>plantA</code> into a <a href="https://en.wikipedia.org/wiki/Botanical_garden">botanic garden</a> and you want it to grow well and fast. The function <code>getPrecictedY</code> tells us that if you pour 35 [L] of water to a field with <code>plantA</code> then on average you should get 42 [kg] of the biomass. Unfortunately after you applied the treatment it turned out the biomass actually dropped to 10 [kg] from the field. What happened? Reality. Most likely you (almost) drowned your plant. Lesson to be learned here. It is unsafe to use a model to make predictions beyond the data range on which it was trained. Ultimately, <a href="https://en.wikipedia.org/wiki/All_models_are_wrong">“All models are wrong, but some are useful”</a>.</p>
<p>The above is the reason why in most cases we aren’t interested in the value of the intercept. The intercept is the value on the Y-axis when X is equal to 0, it is necessary for our model to work, but most likely it isn’t very informative (in our case a plant that receives no water simply dies).</p>
<p>So what is regression good for if it only enables us to make a prediction within the range on which it was trained? Well, if you ever underwent <a href="https://en.wikipedia.org/wiki/Spirometry">spirometry</a> then you used regression in practice (or at least benefited from it). The functional examination of the respiratory system goes as follows. First, you enter your data: name, sex, height, weight, age, etc. Then you breathe (in a manner recommended by a technician) through a mouthpiece connected to an analyzer. Finally, you compare your results with the ones you should have obtained. If, let’s say your <a href="https://en.wikipedia.org/wiki/Vital_capacity">vital capacity</a> is equal to 5.1 [L] and should be equal to 5 [L] then it is a good sign. However, if the obtained value is equal to 4 [L] when it should be 5 [L] (4/5 = 0.8 = 80% of the norm) then you should consult your physician. But where does the reference value come from?</p>
<p>One way to get it would be to rely on a large database, of let’s say 100-200 million healthy individuals (a data frame with 100-200 million rows and 5-6 columns for age, gender, height, etc. that is stored on a hard drive). Then all you have to do is to find a person (or people) whose data match yours exactly. Then you can take their vital capacity (or their a mean if there is more than one person that matches your features) as a reference point for yours. But this would be a great burden. For once you would have to collect data for a lot of individuals to be pretty sure that an exact combination of a given set of features occurs (hence the 100-200 million mentioned above). The other problem is that such a data frame would occupy a lot of disk space and would be slow to search through. A better solution is regression (most likely multiple linear regression that we will cover in Section <a href="./assoc_pred_multiple_lin_reg.html#sec:assoc_pred_multiple_lin_reg">7.7</a>). In that case you collect a smaller sample of let’s say 10’000 healthy individuals. You train your regression model. And store it together with the <code>getPrecictedY</code> function (where <code>Y</code> could be the discussed vital capacity). Now, you can easily and quickly calculate the reference value for a patient even if the exact set of features (values of predictor variables) was not in your training data set (still, you can be fairly sure that the values of the features of the patient are in the range of the training data set).</p>
<p>Anyway, in real life whenever you want to fit a regression line in Julia you should probably use <a href="https://juliastats.org/GLM.jl/stable/">GLM.jl</a> package. In our case an exemplary output for <code>plantA</code> looks as follows.</p>
<pre class="language-julia"><code>import GLM as Glm

mod1 = Glm.lm(Glm.@formula(plantAkg ~ rainL), biomass)
mod1</code></pre>
<pre class="output"><code>plantAkg ~ 1 + rainL

Coefficients:
──────────────────────────────────────────────────────────────────────
               Coef.  Std. Error     t  Pr(&gt;|t|)  Lower 95%  Upper 95%
──────────────────────────────────────────────────────────────────────
(Intercept)  2.14751    2.04177   1.05    0.3068  -2.14208     6.43711
rainL        1.04218    0.195771  5.32    &lt;1e-04   0.630877    1.45347
──────────────────────────────────────────────────────────────────────</code></pre>
<p>We begin with <code>Glm.lm(formula, dataFrame)</code> (<code>lm</code> stands for linear model). Next, we specify our relationship (<code>Cmk.@formula</code>) in the form <code>Y ~ X</code>, where <code>Y</code> is the dependent (outcome) variable, <code>~</code> is explained by, and <code>X</code> is the independent (explanatory) variable. This fits our model (<code>mod1</code>) to the data and yields quite some output.</p>
<p>The <code>Coef.</code> column contains the values of the intercept (previously estimated with <code>getIntercept</code>) and slope (before we used <code>getSlope</code> for that). It is followed by the <code>Std. Error</code> of the estimation (similar to the <code>sem</code> from Section <a href="./compare_contin_data_one_samp_ttest.html#sec:compare_contin_data_one_samp_ttest">5.2</a>). Then, just like in the case of the correlation (Section <a href="./assoc_pred_correlation.html#sec:assoc_pred_correlation">7.4</a>), some clever mathematical tweaking allows us to obtain a t-statistic for the <code>Coef.</code>s and p-values for them. The p-values tell us if the coefficients are really different from 0 (<span class="math inline">\(H_{0}\)</span>: a <code>Coeff.</code> is equal to 0) or estimate the probability that such a big value (or bigger) happened by chance alone (assuming that <span class="math inline">\(H_{0}\)</span> is true). Finally, we end up with 95% confidence interval (similar to the one discussed in Section <a href="./compare_contin_data_one_samp_ttest.html#sec:compare_contin_data_hypo_tests_package">5.2.1</a>) that (oversimplifying stuff) tells us, with a degree of certainty, within what limits the true value of the coefficient in the population is.</p>
<p>We can use <code>GLM</code> to make our predictions as well.</p>
<pre class="language-julia"><code>round.(
    Glm.predict(mod1, Dfs.DataFrame(Dict(&quot;rainL&quot; =&gt; [6, 10, 12]))),
    digits = 2
)</code></pre>
<pre class="output"><code>[8.4, 12.57, 14.65]</code></pre>
<p>For that to work we feed <code>Glm.predict</code> with our model (<code>mod1</code>) and a <code>DataFrame</code> containing a column <code>rainL</code> that was used as a predictor in our model and voila, the results match those returned by <code>getPrecictedY</code> somewhat before in this section.</p>
<p>We can also get the general impression of how imprecise our prediction is by using the residuals (differences between the predicted and actual value on the Y-axis). Like so</p>
<pre class="language-julia"><code># an average estimation error in prediction
# (based on abs differences)
function getAvgEstimError(
    lm::Glm.StatsModels.TableRegressionModel)::Float64
    return abs.(Glm.residuals(lm)) |&gt; Stats.mean
end

getAvgEstimError(mod1)</code></pre>
<p>2.075254994044967</p>
<p>So, on average our model miscalculates the value on the Y-axis (<code>plantAkg</code>) by 2 units (here kilograms). Of course, this is a slightly optimistic view, since we expect that on a new, previously unseen data set, the prediction error will be greater.</p>
<p>Moreover, the package allows us to calculate other useful stuff, like the <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">coefficient of determination</a> that tells us how much change in the variability on Y-axis is explained by our model (our explanatory variable(s)).</p>
<pre class="language-julia"><code>(
    Glm.r2(mod1),
    Stats.cor(biomass.rainL, biomass.plantAkg) ^ 2
)</code></pre>
<pre class="output"><code>(0.6115596392611107, 0.6115596392611111)</code></pre>
<p>The coefficient of determination is called <span class="math inline">\(r^2\)</span> (r squared) and in this case (simple linear regression) it is equal to the Pearson’s correlation coefficient (denoted as <code>r</code>) times itself. As we can see our model explains roughly 61% of variability in <code>plantAkg</code> biomass.</p>


<div class="bottom-nav">
    <p id="nav-prev" style="text-align: left;">
        <a class="menu-level-2" href="./assoc_pred_corr_pitfalls.html"><b>7.5</b> Correlation Pitfalls</a> <kbd>←</kbd>
        <span id="nav-next" style="float: right;">
            <kbd>→</kbd> <a class="menu-level-2" href="./assoc_pred_multiple_lin_reg.html"><b>7.7</b> Multiple Linear Regressi..</a>
        </span>
    </p>
</div>


<div class="license">
    <br/>
  <br/>
  <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>
    Bartlomiej Lukaszuk
</div>
</div>
</div>
</body>
</html>